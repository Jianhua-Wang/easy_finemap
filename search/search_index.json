{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"easyfinemap \u00b6 user-friendly pipeline for GWAS fine-mapping Documentation: https://Jianhua-Wang.github.io/easyfinemap GitHub: https://github.com/Jianhua-Wang/easyfinemap PyPI: https://pypi.org/project/easyfinemap/ Free software: MIT Features \u00b6 Prepare LD reference for fine-mapping Standardize input summary statistics Identify independent loci by distance, LD clumping, or conditional analysis Fine-mapping with or without LD reference Finemapping approaches \u00b6 LD-free aBF LD-based FINEMAP CAVIARBF PAINTOR","title":"Index"},{"location":"#easyfinemap","text":"user-friendly pipeline for GWAS fine-mapping Documentation: https://Jianhua-Wang.github.io/easyfinemap GitHub: https://github.com/Jianhua-Wang/easyfinemap PyPI: https://pypi.org/project/easyfinemap/ Free software: MIT","title":"easyfinemap"},{"location":"#features","text":"Prepare LD reference for fine-mapping Standardize input summary statistics Identify independent loci by distance, LD clumping, or conditional analysis Fine-mapping with or without LD reference","title":"Features"},{"location":"#finemapping-approaches","text":"LD-free aBF LD-based FINEMAP CAVIARBF PAINTOR","title":"Finemapping approaches"},{"location":"changelog/","text":"Changelog \u00b6 [0.3.5] - 2023-06-13 \u00b6 Added \u00b6 Changed \u00b6 fix smunger independency Fixed \u00b6 [0.3.4] - 2023-06-12 \u00b6 Added \u00b6 Changed \u00b6 fix bugs in loci by ldblock Fixed \u00b6 [0.3.3] - 2023-06-12 \u00b6 Added \u00b6 Changed \u00b6 fix bugs in loci by ldblock Fixed \u00b6 [0.3.2] - 2023-05-25 \u00b6 Added \u00b6 Changed \u00b6 remove pathos Fixed \u00b6 [0.3.1] - 2023-05-25 \u00b6 Added \u00b6 locus plot Changed \u00b6 Fixed \u00b6 [0.3.0] - 2023-05-24 \u00b6 Added \u00b6 annotate R2 for locus plot Changed \u00b6 Fixed \u00b6 [0.2.9] - 2023-05-24 \u00b6 Added \u00b6 Changed \u00b6 Speed up susie by using fread Fixed \u00b6 [0.2.8] - 2023-05-23 \u00b6 Added \u00b6 Changed \u00b6 Speed up conditional analysis by intersect ldref with input sumstat first Fixed \u00b6 [0.2.7] - 2023-05-20 \u00b6 Added \u00b6 Changed \u00b6 Use tabix to load sumstats when finemap Fixed \u00b6 [0.2.6] - 2023-05-19 \u00b6 Added \u00b6 Support polyfun for finemap and susie Changed \u00b6 Use the most significant SNP as lead SNP when COJO fails Fixed \u00b6 [0.2.5] - 2023-03-30 \u00b6 Added \u00b6 set the most significant SNP as SNP, if there are no SNPs with P-value \u2264 threshold. Changed \u00b6 Fixed \u00b6 [0.2.4] - 2023-03-30 \u00b6 Added \u00b6 support susie Changed \u00b6 Fixed \u00b6 [0.2.3] - 2023-03-24 \u00b6 Added \u00b6 suppor using LD blocks as loci boudaries. Changed \u00b6 Fixed \u00b6 [0.2.2] - 2023-03-22 \u00b6 Added \u00b6 Changed \u00b6 deleted format function Fixed \u00b6 [0.2.1] - 2023-01-10 \u00b6 Added \u00b6 Instruction of installation Changed \u00b6 Fixed \u00b6 typo in easyfinemap.py line519 [0.2.0] - 2023-01-10 \u00b6 Added \u00b6 add CAVIARBF Changed \u00b6 Fixed \u00b6 [0.1.4] - 2023-01-10 \u00b6 Added \u00b6 add PAINTOR Changed \u00b6 Fixed \u00b6 [0.1.3] - 2023-01-09 \u00b6 Added \u00b6 output credible sets Changed \u00b6 Fixed \u00b6 [0.1.2] - 2023-01-07 \u00b6 Added \u00b6 update summary statistics using cojo-cond make ld matrix using plink --r2 fine-mapping tools: FINEMAP Changed \u00b6 Fixed \u00b6 [0.1.1] - 2023-01-07 \u00b6 Added \u00b6 add temp dir decorator identify lead SNPs by LD clumping identify lead SNPs by conditional analysis Changed \u00b6 Fixed \u00b6 [0.0.5] - 2022-12-26 \u00b6 Added \u00b6 validate GWAS summary statistics Changed \u00b6 Fixed \u00b6 [0.0.4] - 2022-12-26 \u00b6 Added \u00b6 prepare and validate LD reference panel Changed \u00b6 Fixed \u00b6 [0.0.1] - 2022-12-20 \u00b6 Added \u00b6 merge the overlapped independent loci (optional). Changed \u00b6 Fixed \u00b6 [0.0.2] - 2022-12-22 \u00b6 Added \u00b6 identify the independent lead snps by distance only expand the independent lead snps to independent loci by given range. Changed \u00b6 Fixed \u00b6 [0.0.3] - 2022-12-22 \u00b6 Added \u00b6 extract LD ref plink bfile and clean it. Changed \u00b6 Fixed \u00b6","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#035-2023-06-13","text":"","title":"[0.3.5] - 2023-06-13"},{"location":"changelog/#added","text":"","title":"Added"},{"location":"changelog/#changed","text":"fix smunger independency","title":"Changed"},{"location":"changelog/#fixed","text":"","title":"Fixed"},{"location":"changelog/#034-2023-06-12","text":"","title":"[0.3.4] - 2023-06-12"},{"location":"changelog/#added_1","text":"","title":"Added"},{"location":"changelog/#changed_1","text":"fix bugs in loci by ldblock","title":"Changed"},{"location":"changelog/#fixed_1","text":"","title":"Fixed"},{"location":"changelog/#033-2023-06-12","text":"","title":"[0.3.3] - 2023-06-12"},{"location":"changelog/#added_2","text":"","title":"Added"},{"location":"changelog/#changed_2","text":"fix bugs in loci by ldblock","title":"Changed"},{"location":"changelog/#fixed_2","text":"","title":"Fixed"},{"location":"changelog/#032-2023-05-25","text":"","title":"[0.3.2] - 2023-05-25"},{"location":"changelog/#added_3","text":"","title":"Added"},{"location":"changelog/#changed_3","text":"remove pathos","title":"Changed"},{"location":"changelog/#fixed_3","text":"","title":"Fixed"},{"location":"changelog/#031-2023-05-25","text":"","title":"[0.3.1] - 2023-05-25"},{"location":"changelog/#added_4","text":"locus plot","title":"Added"},{"location":"changelog/#changed_4","text":"","title":"Changed"},{"location":"changelog/#fixed_4","text":"","title":"Fixed"},{"location":"changelog/#030-2023-05-24","text":"","title":"[0.3.0] - 2023-05-24"},{"location":"changelog/#added_5","text":"annotate R2 for locus plot","title":"Added"},{"location":"changelog/#changed_5","text":"","title":"Changed"},{"location":"changelog/#fixed_5","text":"","title":"Fixed"},{"location":"changelog/#029-2023-05-24","text":"","title":"[0.2.9] - 2023-05-24"},{"location":"changelog/#added_6","text":"","title":"Added"},{"location":"changelog/#changed_6","text":"Speed up susie by using fread","title":"Changed"},{"location":"changelog/#fixed_6","text":"","title":"Fixed"},{"location":"changelog/#028-2023-05-23","text":"","title":"[0.2.8] - 2023-05-23"},{"location":"changelog/#added_7","text":"","title":"Added"},{"location":"changelog/#changed_7","text":"Speed up conditional analysis by intersect ldref with input sumstat first","title":"Changed"},{"location":"changelog/#fixed_7","text":"","title":"Fixed"},{"location":"changelog/#027-2023-05-20","text":"","title":"[0.2.7] - 2023-05-20"},{"location":"changelog/#added_8","text":"","title":"Added"},{"location":"changelog/#changed_8","text":"Use tabix to load sumstats when finemap","title":"Changed"},{"location":"changelog/#fixed_8","text":"","title":"Fixed"},{"location":"changelog/#026-2023-05-19","text":"","title":"[0.2.6] - 2023-05-19"},{"location":"changelog/#added_9","text":"Support polyfun for finemap and susie","title":"Added"},{"location":"changelog/#changed_9","text":"Use the most significant SNP as lead SNP when COJO fails","title":"Changed"},{"location":"changelog/#fixed_9","text":"","title":"Fixed"},{"location":"changelog/#025-2023-03-30","text":"","title":"[0.2.5] - 2023-03-30"},{"location":"changelog/#added_10","text":"set the most significant SNP as SNP, if there are no SNPs with P-value \u2264 threshold.","title":"Added"},{"location":"changelog/#changed_10","text":"","title":"Changed"},{"location":"changelog/#fixed_10","text":"","title":"Fixed"},{"location":"changelog/#024-2023-03-30","text":"","title":"[0.2.4] - 2023-03-30"},{"location":"changelog/#added_11","text":"support susie","title":"Added"},{"location":"changelog/#changed_11","text":"","title":"Changed"},{"location":"changelog/#fixed_11","text":"","title":"Fixed"},{"location":"changelog/#023-2023-03-24","text":"","title":"[0.2.3] - 2023-03-24"},{"location":"changelog/#added_12","text":"suppor using LD blocks as loci boudaries.","title":"Added"},{"location":"changelog/#changed_12","text":"","title":"Changed"},{"location":"changelog/#fixed_12","text":"","title":"Fixed"},{"location":"changelog/#022-2023-03-22","text":"","title":"[0.2.2] - 2023-03-22"},{"location":"changelog/#added_13","text":"","title":"Added"},{"location":"changelog/#changed_13","text":"deleted format function","title":"Changed"},{"location":"changelog/#fixed_13","text":"","title":"Fixed"},{"location":"changelog/#021-2023-01-10","text":"","title":"[0.2.1] - 2023-01-10"},{"location":"changelog/#added_14","text":"Instruction of installation","title":"Added"},{"location":"changelog/#changed_14","text":"","title":"Changed"},{"location":"changelog/#fixed_14","text":"typo in easyfinemap.py line519","title":"Fixed"},{"location":"changelog/#020-2023-01-10","text":"","title":"[0.2.0] - 2023-01-10"},{"location":"changelog/#added_15","text":"add CAVIARBF","title":"Added"},{"location":"changelog/#changed_15","text":"","title":"Changed"},{"location":"changelog/#fixed_15","text":"","title":"Fixed"},{"location":"changelog/#014-2023-01-10","text":"","title":"[0.1.4] - 2023-01-10"},{"location":"changelog/#added_16","text":"add PAINTOR","title":"Added"},{"location":"changelog/#changed_16","text":"","title":"Changed"},{"location":"changelog/#fixed_16","text":"","title":"Fixed"},{"location":"changelog/#013-2023-01-09","text":"","title":"[0.1.3] - 2023-01-09"},{"location":"changelog/#added_17","text":"output credible sets","title":"Added"},{"location":"changelog/#changed_17","text":"","title":"Changed"},{"location":"changelog/#fixed_17","text":"","title":"Fixed"},{"location":"changelog/#012-2023-01-07","text":"","title":"[0.1.2] - 2023-01-07"},{"location":"changelog/#added_18","text":"update summary statistics using cojo-cond make ld matrix using plink --r2 fine-mapping tools: FINEMAP","title":"Added"},{"location":"changelog/#changed_18","text":"","title":"Changed"},{"location":"changelog/#fixed_18","text":"","title":"Fixed"},{"location":"changelog/#011-2023-01-07","text":"","title":"[0.1.1] - 2023-01-07"},{"location":"changelog/#added_19","text":"add temp dir decorator identify lead SNPs by LD clumping identify lead SNPs by conditional analysis","title":"Added"},{"location":"changelog/#changed_19","text":"","title":"Changed"},{"location":"changelog/#fixed_19","text":"","title":"Fixed"},{"location":"changelog/#005-2022-12-26","text":"","title":"[0.0.5] - 2022-12-26"},{"location":"changelog/#added_20","text":"validate GWAS summary statistics","title":"Added"},{"location":"changelog/#changed_20","text":"","title":"Changed"},{"location":"changelog/#fixed_20","text":"","title":"Fixed"},{"location":"changelog/#004-2022-12-26","text":"","title":"[0.0.4] - 2022-12-26"},{"location":"changelog/#added_21","text":"prepare and validate LD reference panel","title":"Added"},{"location":"changelog/#changed_21","text":"","title":"Changed"},{"location":"changelog/#fixed_21","text":"","title":"Fixed"},{"location":"changelog/#001-2022-12-20","text":"","title":"[0.0.1] - 2022-12-20"},{"location":"changelog/#added_22","text":"merge the overlapped independent loci (optional).","title":"Added"},{"location":"changelog/#changed_22","text":"","title":"Changed"},{"location":"changelog/#fixed_22","text":"","title":"Fixed"},{"location":"changelog/#002-2022-12-22","text":"","title":"[0.0.2] - 2022-12-22"},{"location":"changelog/#added_23","text":"identify the independent lead snps by distance only expand the independent lead snps to independent loci by given range.","title":"Added"},{"location":"changelog/#changed_23","text":"","title":"Changed"},{"location":"changelog/#fixed_23","text":"","title":"Fixed"},{"location":"changelog/#003-2022-12-22","text":"","title":"[0.0.3] - 2022-12-22"},{"location":"changelog/#added_24","text":"extract LD ref plink bfile and clean it.","title":"Added"},{"location":"changelog/#changed_24","text":"","title":"Changed"},{"location":"changelog/#fixed_24","text":"","title":"Fixed"},{"location":"installation/","text":"Installation \u00b6 Stable release \u00b6 To install easy_finemap, run this command in your terminal: $ pip install easyfinemap This is the preferred method to install easyfinemap, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process. From source \u00b6 The source for easy_finemap can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/Jianhua-Wang/easy_finemap Or download the tarball : $ curl -OJL https://github.com/Jianhua-Wang/easy_finemap/tarball/master Once you have a copy of the source, you can install it with: $ pip install . Install finemappingn tools used in EasyFinemap \u00b6 EasyFinemap uses several famous finemapping tools, such as FINEMAP, CAVIARBF, and PAINTOR. You can make them from the source code by yourself or install them using conda . Or you just create the environment of EasyFinemap using conda .","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#stable-release","text":"To install easy_finemap, run this command in your terminal: $ pip install easyfinemap This is the preferred method to install easyfinemap, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process.","title":"Stable release"},{"location":"installation/#from-source","text":"The source for easy_finemap can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/Jianhua-Wang/easy_finemap Or download the tarball : $ curl -OJL https://github.com/Jianhua-Wang/easy_finemap/tarball/master Once you have a copy of the source, you can install it with: $ pip install .","title":"From source"},{"location":"installation/#install-finemappingn-tools-used-in-easyfinemap","text":"EasyFinemap uses several famous finemapping tools, such as FINEMAP, CAVIARBF, and PAINTOR. You can make them from the source code by yourself or install them using conda . Or you just create the environment of EasyFinemap using conda .","title":"Install finemappingn tools used in EasyFinemap"},{"location":"usage/","text":"Usage \u00b6 To use easy_finemap in a project import easy_finemap","title":"Usage"},{"location":"usage/#usage","text":"To use easy_finemap in a project import easy_finemap","title":"Usage"},{"location":"api/LDRef/","text":"Prepare LD reference for easyfinemap. Source code in easyfinemap/ldref.py 31 32 33 34 35 36 37 38 def __init__ ( self ): \"\"\"Initialize the LDRef class.\"\"\" self . logger = logging . getLogger ( \"LDRef\" ) self . plink = Tools () . plink self . gcta = Tools () . gcta self . tmp_root = Path . cwd () / \"tmp\" / \"ldref\" if not self . tmp_root . exists (): self . tmp_root . mkdir ( parents = True ) annotate_r2 ( sumstat , ldref , ld_snp , temp_dir = None ) \u00b6 Annotate SNPs with r2 to the lead SNP. Parameters: Name Type Description Default sumstat pd . DataFrame The summary statistics. required ldref str The path to the LD reference file. required ld_snp str The lead SNP. required temp_dir Optional [ str ], optional The path to the temporary directory, by default None None Returns: Type Description pd . DataFrame The annotated summary statistics. Source code in easyfinemap/ldref.py 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 @io_in_tempdir ( './tmp/ldref' ) def annotate_r2 ( self , sumstat : pd . DataFrame , ldref : str , ld_snp : str , temp_dir : Optional [ str ] = None , ) -> pd . DataFrame : \"\"\" Annotate SNPs with r2 to the lead SNP. Parameters ---------- sumstat : pd.DataFrame The summary statistics. ldref : str The path to the LD reference file. ld_snp : str The lead SNP. temp_dir : Optional[str], optional The path to the temporary directory, by default None Returns ------- pd.DataFrame The annotated summary statistics. \"\"\" if len ( sumstat [ ColName . CHR ] . unique ()) > 1 : raise ValueError ( \"Only one chromosome is allowed.\" ) chrom = sumstat [ ColName . CHR ] . iloc [ 0 ] if len ( sumstat ) > 100000 : self . logger . warning ( \"The sumstats is large, it may take a long time to annotate the r2.\" ) ld = LDRef () r2_df = sumstat . copy () r2_input = ld . intersect ( sumstat , ldref . format ( chrom = chrom ), f \" { temp_dir } /r2_input_ { chrom } \" ) if ld_snp not in r2_input [ ColName . SNPID ] . tolist (): raise ValueError ( f \" { ld_snp } not in the LD reference.\" ) cmd = [ self . plink , \"--bfile\" , f \" { temp_dir } /r2_input_ { chrom } \" , \"--r2\" , \"--ld-snp\" , ld_snp , \"--ld-window-kb\" , \"100000\" , \"--ld-window\" , \"99999999\" , \"--ld-window-r2\" , \"0\" , \"--keep-allele-order\" , \"--out\" , f \" { temp_dir } /r2_ { chrom } \" , ] self . logger . debug ( f \"annotate r2: { ' ' . join ( cmd ) } \" ) res = run ( cmd , stdout = PIPE , stderr = PIPE , universal_newlines = True ) if res . returncode != 0 : self . logger . error ( res . stderr ) raise RuntimeError ( res . stderr ) else : res_r2 = pd . read_csv ( f \" { temp_dir } /r2_ { chrom } .ld\" , delim_whitespace = True ) res_r2 = pd . Series ( res_r2 [ \"R2\" ] . values , index = res_r2 [ \"SNP_B\" ] . values ) r2_df [ \"R2\" ] = r2_df [ ColName . SNPID ] . map ( res_r2 ) r2_df . loc [ r2_df [ ColName . SNPID ] == ld_snp , \"R2\" ] = 1 r2_df [ 'R2' ] = r2_df [ 'R2' ] . fillna ( - 1 ) return r2_df cojo_cond ( sumstats , cond_snps , ldref , sample_size , use_ref_EAF = False , temp_dir = None ) \u00b6 Conditional analysis. Update the beta, se, pval of the conditional SNPs. Parameters: Name Type Description Default sumstats pd . DataFrame The summary statistics. required cond_snps pd . DataFrame The conditional SNPs. required ldref str The path to the LD reference file. required sample_size int The sample size. required use_ref_EAF bool , optional Whether to use the EAF in the LD reference file, by default False False temp_dir Optional [ str ], optional The path to the temporary directory, by default None None Raises: Type Description ValueError If the EAF is not in the sumstats and use_ref_EAF is False. Returns: Type Description pd . DataFrame The updated summary statistics. Source code in easyfinemap/ldref.py 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 @io_in_tempdir ( './tmp/ldref' ) def cojo_cond ( self , sumstats : pd . DataFrame , cond_snps : pd . DataFrame , ldref : str , sample_size : int , use_ref_EAF : bool = False , temp_dir : Optional [ str ] = None , ) -> pd . DataFrame : \"\"\" Conditional analysis. Update the beta, se, pval of the conditional SNPs. Parameters ---------- sumstats : pd.DataFrame The summary statistics. cond_snps : pd.DataFrame The conditional SNPs. ldref : str The path to the LD reference file. sample_size : int The sample size. use_ref_EAF : bool, optional Whether to use the EAF in the LD reference file, by default False temp_dir : Optional[str], optional The path to the temporary directory, by default None Raises ------ ValueError If the EAF is not in the sumstats and use_ref_EAF is False. Returns ------- pd.DataFrame The updated summary statistics. \"\"\" if not use_ref_EAF and ColName . EAF not in sumstats . columns : raise ValueError ( f \" { ColName . EAF } is not in the sumstats, please set use_ref_EAF to True\" ) chrom = sumstats [ ColName . CHR ] . iloc [ 0 ] ld = LDRef () all_sumstats = pd . concat ([ sumstats , cond_snps ], ignore_index = True ) all_sumstats . drop_duplicates ( subset = [ ColName . SNPID ], inplace = True ) all_sumstats . sort_values ( by = [ ColName . CHR , ColName . BP ], inplace = True ) all_sumstats . reset_index ( drop = True , inplace = True ) cojo_input = ld . intersect ( all_sumstats , ldref , f \" { temp_dir } /cojo_input_ { chrom } \" , use_ref_EAF ) cojo_input [ ColName . N ] = sample_size cojo_input = cojo_input [ [ ColName . SNPID , ColName . EA , ColName . NEA , ColName . EAF , ColName . BETA , ColName . SE , ColName . P , ColName . N ] ] cojo_input . rename ( columns = { ColName . SNPID : \"SNP\" , ColName . EA : \"A1\" , ColName . NEA : \"A2\" , ColName . EAF : \"freq\" , ColName . BETA : \"b\" , ColName . SE : \"se\" , ColName . P : \"p\" , ColName . N : \"N\" , }, inplace = True , ) cojo_p_file = f \" { temp_dir } /cojo_input_ { chrom } .ma\" cojo_input . to_csv ( cojo_p_file , sep = \" \" , index = False ) with open ( f \" { temp_dir } /cojo_cond_ { chrom } .snps\" , \"w\" ) as f : f . write ( ' \\n ' . join ( cond_snps [ ColName . SNPID ] . tolist ())) cojo_outfile = f \" { temp_dir } /cojo_ { chrom } .cond\" cmd = [ self . gcta , \"--bfile\" , ldref , \"--cojo-file\" , cojo_p_file , \"--cojo-cond\" , f \" { temp_dir } /cojo_cond_ { chrom } .snps\" , \"--out\" , cojo_outfile , ] res = run ( cmd , stdout = PIPE , stderr = PIPE , universal_newlines = True ) if res . returncode != 0 : self . logger . error ( res . stderr ) raise RuntimeError ( res . stderr ) else : cond_res = pd . read_csv ( f \" { cojo_outfile } .cma.cojo\" , sep = \" \\t \" , usecols = [ \"SNP\" , \"bC\" , \"bC_se\" , \"pC\" ]) cond_res . rename ( columns = { \"SNP\" : ColName . SNPID , \"bC\" : ColName . COJO_BETA , \"bC_se\" : ColName . COJO_SE , \"pC\" : ColName . COJO_P }, inplace = True , ) output = sumstats . merge ( cond_res , on = ColName . SNPID , how = \"left\" ) output = output . dropna ( subset = [ ColName . COJO_P , ColName . COJO_BETA , ColName . COJO_SE ]) return output extract ( inprefix , outprefix , chrom , temp_dir = None , start = None , end = None , mac = 10 ) \u00b6 Extract the genotypes of given region from the LD reference. Parameters: Name Type Description Default inprefix str The input prefix. required outprefix str The output prefix. required chrom int The chromosome number. required temp_dir str The temporary directory. None start int , optional The start position, by default None None end int , optional The end position, by default None None mac int The minor allele count threshold, by default 10 10 Returns: Type Description None Source code in easyfinemap/ldref.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 @io_in_tempdir ( dir = \"./tmp/ldref\" ) def extract ( self , inprefix : str , outprefix : str , chrom : int , temp_dir : Optional [ str ] = None , start : Optional [ int ] = None , end : Optional [ int ] = None , mac : int = 10 , ) -> None : \"\"\" Extract the genotypes of given region from the LD reference. Parameters ---------- inprefix : str The input prefix. outprefix : str The output prefix. chrom : int The chromosome number. temp_dir : str The temporary directory. start : int, optional The start position, by default None end : int, optional The end position, by default None mac: int, optional The minor allele count threshold, by default 10 Returns ------- None \"\"\" region_file = f \" { temp_dir } / { outprefix . split ( '/' )[ - 1 ] } .region\" if start is None : extract_cmd = [ \"--chr\" , str ( chrom )] else : with open ( region_file , \"w\" ) as f : f . write ( f \" { chrom } \\t { start } \\t { end } \\t region\" ) extract_cmd = [ \"--extract\" , \"range\" , region_file ] if \" {chrom} \" in inprefix : inprefix = inprefix . replace ( \" {chrom} \" , str ( chrom )) if not os . path . exists ( f \" { inprefix } .bed\" ): raise FileNotFoundError ( f \" { inprefix } .bed not found.\" ) cmd = [ self . plink , \"--bfile\" , inprefix , * extract_cmd , \"--keep-allele-order\" , \"--mac\" , str ( mac ), \"--make-bed\" , \"--out\" , outprefix , ] res = run ( cmd , stdout = PIPE , stderr = PIPE , universal_newlines = True ) self . logger . debug ( ' ' . join ( cmd )) self . logger . debug ( f \"extract chr { chrom } : { start } - { end } from { inprefix } \" ) if res . returncode != 0 : self . logger . error ( res . stderr ) self . logger . error ( f 'see log file: { outprefix } .log for details' ) raise RuntimeError ( res . stderr ) intersect ( sumstats , ldref , out_plink , use_ref_EAF = False , temp_dir = None ) \u00b6 Intersect the significant snps with the LD reference. Parameters: Name Type Description Default sumstats pd . DataFrame The summary statistics. required ldref str The path to the LD reference file. required out_plink str The output prefix. required use_ref_EAF bool , optional Use the EAF in the LD reference, by default False False temp_dir Optional [ str ], optional The path to the temporary directory, by default None None Returns: Type Description pd . DataFrame The intersected significant snps. Source code in easyfinemap/ldref.py 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 @io_in_tempdir ( dir = \"./tmp/ldref\" ) def intersect ( self , sumstats : pd . DataFrame , ldref : str , out_plink : str , use_ref_EAF : bool = False , temp_dir : Optional [ str ] = None , ) -> pd . DataFrame : \"\"\" Intersect the significant snps with the LD reference. Parameters ---------- sumstats : pd.DataFrame The summary statistics. ldref : str The path to the LD reference file. out_plink : str The output prefix. use_ref_EAF : bool, optional Use the EAF in the LD reference, by default False temp_dir : Optional[str], optional The path to the temporary directory, by default None Returns ------- pd.DataFrame The intersected significant snps. \"\"\" if not os . path . exists ( f \" { ldref } .bim\" ): raise FileNotFoundError ( f \" { ldref } .bim not found.\" ) sumstats [ ColName . SNPID ] . to_csv ( f \" { temp_dir } /overlap_snpid.txt\" , index = False , header = False ) cmd = [ self . plink , \"--bfile\" , ldref , \"--extract\" , f \" { temp_dir } /overlap_snpid.txt\" , \"--keep-allele-order\" , \"--make-bed\" , \"--out\" , out_plink , ] res = run ( cmd , stdout = PIPE , stderr = PIPE , universal_newlines = True ) self . logger . debug ( ' ' . join ( cmd )) self . logger . debug ( f \"intersect { sumstats . shape [ 0 ] } SNPs with { ldref } \" ) if res . returncode != 0 : self . logger . warning ( res . stderr ) self . logger . warning ( f 'see log file: { out_plink } .log for details' ) # raise RuntimeError(res.stderr) return pd . DataFrame () else : bim = pd . read_csv ( f \" { out_plink } .bim\" , delim_whitespace = True , names = [ ColName . CHR , ColName . RSID , \"cM\" , ColName . BP , ColName . EA , ColName . NEA ], ) overlap_sumstat = sumstats [ sumstats [ ColName . SNPID ] . isin ( bim [ ColName . RSID ])] . copy () overlap_sumstat . reset_index ( drop = True , inplace = True ) if use_ref_EAF : cmd = [ self . plink , \"--bfile\" , out_plink , \"--freq\" , \"--out\" , f \" { temp_dir } /freq\" , ] res = run ( cmd , stdout = PIPE , stderr = PIPE , universal_newlines = True ) self . logger . debug ( f \"calculate EAF of { out_plink } \" ) self . logger . debug ( f \"calculate EAF: { ' ' . join ( cmd ) } \" ) # if res.returncode != 0: # self.logger.error(res.stderr) # self.logger.error(f'see log file: {temp_dir}/freq.log for details') # raise RuntimeError(res.stderr) freq = pd . read_csv ( f \" { temp_dir } /freq.frq\" , delim_whitespace = True ) freq [ 'A2_frq' ] = 1 - freq [ 'MAF' ] overlap_sumstat [ 'EAF' ] = freq [ 'A2_frq' ] . where ( freq [ 'A2' ] == overlap_sumstat [ 'EA' ], freq [ 'MAF' ]) overlap_sumstat [ 'MAF' ] = freq [ 'MAF' ] return overlap_sumstat make_ld ( ldref , outprefix , ** kwargs ) \u00b6 Make the LD matrix. TODO: Calculate LD matrix using plink-pandas, because plink1.9 --ld contains bug. Parameters: Name Type Description Default ldref str The path to the LD reference file. required outprefix str The output prefix. required Raises: Type Description RuntimeError If the return code is not 0. Returns: Type Description None Source code in easyfinemap/ldref.py 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 @io_in_tempdir ( './tmp/ldref' ) def make_ld ( self , ldref : str , outprefix : str , ** kwargs , ): \"\"\" Make the LD matrix. TODO: Calculate LD matrix using plink-pandas, because plink1.9 --ld contains bug. Parameters ---------- ldref : str The path to the LD reference file. outprefix : str The output prefix. Raises ------ RuntimeError If the return code is not 0. Returns ------- None \"\"\" self . logger . info ( \"Making the LD matrix\" ) cmd = [ self . plink , \"--bfile\" , ldref , \"--r2\" , \"square\" , \"spaces\" , \"--threads\" , \"1\" , \"--out\" , outprefix , ] res = run ( cmd , stdout = PIPE , stderr = PIPE , universal_newlines = True ) self . logger . debug ( f \"get LD matrix: { ' ' . join ( cmd ) } \" ) if res . returncode != 0 : self . logger . error ( res . stderr ) raise RuntimeError ( res . stderr ) else : self . logger . debug ( \"LD matrix is made\" ) run ([ \"sed\" , \"-i\" , \"s/nan/1e-6/g\" , f \" { outprefix } .ld\" ]) valid ( ldref_path , outprefix , file_type = 'plink' , mac = 10 , threads = 1 , temp_dir = None ) \u00b6 Validate the LD reference file. TODO:1. format vcfs to plink files. 2. remove duplicated snps. 3. remove snps with MAC < mac. 4. make SNP names unique, chr-bp-sorted(EA,NEA). TODO:5. mark bim file with \"#easyfinemap validated\" flag in the first line. Parameters: Name Type Description Default ldref_path str The path to the LD reference file. required outprefix str The output prefix. required file_type str , optional The file type of the LD reference file, by default \"plink\" 'plink' mac int The minor allele count threshold, by default 10 SNPs with MAC < mac will be removed. 10 threads int , optional The number of threads to use, by default 1 1 temp_dir Optional [ str ], optional The path to the temporary directory, by default None None Raises: Type Description ValueError If the file type is not supported. Returns: Type Description None Source code in easyfinemap/ldref.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 @io_in_tempdir ( dir = './tmp/ldref' ) def valid ( self , ldref_path : str , outprefix : str , file_type : str = \"plink\" , mac : int = 10 , threads : int = 1 , temp_dir : Optional [ str ] = None , ) -> None : \"\"\" Validate the LD reference file. TODO:1. format vcfs to plink files. 2. remove duplicated snps. 3. remove snps with MAC < mac. 4. make SNP names unique, chr-bp-sorted(EA,NEA). TODO:5. mark bim file with \"#easyfinemap validated\" flag in the first line. Parameters ---------- ldref_path : str The path to the LD reference file. outprefix : str The output prefix. file_type : str, optional The file type of the LD reference file, by default \"plink\" mac: int, optional The minor allele count threshold, by default 10 SNPs with MAC < mac will be removed. threads : int, optional The number of threads to use, by default 1 temp_dir : Optional[str], optional The path to the temporary directory, by default None Raises ------ ValueError If the file type is not supported. Returns ------- None \"\"\" if file_type == \"plink\" : self . file_type = file_type else : raise ValueError ( f \"Unsupported file type: { file_type } \" ) params : List [ List [ Union [ str , int ]]] = [[] for _ in range ( 3 )] for chrom in CHROMS : if \" {chrom} \" in ldref_path : inprefix = ldref_path . replace ( \" {chrom} \" , str ( chrom )) if not os . path . exists ( f \" { inprefix } .bed\" ): if chrom == 23 : inprefix = ldref_path . replace ( \" {chrom} \" , \"X\" ) if os . path . exists ( f \" { inprefix } .bed\" ): self . logger . warning ( f \"chr { chrom } not found, use X instead.\" ) params [ 0 ] . append ( inprefix ) params [ 1 ] . append ( f \" { outprefix } .chr { chrom } \" ) params [ 2 ] . append ( mac ) else : self . logger . warning ( f \" { inprefix } .bed not found.\" ) else : self . logger . warning ( f \" { inprefix } .bed not found.\" ) continue else : params [ 0 ] . append ( inprefix ) params [ 1 ] . append ( f \" { outprefix } .chr { chrom } \" ) params [ 2 ] . append ( mac ) else : inprefix = ldref_path if not os . path . exists ( f \" { inprefix } .bed\" ): raise FileNotFoundError ( f \" { inprefix } .bed not found.\" ) else : # check if chrom is in the bim file res = check_output ( f 'grep \"^ { chrom } [[:space:]]\" { inprefix } .bim | head -n 1' , shell = True ) if len ( res . decode ()) == 0 : self . logger . warning ( f \"Chrom { chrom } not found in { inprefix } .bim\" ) continue else : intermed_prefix = f \" { temp_dir } / { outprefix . split ( '/' )[ - 1 ] } .chr { chrom } \" self . extract ( inprefix , intermed_prefix , chrom , mac = mac ) params [ 0 ] . append ( intermed_prefix ) params [ 1 ] . append ( f \" { outprefix } .chr { chrom } \" ) params [ 2 ] . append ( mac ) with Pool ( threads ) as p : p . map ( self . _clean_per_chr , * params )","title":"LDRef"},{"location":"api/LDRef/#easyfinemap.ldref.LDRef.annotate_r2","text":"Annotate SNPs with r2 to the lead SNP. Parameters: Name Type Description Default sumstat pd . DataFrame The summary statistics. required ldref str The path to the LD reference file. required ld_snp str The lead SNP. required temp_dir Optional [ str ], optional The path to the temporary directory, by default None None Returns: Type Description pd . DataFrame The annotated summary statistics. Source code in easyfinemap/ldref.py 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 @io_in_tempdir ( './tmp/ldref' ) def annotate_r2 ( self , sumstat : pd . DataFrame , ldref : str , ld_snp : str , temp_dir : Optional [ str ] = None , ) -> pd . DataFrame : \"\"\" Annotate SNPs with r2 to the lead SNP. Parameters ---------- sumstat : pd.DataFrame The summary statistics. ldref : str The path to the LD reference file. ld_snp : str The lead SNP. temp_dir : Optional[str], optional The path to the temporary directory, by default None Returns ------- pd.DataFrame The annotated summary statistics. \"\"\" if len ( sumstat [ ColName . CHR ] . unique ()) > 1 : raise ValueError ( \"Only one chromosome is allowed.\" ) chrom = sumstat [ ColName . CHR ] . iloc [ 0 ] if len ( sumstat ) > 100000 : self . logger . warning ( \"The sumstats is large, it may take a long time to annotate the r2.\" ) ld = LDRef () r2_df = sumstat . copy () r2_input = ld . intersect ( sumstat , ldref . format ( chrom = chrom ), f \" { temp_dir } /r2_input_ { chrom } \" ) if ld_snp not in r2_input [ ColName . SNPID ] . tolist (): raise ValueError ( f \" { ld_snp } not in the LD reference.\" ) cmd = [ self . plink , \"--bfile\" , f \" { temp_dir } /r2_input_ { chrom } \" , \"--r2\" , \"--ld-snp\" , ld_snp , \"--ld-window-kb\" , \"100000\" , \"--ld-window\" , \"99999999\" , \"--ld-window-r2\" , \"0\" , \"--keep-allele-order\" , \"--out\" , f \" { temp_dir } /r2_ { chrom } \" , ] self . logger . debug ( f \"annotate r2: { ' ' . join ( cmd ) } \" ) res = run ( cmd , stdout = PIPE , stderr = PIPE , universal_newlines = True ) if res . returncode != 0 : self . logger . error ( res . stderr ) raise RuntimeError ( res . stderr ) else : res_r2 = pd . read_csv ( f \" { temp_dir } /r2_ { chrom } .ld\" , delim_whitespace = True ) res_r2 = pd . Series ( res_r2 [ \"R2\" ] . values , index = res_r2 [ \"SNP_B\" ] . values ) r2_df [ \"R2\" ] = r2_df [ ColName . SNPID ] . map ( res_r2 ) r2_df . loc [ r2_df [ ColName . SNPID ] == ld_snp , \"R2\" ] = 1 r2_df [ 'R2' ] = r2_df [ 'R2' ] . fillna ( - 1 ) return r2_df","title":"annotate_r2()"},{"location":"api/LDRef/#easyfinemap.ldref.LDRef.cojo_cond","text":"Conditional analysis. Update the beta, se, pval of the conditional SNPs. Parameters: Name Type Description Default sumstats pd . DataFrame The summary statistics. required cond_snps pd . DataFrame The conditional SNPs. required ldref str The path to the LD reference file. required sample_size int The sample size. required use_ref_EAF bool , optional Whether to use the EAF in the LD reference file, by default False False temp_dir Optional [ str ], optional The path to the temporary directory, by default None None Raises: Type Description ValueError If the EAF is not in the sumstats and use_ref_EAF is False. Returns: Type Description pd . DataFrame The updated summary statistics. Source code in easyfinemap/ldref.py 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 @io_in_tempdir ( './tmp/ldref' ) def cojo_cond ( self , sumstats : pd . DataFrame , cond_snps : pd . DataFrame , ldref : str , sample_size : int , use_ref_EAF : bool = False , temp_dir : Optional [ str ] = None , ) -> pd . DataFrame : \"\"\" Conditional analysis. Update the beta, se, pval of the conditional SNPs. Parameters ---------- sumstats : pd.DataFrame The summary statistics. cond_snps : pd.DataFrame The conditional SNPs. ldref : str The path to the LD reference file. sample_size : int The sample size. use_ref_EAF : bool, optional Whether to use the EAF in the LD reference file, by default False temp_dir : Optional[str], optional The path to the temporary directory, by default None Raises ------ ValueError If the EAF is not in the sumstats and use_ref_EAF is False. Returns ------- pd.DataFrame The updated summary statistics. \"\"\" if not use_ref_EAF and ColName . EAF not in sumstats . columns : raise ValueError ( f \" { ColName . EAF } is not in the sumstats, please set use_ref_EAF to True\" ) chrom = sumstats [ ColName . CHR ] . iloc [ 0 ] ld = LDRef () all_sumstats = pd . concat ([ sumstats , cond_snps ], ignore_index = True ) all_sumstats . drop_duplicates ( subset = [ ColName . SNPID ], inplace = True ) all_sumstats . sort_values ( by = [ ColName . CHR , ColName . BP ], inplace = True ) all_sumstats . reset_index ( drop = True , inplace = True ) cojo_input = ld . intersect ( all_sumstats , ldref , f \" { temp_dir } /cojo_input_ { chrom } \" , use_ref_EAF ) cojo_input [ ColName . N ] = sample_size cojo_input = cojo_input [ [ ColName . SNPID , ColName . EA , ColName . NEA , ColName . EAF , ColName . BETA , ColName . SE , ColName . P , ColName . N ] ] cojo_input . rename ( columns = { ColName . SNPID : \"SNP\" , ColName . EA : \"A1\" , ColName . NEA : \"A2\" , ColName . EAF : \"freq\" , ColName . BETA : \"b\" , ColName . SE : \"se\" , ColName . P : \"p\" , ColName . N : \"N\" , }, inplace = True , ) cojo_p_file = f \" { temp_dir } /cojo_input_ { chrom } .ma\" cojo_input . to_csv ( cojo_p_file , sep = \" \" , index = False ) with open ( f \" { temp_dir } /cojo_cond_ { chrom } .snps\" , \"w\" ) as f : f . write ( ' \\n ' . join ( cond_snps [ ColName . SNPID ] . tolist ())) cojo_outfile = f \" { temp_dir } /cojo_ { chrom } .cond\" cmd = [ self . gcta , \"--bfile\" , ldref , \"--cojo-file\" , cojo_p_file , \"--cojo-cond\" , f \" { temp_dir } /cojo_cond_ { chrom } .snps\" , \"--out\" , cojo_outfile , ] res = run ( cmd , stdout = PIPE , stderr = PIPE , universal_newlines = True ) if res . returncode != 0 : self . logger . error ( res . stderr ) raise RuntimeError ( res . stderr ) else : cond_res = pd . read_csv ( f \" { cojo_outfile } .cma.cojo\" , sep = \" \\t \" , usecols = [ \"SNP\" , \"bC\" , \"bC_se\" , \"pC\" ]) cond_res . rename ( columns = { \"SNP\" : ColName . SNPID , \"bC\" : ColName . COJO_BETA , \"bC_se\" : ColName . COJO_SE , \"pC\" : ColName . COJO_P }, inplace = True , ) output = sumstats . merge ( cond_res , on = ColName . SNPID , how = \"left\" ) output = output . dropna ( subset = [ ColName . COJO_P , ColName . COJO_BETA , ColName . COJO_SE ]) return output","title":"cojo_cond()"},{"location":"api/LDRef/#easyfinemap.ldref.LDRef.extract","text":"Extract the genotypes of given region from the LD reference. Parameters: Name Type Description Default inprefix str The input prefix. required outprefix str The output prefix. required chrom int The chromosome number. required temp_dir str The temporary directory. None start int , optional The start position, by default None None end int , optional The end position, by default None None mac int The minor allele count threshold, by default 10 10 Returns: Type Description None Source code in easyfinemap/ldref.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 @io_in_tempdir ( dir = \"./tmp/ldref\" ) def extract ( self , inprefix : str , outprefix : str , chrom : int , temp_dir : Optional [ str ] = None , start : Optional [ int ] = None , end : Optional [ int ] = None , mac : int = 10 , ) -> None : \"\"\" Extract the genotypes of given region from the LD reference. Parameters ---------- inprefix : str The input prefix. outprefix : str The output prefix. chrom : int The chromosome number. temp_dir : str The temporary directory. start : int, optional The start position, by default None end : int, optional The end position, by default None mac: int, optional The minor allele count threshold, by default 10 Returns ------- None \"\"\" region_file = f \" { temp_dir } / { outprefix . split ( '/' )[ - 1 ] } .region\" if start is None : extract_cmd = [ \"--chr\" , str ( chrom )] else : with open ( region_file , \"w\" ) as f : f . write ( f \" { chrom } \\t { start } \\t { end } \\t region\" ) extract_cmd = [ \"--extract\" , \"range\" , region_file ] if \" {chrom} \" in inprefix : inprefix = inprefix . replace ( \" {chrom} \" , str ( chrom )) if not os . path . exists ( f \" { inprefix } .bed\" ): raise FileNotFoundError ( f \" { inprefix } .bed not found.\" ) cmd = [ self . plink , \"--bfile\" , inprefix , * extract_cmd , \"--keep-allele-order\" , \"--mac\" , str ( mac ), \"--make-bed\" , \"--out\" , outprefix , ] res = run ( cmd , stdout = PIPE , stderr = PIPE , universal_newlines = True ) self . logger . debug ( ' ' . join ( cmd )) self . logger . debug ( f \"extract chr { chrom } : { start } - { end } from { inprefix } \" ) if res . returncode != 0 : self . logger . error ( res . stderr ) self . logger . error ( f 'see log file: { outprefix } .log for details' ) raise RuntimeError ( res . stderr )","title":"extract()"},{"location":"api/LDRef/#easyfinemap.ldref.LDRef.intersect","text":"Intersect the significant snps with the LD reference. Parameters: Name Type Description Default sumstats pd . DataFrame The summary statistics. required ldref str The path to the LD reference file. required out_plink str The output prefix. required use_ref_EAF bool , optional Use the EAF in the LD reference, by default False False temp_dir Optional [ str ], optional The path to the temporary directory, by default None None Returns: Type Description pd . DataFrame The intersected significant snps. Source code in easyfinemap/ldref.py 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 @io_in_tempdir ( dir = \"./tmp/ldref\" ) def intersect ( self , sumstats : pd . DataFrame , ldref : str , out_plink : str , use_ref_EAF : bool = False , temp_dir : Optional [ str ] = None , ) -> pd . DataFrame : \"\"\" Intersect the significant snps with the LD reference. Parameters ---------- sumstats : pd.DataFrame The summary statistics. ldref : str The path to the LD reference file. out_plink : str The output prefix. use_ref_EAF : bool, optional Use the EAF in the LD reference, by default False temp_dir : Optional[str], optional The path to the temporary directory, by default None Returns ------- pd.DataFrame The intersected significant snps. \"\"\" if not os . path . exists ( f \" { ldref } .bim\" ): raise FileNotFoundError ( f \" { ldref } .bim not found.\" ) sumstats [ ColName . SNPID ] . to_csv ( f \" { temp_dir } /overlap_snpid.txt\" , index = False , header = False ) cmd = [ self . plink , \"--bfile\" , ldref , \"--extract\" , f \" { temp_dir } /overlap_snpid.txt\" , \"--keep-allele-order\" , \"--make-bed\" , \"--out\" , out_plink , ] res = run ( cmd , stdout = PIPE , stderr = PIPE , universal_newlines = True ) self . logger . debug ( ' ' . join ( cmd )) self . logger . debug ( f \"intersect { sumstats . shape [ 0 ] } SNPs with { ldref } \" ) if res . returncode != 0 : self . logger . warning ( res . stderr ) self . logger . warning ( f 'see log file: { out_plink } .log for details' ) # raise RuntimeError(res.stderr) return pd . DataFrame () else : bim = pd . read_csv ( f \" { out_plink } .bim\" , delim_whitespace = True , names = [ ColName . CHR , ColName . RSID , \"cM\" , ColName . BP , ColName . EA , ColName . NEA ], ) overlap_sumstat = sumstats [ sumstats [ ColName . SNPID ] . isin ( bim [ ColName . RSID ])] . copy () overlap_sumstat . reset_index ( drop = True , inplace = True ) if use_ref_EAF : cmd = [ self . plink , \"--bfile\" , out_plink , \"--freq\" , \"--out\" , f \" { temp_dir } /freq\" , ] res = run ( cmd , stdout = PIPE , stderr = PIPE , universal_newlines = True ) self . logger . debug ( f \"calculate EAF of { out_plink } \" ) self . logger . debug ( f \"calculate EAF: { ' ' . join ( cmd ) } \" ) # if res.returncode != 0: # self.logger.error(res.stderr) # self.logger.error(f'see log file: {temp_dir}/freq.log for details') # raise RuntimeError(res.stderr) freq = pd . read_csv ( f \" { temp_dir } /freq.frq\" , delim_whitespace = True ) freq [ 'A2_frq' ] = 1 - freq [ 'MAF' ] overlap_sumstat [ 'EAF' ] = freq [ 'A2_frq' ] . where ( freq [ 'A2' ] == overlap_sumstat [ 'EA' ], freq [ 'MAF' ]) overlap_sumstat [ 'MAF' ] = freq [ 'MAF' ] return overlap_sumstat","title":"intersect()"},{"location":"api/LDRef/#easyfinemap.ldref.LDRef.make_ld","text":"Make the LD matrix. TODO: Calculate LD matrix using plink-pandas, because plink1.9 --ld contains bug. Parameters: Name Type Description Default ldref str The path to the LD reference file. required outprefix str The output prefix. required Raises: Type Description RuntimeError If the return code is not 0. Returns: Type Description None Source code in easyfinemap/ldref.py 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 @io_in_tempdir ( './tmp/ldref' ) def make_ld ( self , ldref : str , outprefix : str , ** kwargs , ): \"\"\" Make the LD matrix. TODO: Calculate LD matrix using plink-pandas, because plink1.9 --ld contains bug. Parameters ---------- ldref : str The path to the LD reference file. outprefix : str The output prefix. Raises ------ RuntimeError If the return code is not 0. Returns ------- None \"\"\" self . logger . info ( \"Making the LD matrix\" ) cmd = [ self . plink , \"--bfile\" , ldref , \"--r2\" , \"square\" , \"spaces\" , \"--threads\" , \"1\" , \"--out\" , outprefix , ] res = run ( cmd , stdout = PIPE , stderr = PIPE , universal_newlines = True ) self . logger . debug ( f \"get LD matrix: { ' ' . join ( cmd ) } \" ) if res . returncode != 0 : self . logger . error ( res . stderr ) raise RuntimeError ( res . stderr ) else : self . logger . debug ( \"LD matrix is made\" ) run ([ \"sed\" , \"-i\" , \"s/nan/1e-6/g\" , f \" { outprefix } .ld\" ])","title":"make_ld()"},{"location":"api/LDRef/#easyfinemap.ldref.LDRef.valid","text":"Validate the LD reference file. TODO:1. format vcfs to plink files. 2. remove duplicated snps. 3. remove snps with MAC < mac. 4. make SNP names unique, chr-bp-sorted(EA,NEA). TODO:5. mark bim file with \"#easyfinemap validated\" flag in the first line. Parameters: Name Type Description Default ldref_path str The path to the LD reference file. required outprefix str The output prefix. required file_type str , optional The file type of the LD reference file, by default \"plink\" 'plink' mac int The minor allele count threshold, by default 10 SNPs with MAC < mac will be removed. 10 threads int , optional The number of threads to use, by default 1 1 temp_dir Optional [ str ], optional The path to the temporary directory, by default None None Raises: Type Description ValueError If the file type is not supported. Returns: Type Description None Source code in easyfinemap/ldref.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 @io_in_tempdir ( dir = './tmp/ldref' ) def valid ( self , ldref_path : str , outprefix : str , file_type : str = \"plink\" , mac : int = 10 , threads : int = 1 , temp_dir : Optional [ str ] = None , ) -> None : \"\"\" Validate the LD reference file. TODO:1. format vcfs to plink files. 2. remove duplicated snps. 3. remove snps with MAC < mac. 4. make SNP names unique, chr-bp-sorted(EA,NEA). TODO:5. mark bim file with \"#easyfinemap validated\" flag in the first line. Parameters ---------- ldref_path : str The path to the LD reference file. outprefix : str The output prefix. file_type : str, optional The file type of the LD reference file, by default \"plink\" mac: int, optional The minor allele count threshold, by default 10 SNPs with MAC < mac will be removed. threads : int, optional The number of threads to use, by default 1 temp_dir : Optional[str], optional The path to the temporary directory, by default None Raises ------ ValueError If the file type is not supported. Returns ------- None \"\"\" if file_type == \"plink\" : self . file_type = file_type else : raise ValueError ( f \"Unsupported file type: { file_type } \" ) params : List [ List [ Union [ str , int ]]] = [[] for _ in range ( 3 )] for chrom in CHROMS : if \" {chrom} \" in ldref_path : inprefix = ldref_path . replace ( \" {chrom} \" , str ( chrom )) if not os . path . exists ( f \" { inprefix } .bed\" ): if chrom == 23 : inprefix = ldref_path . replace ( \" {chrom} \" , \"X\" ) if os . path . exists ( f \" { inprefix } .bed\" ): self . logger . warning ( f \"chr { chrom } not found, use X instead.\" ) params [ 0 ] . append ( inprefix ) params [ 1 ] . append ( f \" { outprefix } .chr { chrom } \" ) params [ 2 ] . append ( mac ) else : self . logger . warning ( f \" { inprefix } .bed not found.\" ) else : self . logger . warning ( f \" { inprefix } .bed not found.\" ) continue else : params [ 0 ] . append ( inprefix ) params [ 1 ] . append ( f \" { outprefix } .chr { chrom } \" ) params [ 2 ] . append ( mac ) else : inprefix = ldref_path if not os . path . exists ( f \" { inprefix } .bed\" ): raise FileNotFoundError ( f \" { inprefix } .bed not found.\" ) else : # check if chrom is in the bim file res = check_output ( f 'grep \"^ { chrom } [[:space:]]\" { inprefix } .bim | head -n 1' , shell = True ) if len ( res . decode ()) == 0 : self . logger . warning ( f \"Chrom { chrom } not found in { inprefix } .bim\" ) continue else : intermed_prefix = f \" { temp_dir } / { outprefix . split ( '/' )[ - 1 ] } .chr { chrom } \" self . extract ( inprefix , intermed_prefix , chrom , mac = mac ) params [ 0 ] . append ( intermed_prefix ) params [ 1 ] . append ( f \" { outprefix } .chr { chrom } \" ) params [ 2 ] . append ( mac ) with Pool ( threads ) as p : p . map ( self . _clean_per_chr , * params )","title":"valid()"},{"location":"api/Loci/","text":"Identify the independent loci. Source code in easyfinemap/loci.py 32 33 34 35 36 37 38 39 def __init__ ( self ): \"\"\"Initialize the Loci class.\"\"\" self . logger = logging . getLogger ( \"Loci\" ) self . plink = Tools () . plink self . gcta = Tools () . gcta self . tmp_root = Path . cwd () / \"tmp\" / \"loci\" if not self . tmp_root . exists (): self . tmp_root . mkdir ( parents = True ) clump_per_chr ( sig_df , ldref , clump_p1 , clump_kb , clump_r2 , temp_dir = None ) \u00b6 LD clumping per chromosome. Parameters: Name Type Description Default sig_df pd . DataFrame The significant snps. required ldref str The LD reference file, (plink bfile format, containing wildcard {chrom}), e.g. EUR.chr{chrom}. required clump_p1 float The p1 threshold. required clump_kb int The kb threshold. required clump_r2 float The r2 threshold. required temp_dir Optional [ str ], optional The temporary directory, by default None None Returns: Type Description pd . DataFrame The clumped snps. Source code in easyfinemap/loci.py 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 @io_in_tempdir ( dir = \"./tmp/loci\" ) def clump_per_chr ( self , sig_df : pd . DataFrame , ldref : str , clump_p1 : float , clump_kb : int , clump_r2 : float , temp_dir : Optional [ str ] = None , ) -> pd . DataFrame : \"\"\" LD clumping per chromosome. Parameters ---------- sig_df : pd.DataFrame The significant snps. ldref : str The LD reference file, (plink bfile format, containing wildcard {chrom}), e.g. EUR.chr{chrom}. clump_p1 : float The p1 threshold. clump_kb : int The kb threshold. clump_r2 : float The r2 threshold. temp_dir : Optional[str], optional The temporary directory, by default None Returns ------- pd.DataFrame The clumped snps. \"\"\" chrom = sig_df [ ColName . CHR ] . unique ()[ 0 ] clump_p_file = f \" { temp_dir } /clump_p_ { chrom } .txt\" sig_df [[ ColName . SNPID , ColName . P ]] . to_csv ( clump_p_file , sep = \" \\t \" , index = False ) clump_outfile = f \" { temp_dir } /clump_ { chrom } .clumped\" cmd = [ self . plink , \"--bfile\" , ldref . format ( chrom = chrom ), \"--clump\" , clump_p_file , \"--clump-p1\" , str ( clump_p1 ), \"--clump-kb\" , str ( clump_kb ), \"--clump-r2\" , str ( clump_r2 ), \"--clump-snp-field\" , ColName . SNPID , \"--clump-field\" , ColName . P , \"--out\" , f \" { temp_dir } /clump_ { chrom } \" , ] res = run ( cmd , stdout = PIPE , stderr = PIPE , universal_newlines = True ) if res . returncode != 0 : self . logger . error ( res . stderr ) raise RuntimeError ( res . stderr ) else : if os . path . exists ( clump_outfile ): clump_snps = pd . read_csv ( clump_outfile , delim_whitespace = True , usecols = [ \"SNP\" ]) clump_snps = clump_snps [ \"SNP\" ] . to_list () clump_snps = sig_df [ sig_df [ ColName . SNPID ] . isin ( clump_snps )] return clump_snps else : logging . warning ( f \"No clumped snps found for chromosome { chrom } \" ) return pd . DataFrame () cojo_slct ( sumstats , ldref , sample_size , cojo_window_kb = 10000 , cojo_collinear = 0.9 , diff_freq = 0.2 , sig_threshold = 5e-08 , use_ref_EAF = False , temp_dir = None ) \u00b6 Conditional analysis for input sumstatistics. Parameters: Name Type Description Default sumstats pd . DataFrame The input sumstatistics, from same chromosome or locus. required ldref str The LD reference file, (plink bfile format, containing wildcard {chrom}), e.g. EUR.chr{chrom}. required sample_size int The sample size of the input sumstatistics. required cojo_window_kb int , optional The cojo window, by default 10000, unit: kb 10000 cojo_collinear float , optional The cojo collinear, by default 0.9 0.9 diff_freq float , optional The difference frequency, by default 0.2 0.2 sig_threshold float , optional The significance threshold, by default 5e-8 5e-08 use_ref_EAF bool , optional Whether to use the reference EAF, by default False False temp_dir Optional [ str ], optional The temporary directory, by default None None Returns: Type Description pd . DataFrame The conditional snps. Source code in easyfinemap/loci.py 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 @io_in_tempdir ( dir = \"./tmp/loci\" ) def cojo_slct ( self , sumstats : pd . DataFrame , ldref : str , sample_size : int , cojo_window_kb : int = 10000 , cojo_collinear : float = 0.9 , diff_freq : float = 0.2 , sig_threshold : float = 5e-8 , use_ref_EAF : bool = False , temp_dir : Optional [ str ] = None , ) -> pd . DataFrame : \"\"\" Conditional analysis for input sumstatistics. Parameters ---------- sumstats : pd.DataFrame The input sumstatistics, from same chromosome or locus. ldref : str The LD reference file, (plink bfile format, containing wildcard {chrom}), e.g. EUR.chr{chrom}. sample_size : int The sample size of the input sumstatistics. cojo_window_kb : int, optional The cojo window, by default 10000, unit: kb cojo_collinear : float, optional The cojo collinear, by default 0.9 diff_freq : float, optional The difference frequency, by default 0.2 sig_threshold : float, optional The significance threshold, by default 5e-8 use_ref_EAF : bool, optional Whether to use the reference EAF, by default False temp_dir : Optional[str], optional The temporary directory, by default None Returns ------- pd.DataFrame The conditional snps. \"\"\" chrom = sumstats [ ColName . CHR ] . unique ()[ 0 ] cojo_input = sumstats . copy () ld = LDRef () cojo_input = ld . intersect ( sumstats , ldref , f \" { temp_dir } /cojo_input_ { chrom } \" , use_ref_EAF ) if cojo_input . empty : self . logger . warning ( f \"No SNPs in LD reference for chromosome { temp_dir } /cojo_input_ { chrom } \" ) self . logger . warning ( \"Use the most significant SNP as the independent lead SNP.\" ) cojo_snps = sumstats . loc [ sumstats [ ColName . P ] == sumstats [ ColName . P ] . min ()] . copy () else : cojo_input [ ColName . N ] = sample_size cojo_input = cojo_input [ [ ColName . SNPID , ColName . EA , ColName . NEA , ColName . EAF , ColName . BETA , ColName . SE , ColName . P , ColName . N ] ] cojo_input . rename ( columns = { ColName . SNPID : \"SNP\" , ColName . EA : \"A1\" , ColName . NEA : \"A2\" , ColName . EAF : \"freq\" , ColName . BETA : \"b\" , ColName . SE : \"se\" , ColName . P : \"p\" , ColName . N : \"N\" , }, inplace = True , ) cojo_p_file = f \" { temp_dir } /cojo_input_ { chrom } .ma\" cojo_input . to_csv ( cojo_p_file , sep = \" \" , index = False ) cojo_outfile = f \" { temp_dir } /cojo_ { chrom } .slct\" cmd = [ self . gcta , \"--bfile\" , f \" { temp_dir } /cojo_input_ { chrom } \" , \"--cojo-file\" , cojo_p_file , \"--cojo-slct\" , \"--cojo-p\" , str ( sig_threshold ), \"--cojo-wind\" , str ( cojo_window_kb ), \"--cojo-collinear\" , str ( cojo_collinear ), \"--diff-freq\" , str ( diff_freq ), \"--out\" , cojo_outfile , ] self . logger . debug ( f \"Run cojo-slct: { ' ' . join ( cmd ) } \" ) res = run ( cmd , stdout = PIPE , stderr = PIPE , universal_newlines = True ) if res . returncode != 0 : self . logger . warning ( res . stderr ) self . logger . warning ( f \"Run cojo-slct failed for chromosome { chrom } \" ) self . logger . warning ( \"Use the most significant SNP as the independent lead SNP.\" ) cojo_snps = sumstats . loc [ sumstats [ ColName . P ] == sumstats [ ColName . P ] . min ()] . copy () else : if os . path . exists ( f \" { cojo_outfile } .jma.cojo\" ): cojo_snps = pd . read_csv ( f \" { cojo_outfile } .jma.cojo\" , delim_whitespace = True , usecols = [ \"SNP\" , \"bJ\" , \"bJ_se\" , \"pJ\" ] ) cojo_snps . rename ( columns = { \"SNP\" : ColName . SNPID , \"bJ\" : ColName . COJO_BETA , \"bJ_se\" : ColName . COJO_SE , \"pJ\" : ColName . COJO_P , }, inplace = True , ) cojo_snps = cojo_snps [ cojo_snps [ ColName . COJO_P ] <= sig_threshold ] cojo_snps = sumstats . merge ( cojo_snps , on = ColName . SNPID , how = \"inner\" ) else : self . logger . warning ( f \"No conditional snps found for chromosome { chrom } \" ) self . logger . warning ( \"Use the most significant SNP as the independent lead SNP.\" ) cojo_snps = sumstats . loc [ sumstats [ ColName . P ] == sumstats [ ColName . P ] . min ()] . copy () return cojo_snps identify_indep_loci ( sumstats , sig_threshold = 5e-08 , loci_extend = 500 , ldblock = None , if_merge = False , outprefix = None , ldref = None , method = 'distance' , distance = 500 , clump_kb = 500 , clump_r2 = 0.1 , sample_size = None , cojo_window_kb = 10000 , cojo_collinear = 0.9 , diff_freq = 0.2 , only_use_sig_snps = False , use_ref_EAF = False , threads = 1 ) \u00b6 Identify the independent loci. Parameters: Name Type Description Default sumstats pd . DataFrame The input summary statistics. required sig_threshold float , optional The pvalue threshold, by default 5e-8 5e-08 loci_extend int , optional The range to extend the independent lead snps to independent loci, by default 500, unit: kb 500 if_merge bool , optional Whether to merge the overlapped independent loci, by default False False ldref Optional [ str ], optional The LD reference file, by default None None method str , optional The method to identify the independent loci, by default \"distance\", choose from [\"distance\", \"clumping\", \"conditional\"] 'distance' distance int , optional The distance threshold to identify the independent loci, by default 500, unit: kb 500 clump_kb int , optional The distance threshold for LD clumping, by default 10000, unit: kb 500 clump_r2 float , optional The r2 threshold for LD clumping, by default 0.1 0.1 sample_size Optional [ int ], optional The sample size for conditional analysis, by default None None cojo_window_kb int , optional The distance threshold for conditional analysis, by default 10000 10000 cojo_collinear float , optional The collinear threshold for conditional analysis, by default 0.9 0.9 diff_freq float , optional The difference frequency threshold for conditional analysis, by default 0.2 0.2 only_use_sig_snps bool , optional Whether to use the significant snps for conditional analysis, by default False False use_ref_EAF bool , optional Whether to use the reference EAF for conditional analysis, by default False False threads int , optional The number of threads, by default 1 1 Returns: Type Description Tuple [ pd . DataFrame , pd . DataFrame ] The independent lead snps and independent loci. Source code in easyfinemap/loci.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def identify_indep_loci ( self , sumstats : pd . DataFrame , sig_threshold : float = 5e-8 , loci_extend : int = 500 , ldblock : Optional [ str ] = None , if_merge : bool = False , outprefix : Optional [ str ] = None , ldref : Optional [ str ] = None , method : str = \"distance\" , distance : int = 500 , clump_kb : int = 500 , clump_r2 : float = 0.1 , sample_size : Optional [ int ] = None , cojo_window_kb : int = 10000 , cojo_collinear : float = 0.9 , diff_freq : float = 0.2 , only_use_sig_snps : bool = False , use_ref_EAF : bool = False , threads : int = 1 , ) -> Union [ Tuple [ pd . DataFrame , pd . DataFrame ], None ]: \"\"\" Identify the independent loci. Parameters ---------- sumstats : pd.DataFrame The input summary statistics. sig_threshold : float, optional The pvalue threshold, by default 5e-8 loci_extend : int, optional The range to extend the independent lead snps to independent loci, by default 500, unit: kb if_merge : bool, optional Whether to merge the overlapped independent loci, by default False ldref : Optional[str], optional The LD reference file, by default None method : str, optional The method to identify the independent loci, by default \"distance\", choose from [\"distance\", \"clumping\", \"conditional\"] distance : int, optional The distance threshold to identify the independent loci, by default 500, unit: kb clump_kb : int, optional The distance threshold for LD clumping, by default 10000, unit: kb clump_r2 : float, optional The r2 threshold for LD clumping, by default 0.1 sample_size : Optional[int], optional The sample size for conditional analysis, by default None cojo_window_kb : int, optional The distance threshold for conditional analysis, by default 10000 cojo_collinear : float, optional The collinear threshold for conditional analysis, by default 0.9 diff_freq : float, optional The difference frequency threshold for conditional analysis, by default 0.2 only_use_sig_snps : bool, optional Whether to use the significant snps for conditional analysis, by default False use_ref_EAF : bool, optional Whether to use the reference EAF for conditional analysis, by default False threads : int, optional The number of threads, by default 1 Returns ------- Tuple[pd.DataFrame, pd.DataFrame] The independent lead snps and independent loci. \"\"\" sumstats = make_SNPID_unique ( sumstats ) if ldblock is not None : ldblock = pd . read_csv ( ldblock , sep = \" \\t \" , names = [ ColName . CHR , ColName . START , ColName . END ]) if method == \"distance\" : sig_df = get_significant_snps ( sumstats , sig_threshold ) lead_snp = self . indep_snps_by_distance ( sig_df , distance , ldblock ) elif method == \"clumping\" : clump_p1 = sig_threshold if ldref is not None : sig_df = get_significant_snps ( sumstats , sig_threshold ) lead_snp = self . indep_snps_by_ldclumping ( sig_df , ldref , clump_p1 , clump_kb , clump_r2 ) else : raise ValueError ( f \"Please provide the ldref file for method: { method } \" ) elif method == \"conditional\" : if ldref is None : raise ValueError ( \"Please provide the ldref file for conditional analysis.\" ) if sample_size is None : raise ValueError ( \"Please provide the sample size for conditional analysis.\" ) else : lead_snp = self . indep_snps_by_conditional ( sumstats , ldref , sample_size , sig_threshold , cojo_window_kb , cojo_collinear , diff_freq , use_ref_EAF , only_use_sig_snps , ldblock , threads , ) else : raise ValueError ( f \"Unsupported method: { method } \" ) loci = self . leadsnp2loci ( lead_snp , loci_extend , if_merge , ldblock ) if if_merge and ColName . COJO_BETA in lead_snp . columns : self . logger . warning ( \"The loci identified by cojo may not need merge.\" ) lead_snp = lead_snp [ lead_snp [ ColName . SNPID ] . isin ( loci [ ColName . LEAD_SNP ])] if outprefix : loci_file = f \" { outprefix } .loci.txt\" loci . to_csv ( loci_file , sep = \" \\t \" , index = False , float_format = \" %.6g \" ) self . logger . info ( f \"Save { len ( loci ) } independent loci to { loci_file } \" ) leadsnp_file = f \" { outprefix } .leadsnp.txt\" lead_snp . to_csv ( leadsnp_file , sep = \" \\t \" , index = False , float_format = \" %.6g \" ) self . logger . info ( f \"Save { len ( lead_snp ) } independent lead snps to { leadsnp_file } \" ) return lead_snp , loci indep_snps_by_conditional ( sumstats , ldref , sample_size , sig_threshold = 5e-08 , cojo_window_kb = 10000 , cojo_collinear = 0.9 , diff_freq = 0.2 , use_ref_EAF = False , only_use_sig_snps = False , ldblock = None , threads = 1 ) staticmethod \u00b6 Identify the independent snps by conditional analysis. Parameters: Name Type Description Default sumstats pd . DataFrame The summary statistics. required ldref str The LD reference file, (plink bfile format, containing wildcard {chrom}), e.g. EUR.chr{chrom}. required sample_size int The sample size. required sig_threshold float , optional The significance threshold, by default 5e-8 5e-08 cojo_window_kb int , optional The cojo window, by default 10000, in kb 10000 cojo_collinear float , optional The cojo collinear, by default 0.9 0.9 diff_freq float , optional The difference frequency, by default 0.2 0.2 use_ref_EAF bool , optional Whether to use the reference EAF, by default False False only_use_sig_snps bool , optional Whether to only use the significant snps, by default False False ldblock Optional [ pd . DataFrame ], optional The LD block, run cojo in each LD block, by default None None threads int , optional The number of threads, by default 1 1 Source code in easyfinemap/loci.py 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 @staticmethod def indep_snps_by_conditional ( sumstats : pd . DataFrame , ldref : str , sample_size : int , sig_threshold : float = 5e-8 , cojo_window_kb : int = 10000 , cojo_collinear : float = 0.9 , diff_freq : float = 0.2 , use_ref_EAF : bool = False , only_use_sig_snps : bool = False , ldblock : Optional [ pd . DataFrame ] = None , threads : int = 1 , ) -> pd . DataFrame : \"\"\" Identify the independent snps by conditional analysis. Parameters ---------- sumstats : pd.DataFrame The summary statistics. ldref : str The LD reference file, (plink bfile format, containing wildcard {chrom}), e.g. EUR.chr{chrom}. sample_size : int The sample size. sig_threshold : float, optional The significance threshold, by default 5e-8 cojo_window_kb : int, optional The cojo window, by default 10000, in kb cojo_collinear : float, optional The cojo collinear, by default 0.9 diff_freq : float, optional The difference frequency, by default 0.2 use_ref_EAF : bool, optional Whether to use the reference EAF, by default False only_use_sig_snps : bool, optional Whether to only use the significant snps, by default False ldblock : Optional[pd.DataFrame], optional The LD block, run cojo in each LD block, by default None threads : int, optional The number of threads, by default 1 \"\"\" logger = logging . getLogger ( 'COJO' ) if not use_ref_EAF and ColName . EAF not in sumstats . columns : raise ValueError ( f \" { ColName . EAF } is not in the sumstats, please set use_ref_EAF to True\" ) sig_df = sumstats [ sumstats [ ColName . P ] <= sig_threshold ] logger . debug ( f \"Number of significant snps: { len ( sig_df ) } \" ) logger . debug ( f \"Number of chromosomes: { len ( sig_df [ ColName . CHR ] . unique ()) } \" ) args_list = [] loci = Loci () if ldblock is not None : sig_blocks = loci . indep_snps_by_distance ( sig_df , ldblock = ldblock ) sig_blocks = loci . leadsnp2loci ( sig_blocks , ldblock = ldblock ) for i in sig_blocks . index : if only_use_sig_snps : in_df = sig_df [ ( sig_df [ ColName . CHR ] == sig_blocks . loc [ i ][ ColName . CHR ]) & ( sig_df [ ColName . BP ] >= sig_blocks . loc [ i ][ ColName . START ]) & ( sig_df [ ColName . BP ] <= sig_blocks . loc [ i ][ ColName . END ]) ] else : in_df = sumstats [ ( sumstats [ ColName . CHR ] == sig_blocks . loc [ i ][ ColName . CHR ]) & ( sumstats [ ColName . BP ] >= sig_blocks . loc [ i ][ ColName . START ]) & ( sumstats [ ColName . BP ] <= sig_blocks . loc [ i ][ ColName . END ]) ] args_list . append ( ( in_df , ldref . format ( chrom = sig_blocks . loc [ i ][ ColName . CHR ]), sample_size , cojo_window_kb , cojo_collinear , diff_freq , sig_threshold , use_ref_EAF , ) ) else : for chrom in sig_df [ ColName . CHR ] . unique (): if only_use_sig_snps : in_df = sig_df [ sig_df [ ColName . CHR ] == chrom ] else : in_df = sumstats [ sumstats [ ColName . CHR ] == chrom ] args_list . append ( ( in_df , ldref . format ( chrom = chrom ), sample_size , cojo_window_kb , cojo_collinear , diff_freq , sig_threshold , use_ref_EAF , ) ) with ProcessPoolExecutor ( max_workers = threads ) as executor : results = [] with Progress ( TextColumn ( \" {task.description} \" ), BarColumn (), MofNCompleteColumn (), TimeElapsedColumn (), auto_refresh = True , ) as progress : task = progress . add_task ( \"Run cojo-slct\" , total = len ( args_list )) for _ in executor . map ( loci . cojo_slct , * zip ( * args_list )): progress . update ( task , advance = 1 ) progress . refresh () results . append ( _ ) cojo_snps = pd . concat ( results , axis = 0 , ignore_index = True ) return cojo_snps indep_snps_by_distance ( sig_df , distance = 500 , ldblock = None ) staticmethod \u00b6 Identify the independent snps by distance only. Parameters: Name Type Description Default sig_df pd . DataFrame The significant snps. required distance int , optional The distance threshold, by default 500, unit: kb 500 ldblock Optional [ pd . DataFrame ], optional The ld block information, use boundary to identify the independent snps, by default None None Returns: Type Description pd . DataFrame The independent snps. Source code in easyfinemap/loci.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 @staticmethod def indep_snps_by_distance ( sig_df : pd . DataFrame , distance : int = 500 , ldblock : Optional [ pd . DataFrame ] = None ) -> pd . DataFrame : \"\"\" Identify the independent snps by distance only. Parameters ---------- sig_df : pd.DataFrame The significant snps. distance : int, optional The distance threshold, by default 500, unit: kb ldblock : Optional[pd.DataFrame], optional The ld block information, use boundary to identify the independent snps, by default None Returns ------- pd.DataFrame The independent snps. \"\"\" sig_df = sig_df . sort_values ( ColName . P ) . copy () lead_snp = [] if ldblock is not None : while len ( sig_df ): lead_snp . append ( sig_df . iloc [[ 0 ]]) sig_block = ldblock [ ( ldblock [ ColName . CHR ] == sig_df . iloc [ 0 ][ ColName . CHR ]) & ( ldblock [ ColName . START ] <= sig_df . iloc [ 0 ][ ColName . BP ]) & ( ldblock [ ColName . END ] >= sig_df . iloc [ 0 ][ ColName . BP ]) ] sig_df = sig_df [ ~ ( ( sig_df [ ColName . CHR ] == sig_df . iloc [ 0 ][ ColName . CHR ]) & ( sig_df [ ColName . BP ] >= sig_block . iloc [ 0 ][ ColName . START ]) & ( sig_df [ ColName . BP ] <= sig_block . iloc [ 0 ][ ColName . END ]) ) ] else : distance = distance * 1000 while len ( sig_df ): lead_snp . append ( sig_df . iloc [[ 0 ]]) sig_df = sig_df [ ~ ( ( sig_df [ ColName . CHR ] == sig_df . iloc [ 0 ][ ColName . CHR ]) & ( sig_df [ ColName . BP ] >= sig_df . iloc [ 0 ][ ColName . BP ] - distance ) & ( sig_df [ ColName . BP ] <= sig_df . iloc [ 0 ][ ColName . BP ] + distance ) ) ] # type: ignore lead_snp = pd . concat ( lead_snp , axis = 0 , ignore_index = True ) return lead_snp indep_snps_by_ldclumping ( sig_df , ldref , clump_p1 = 5e-08 , clump_kb = 500 , clump_r2 = 0.1 ) staticmethod \u00b6 Identify the independent snps by LD clumping. Parameters: Name Type Description Default sig_df pd . DataFrame The significant snps. required ldref str The LD reference file, (plink bfile format, containing wildcard {chrom}), e.g. EUR.chr{chrom}. required clump_p1 float , optional The p1 threshold, by default 5e-8 5e-08 clump_kb int , optional The kb threshold, by default 500, unit: kb 500 clump_r2 float , optional The r2 threshold, by default 0.1 0.1 Returns: Type Description pd . DataFrame Source code in easyfinemap/loci.py 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 @staticmethod def indep_snps_by_ldclumping ( sig_df : pd . DataFrame , ldref : str , clump_p1 : float = 5e-8 , clump_kb : int = 500 , clump_r2 : float = 0.1 ) -> pd . DataFrame : \"\"\" Identify the independent snps by LD clumping. Parameters ---------- sig_df : pd.DataFrame The significant snps. ldref : str The LD reference file, (plink bfile format, containing wildcard {chrom}), e.g. EUR.chr{chrom}. clump_p1 : float, optional The p1 threshold, by default 5e-8 clump_kb : int, optional The kb threshold, by default 500, unit: kb clump_r2 : float, optional The r2 threshold, by default 0.1 Returns ------- pd.DataFrame \"\"\" clumped_snps = [] for chrom in sig_df [ ColName . CHR ] . unique (): sig_df_chr = sig_df [ sig_df [ ColName . CHR ] == chrom ] clumped_snps . append ( Loci () . clump_per_chr ( sig_df_chr , ldref , clump_p1 , clump_kb , clump_r2 )) # type: ignore clumped_snps = pd . concat ( clumped_snps , axis = 0 , ignore_index = True ) return clumped_snps leadsnp2loci ( lead_snps , range = 500 , if_merge = False , ldblock = None ) staticmethod \u00b6 Expand the independent lead snps to independent loci by given range. Parameters: Name Type Description Default lead_snps pd . DataFrame The independent lead snps. required range int , optional The range, by default 500, unit: kb 500 if_merge bool , optional Whether merge the overlapped loci, by default False False ldblock Optional [ pd . DataFrame ], optional The ld block, using LD block to expand the independent loci, by default None None Returns: Type Description pd . DataFrame The independent loci. Source code in easyfinemap/loci.py 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 @staticmethod def leadsnp2loci ( lead_snps : pd . DataFrame , range : int = 500 , if_merge : bool = False , ldblock : Optional [ pd . DataFrame ] = None ) -> pd . DataFrame : \"\"\" Expand the independent lead snps to independent loci by given range. Parameters ---------- lead_snps : pd.DataFrame The independent lead snps. range : int, optional The range, by default 500, unit: kb if_merge : bool, optional Whether merge the overlapped loci, by default False ldblock : Optional[pd.DataFrame], optional The ld block, using LD block to expand the independent loci, by default None Returns ------- pd.DataFrame The independent loci. \"\"\" loci_df = lead_snps . copy () loci_df = loci_df [[ ColName . CHR , ColName . BP , ColName . P , ColName . SNPID ]] loci_df . columns = [ ColName . CHR , ColName . LEAD_SNP_BP , ColName . LEAD_SNP_P , ColName . LEAD_SNP ] # type: ignore if ldblock is not None : loci_df [ ColName . START ] = 0 loci_df [ ColName . END ] = 0 for i in loci_df . index : sub_ldblock = ldblock [ ( ldblock [ ColName . CHR ] == loci_df . loc [ i , ColName . CHR ]) & ( ldblock [ ColName . START ] <= loci_df . loc [ i , ColName . LEAD_SNP_BP ]) & ( ldblock [ ColName . END ] >= loci_df . loc [ i , ColName . LEAD_SNP_BP ]) ] if sub_ldblock . empty : continue else : loci_df . loc [ i , ColName . START ] = sub_ldblock . iloc [ 0 ][ ColName . START ] loci_df . loc [ i , ColName . END ] = sub_ldblock . iloc [ 0 ][ ColName . END ] else : range = range * 1000 loci_df [ ColName . START ] = loci_df [ ColName . LEAD_SNP_BP ] - range loci_df [ ColName . START ] = loci_df [ ColName . START ] . apply ( lambda x : 0 if x < 0 else x ) loci_df [ ColName . END ] = loci_df [ ColName . LEAD_SNP_BP ] + range loci_df = loci_df [ ColName . loci_cols ] . copy () if if_merge : loci_df = Loci . merge_overlapped_loci ( loci_df ) loci_df = loci_df . sort_values ( by = [ ColName . CHR , ColName . START , ColName . END ]) return loci_df merge_overlapped_loci ( loci_df ) staticmethod \u00b6 Merge the overlapped loci. Parameters: Name Type Description Default loci_df pd . DataFrame The independent loci. required Returns: Type Description pd . DataFrame The merged independent loci. Source code in easyfinemap/loci.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 @staticmethod def merge_overlapped_loci ( loci_df : pd . DataFrame ): \"\"\" Merge the overlapped loci. Parameters ---------- loci_df : pd.DataFrame The independent loci. Returns ------- pd.DataFrame The merged independent loci. \"\"\" merged_loci = loci_df . copy () merged_loci . sort_values ([ ColName . CHR , ColName . START , ColName . END ], inplace = True ) merged_loci [ 'no_overlap' ] = merged_loci [ ColName . START ] > merged_loci [ ColName . END ] . shift () . cummax () merged_loci [ 'diff_chr' ] = merged_loci [ ColName . CHR ] != merged_loci [ ColName . CHR ] . shift () merged_loci [ \"break\" ] = merged_loci [ \"no_overlap\" ] | merged_loci [ 'diff_chr' ] merged_loci [ 'group' ] = merged_loci [ 'break' ] . cumsum () merged_loci = merged_loci . sort_values ([ 'group' , ColName . LEAD_SNP_P ], ascending = True ) agg_func = {} for col in loci_df . columns : if col == ColName . START : agg_func [ col ] = 'min' elif col == ColName . END : agg_func [ col ] = 'max' else : agg_func [ col ] = 'first' result = merged_loci . groupby ( \"group\" ) . agg ( agg_func ) result . reset_index ( drop = True , inplace = True ) return result","title":"Loci"},{"location":"api/Loci/#easyfinemap.loci.Loci.clump_per_chr","text":"LD clumping per chromosome. Parameters: Name Type Description Default sig_df pd . DataFrame The significant snps. required ldref str The LD reference file, (plink bfile format, containing wildcard {chrom}), e.g. EUR.chr{chrom}. required clump_p1 float The p1 threshold. required clump_kb int The kb threshold. required clump_r2 float The r2 threshold. required temp_dir Optional [ str ], optional The temporary directory, by default None None Returns: Type Description pd . DataFrame The clumped snps. Source code in easyfinemap/loci.py 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 @io_in_tempdir ( dir = \"./tmp/loci\" ) def clump_per_chr ( self , sig_df : pd . DataFrame , ldref : str , clump_p1 : float , clump_kb : int , clump_r2 : float , temp_dir : Optional [ str ] = None , ) -> pd . DataFrame : \"\"\" LD clumping per chromosome. Parameters ---------- sig_df : pd.DataFrame The significant snps. ldref : str The LD reference file, (plink bfile format, containing wildcard {chrom}), e.g. EUR.chr{chrom}. clump_p1 : float The p1 threshold. clump_kb : int The kb threshold. clump_r2 : float The r2 threshold. temp_dir : Optional[str], optional The temporary directory, by default None Returns ------- pd.DataFrame The clumped snps. \"\"\" chrom = sig_df [ ColName . CHR ] . unique ()[ 0 ] clump_p_file = f \" { temp_dir } /clump_p_ { chrom } .txt\" sig_df [[ ColName . SNPID , ColName . P ]] . to_csv ( clump_p_file , sep = \" \\t \" , index = False ) clump_outfile = f \" { temp_dir } /clump_ { chrom } .clumped\" cmd = [ self . plink , \"--bfile\" , ldref . format ( chrom = chrom ), \"--clump\" , clump_p_file , \"--clump-p1\" , str ( clump_p1 ), \"--clump-kb\" , str ( clump_kb ), \"--clump-r2\" , str ( clump_r2 ), \"--clump-snp-field\" , ColName . SNPID , \"--clump-field\" , ColName . P , \"--out\" , f \" { temp_dir } /clump_ { chrom } \" , ] res = run ( cmd , stdout = PIPE , stderr = PIPE , universal_newlines = True ) if res . returncode != 0 : self . logger . error ( res . stderr ) raise RuntimeError ( res . stderr ) else : if os . path . exists ( clump_outfile ): clump_snps = pd . read_csv ( clump_outfile , delim_whitespace = True , usecols = [ \"SNP\" ]) clump_snps = clump_snps [ \"SNP\" ] . to_list () clump_snps = sig_df [ sig_df [ ColName . SNPID ] . isin ( clump_snps )] return clump_snps else : logging . warning ( f \"No clumped snps found for chromosome { chrom } \" ) return pd . DataFrame ()","title":"clump_per_chr()"},{"location":"api/Loci/#easyfinemap.loci.Loci.cojo_slct","text":"Conditional analysis for input sumstatistics. Parameters: Name Type Description Default sumstats pd . DataFrame The input sumstatistics, from same chromosome or locus. required ldref str The LD reference file, (plink bfile format, containing wildcard {chrom}), e.g. EUR.chr{chrom}. required sample_size int The sample size of the input sumstatistics. required cojo_window_kb int , optional The cojo window, by default 10000, unit: kb 10000 cojo_collinear float , optional The cojo collinear, by default 0.9 0.9 diff_freq float , optional The difference frequency, by default 0.2 0.2 sig_threshold float , optional The significance threshold, by default 5e-8 5e-08 use_ref_EAF bool , optional Whether to use the reference EAF, by default False False temp_dir Optional [ str ], optional The temporary directory, by default None None Returns: Type Description pd . DataFrame The conditional snps. Source code in easyfinemap/loci.py 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 @io_in_tempdir ( dir = \"./tmp/loci\" ) def cojo_slct ( self , sumstats : pd . DataFrame , ldref : str , sample_size : int , cojo_window_kb : int = 10000 , cojo_collinear : float = 0.9 , diff_freq : float = 0.2 , sig_threshold : float = 5e-8 , use_ref_EAF : bool = False , temp_dir : Optional [ str ] = None , ) -> pd . DataFrame : \"\"\" Conditional analysis for input sumstatistics. Parameters ---------- sumstats : pd.DataFrame The input sumstatistics, from same chromosome or locus. ldref : str The LD reference file, (plink bfile format, containing wildcard {chrom}), e.g. EUR.chr{chrom}. sample_size : int The sample size of the input sumstatistics. cojo_window_kb : int, optional The cojo window, by default 10000, unit: kb cojo_collinear : float, optional The cojo collinear, by default 0.9 diff_freq : float, optional The difference frequency, by default 0.2 sig_threshold : float, optional The significance threshold, by default 5e-8 use_ref_EAF : bool, optional Whether to use the reference EAF, by default False temp_dir : Optional[str], optional The temporary directory, by default None Returns ------- pd.DataFrame The conditional snps. \"\"\" chrom = sumstats [ ColName . CHR ] . unique ()[ 0 ] cojo_input = sumstats . copy () ld = LDRef () cojo_input = ld . intersect ( sumstats , ldref , f \" { temp_dir } /cojo_input_ { chrom } \" , use_ref_EAF ) if cojo_input . empty : self . logger . warning ( f \"No SNPs in LD reference for chromosome { temp_dir } /cojo_input_ { chrom } \" ) self . logger . warning ( \"Use the most significant SNP as the independent lead SNP.\" ) cojo_snps = sumstats . loc [ sumstats [ ColName . P ] == sumstats [ ColName . P ] . min ()] . copy () else : cojo_input [ ColName . N ] = sample_size cojo_input = cojo_input [ [ ColName . SNPID , ColName . EA , ColName . NEA , ColName . EAF , ColName . BETA , ColName . SE , ColName . P , ColName . N ] ] cojo_input . rename ( columns = { ColName . SNPID : \"SNP\" , ColName . EA : \"A1\" , ColName . NEA : \"A2\" , ColName . EAF : \"freq\" , ColName . BETA : \"b\" , ColName . SE : \"se\" , ColName . P : \"p\" , ColName . N : \"N\" , }, inplace = True , ) cojo_p_file = f \" { temp_dir } /cojo_input_ { chrom } .ma\" cojo_input . to_csv ( cojo_p_file , sep = \" \" , index = False ) cojo_outfile = f \" { temp_dir } /cojo_ { chrom } .slct\" cmd = [ self . gcta , \"--bfile\" , f \" { temp_dir } /cojo_input_ { chrom } \" , \"--cojo-file\" , cojo_p_file , \"--cojo-slct\" , \"--cojo-p\" , str ( sig_threshold ), \"--cojo-wind\" , str ( cojo_window_kb ), \"--cojo-collinear\" , str ( cojo_collinear ), \"--diff-freq\" , str ( diff_freq ), \"--out\" , cojo_outfile , ] self . logger . debug ( f \"Run cojo-slct: { ' ' . join ( cmd ) } \" ) res = run ( cmd , stdout = PIPE , stderr = PIPE , universal_newlines = True ) if res . returncode != 0 : self . logger . warning ( res . stderr ) self . logger . warning ( f \"Run cojo-slct failed for chromosome { chrom } \" ) self . logger . warning ( \"Use the most significant SNP as the independent lead SNP.\" ) cojo_snps = sumstats . loc [ sumstats [ ColName . P ] == sumstats [ ColName . P ] . min ()] . copy () else : if os . path . exists ( f \" { cojo_outfile } .jma.cojo\" ): cojo_snps = pd . read_csv ( f \" { cojo_outfile } .jma.cojo\" , delim_whitespace = True , usecols = [ \"SNP\" , \"bJ\" , \"bJ_se\" , \"pJ\" ] ) cojo_snps . rename ( columns = { \"SNP\" : ColName . SNPID , \"bJ\" : ColName . COJO_BETA , \"bJ_se\" : ColName . COJO_SE , \"pJ\" : ColName . COJO_P , }, inplace = True , ) cojo_snps = cojo_snps [ cojo_snps [ ColName . COJO_P ] <= sig_threshold ] cojo_snps = sumstats . merge ( cojo_snps , on = ColName . SNPID , how = \"inner\" ) else : self . logger . warning ( f \"No conditional snps found for chromosome { chrom } \" ) self . logger . warning ( \"Use the most significant SNP as the independent lead SNP.\" ) cojo_snps = sumstats . loc [ sumstats [ ColName . P ] == sumstats [ ColName . P ] . min ()] . copy () return cojo_snps","title":"cojo_slct()"},{"location":"api/Loci/#easyfinemap.loci.Loci.identify_indep_loci","text":"Identify the independent loci. Parameters: Name Type Description Default sumstats pd . DataFrame The input summary statistics. required sig_threshold float , optional The pvalue threshold, by default 5e-8 5e-08 loci_extend int , optional The range to extend the independent lead snps to independent loci, by default 500, unit: kb 500 if_merge bool , optional Whether to merge the overlapped independent loci, by default False False ldref Optional [ str ], optional The LD reference file, by default None None method str , optional The method to identify the independent loci, by default \"distance\", choose from [\"distance\", \"clumping\", \"conditional\"] 'distance' distance int , optional The distance threshold to identify the independent loci, by default 500, unit: kb 500 clump_kb int , optional The distance threshold for LD clumping, by default 10000, unit: kb 500 clump_r2 float , optional The r2 threshold for LD clumping, by default 0.1 0.1 sample_size Optional [ int ], optional The sample size for conditional analysis, by default None None cojo_window_kb int , optional The distance threshold for conditional analysis, by default 10000 10000 cojo_collinear float , optional The collinear threshold for conditional analysis, by default 0.9 0.9 diff_freq float , optional The difference frequency threshold for conditional analysis, by default 0.2 0.2 only_use_sig_snps bool , optional Whether to use the significant snps for conditional analysis, by default False False use_ref_EAF bool , optional Whether to use the reference EAF for conditional analysis, by default False False threads int , optional The number of threads, by default 1 1 Returns: Type Description Tuple [ pd . DataFrame , pd . DataFrame ] The independent lead snps and independent loci. Source code in easyfinemap/loci.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def identify_indep_loci ( self , sumstats : pd . DataFrame , sig_threshold : float = 5e-8 , loci_extend : int = 500 , ldblock : Optional [ str ] = None , if_merge : bool = False , outprefix : Optional [ str ] = None , ldref : Optional [ str ] = None , method : str = \"distance\" , distance : int = 500 , clump_kb : int = 500 , clump_r2 : float = 0.1 , sample_size : Optional [ int ] = None , cojo_window_kb : int = 10000 , cojo_collinear : float = 0.9 , diff_freq : float = 0.2 , only_use_sig_snps : bool = False , use_ref_EAF : bool = False , threads : int = 1 , ) -> Union [ Tuple [ pd . DataFrame , pd . DataFrame ], None ]: \"\"\" Identify the independent loci. Parameters ---------- sumstats : pd.DataFrame The input summary statistics. sig_threshold : float, optional The pvalue threshold, by default 5e-8 loci_extend : int, optional The range to extend the independent lead snps to independent loci, by default 500, unit: kb if_merge : bool, optional Whether to merge the overlapped independent loci, by default False ldref : Optional[str], optional The LD reference file, by default None method : str, optional The method to identify the independent loci, by default \"distance\", choose from [\"distance\", \"clumping\", \"conditional\"] distance : int, optional The distance threshold to identify the independent loci, by default 500, unit: kb clump_kb : int, optional The distance threshold for LD clumping, by default 10000, unit: kb clump_r2 : float, optional The r2 threshold for LD clumping, by default 0.1 sample_size : Optional[int], optional The sample size for conditional analysis, by default None cojo_window_kb : int, optional The distance threshold for conditional analysis, by default 10000 cojo_collinear : float, optional The collinear threshold for conditional analysis, by default 0.9 diff_freq : float, optional The difference frequency threshold for conditional analysis, by default 0.2 only_use_sig_snps : bool, optional Whether to use the significant snps for conditional analysis, by default False use_ref_EAF : bool, optional Whether to use the reference EAF for conditional analysis, by default False threads : int, optional The number of threads, by default 1 Returns ------- Tuple[pd.DataFrame, pd.DataFrame] The independent lead snps and independent loci. \"\"\" sumstats = make_SNPID_unique ( sumstats ) if ldblock is not None : ldblock = pd . read_csv ( ldblock , sep = \" \\t \" , names = [ ColName . CHR , ColName . START , ColName . END ]) if method == \"distance\" : sig_df = get_significant_snps ( sumstats , sig_threshold ) lead_snp = self . indep_snps_by_distance ( sig_df , distance , ldblock ) elif method == \"clumping\" : clump_p1 = sig_threshold if ldref is not None : sig_df = get_significant_snps ( sumstats , sig_threshold ) lead_snp = self . indep_snps_by_ldclumping ( sig_df , ldref , clump_p1 , clump_kb , clump_r2 ) else : raise ValueError ( f \"Please provide the ldref file for method: { method } \" ) elif method == \"conditional\" : if ldref is None : raise ValueError ( \"Please provide the ldref file for conditional analysis.\" ) if sample_size is None : raise ValueError ( \"Please provide the sample size for conditional analysis.\" ) else : lead_snp = self . indep_snps_by_conditional ( sumstats , ldref , sample_size , sig_threshold , cojo_window_kb , cojo_collinear , diff_freq , use_ref_EAF , only_use_sig_snps , ldblock , threads , ) else : raise ValueError ( f \"Unsupported method: { method } \" ) loci = self . leadsnp2loci ( lead_snp , loci_extend , if_merge , ldblock ) if if_merge and ColName . COJO_BETA in lead_snp . columns : self . logger . warning ( \"The loci identified by cojo may not need merge.\" ) lead_snp = lead_snp [ lead_snp [ ColName . SNPID ] . isin ( loci [ ColName . LEAD_SNP ])] if outprefix : loci_file = f \" { outprefix } .loci.txt\" loci . to_csv ( loci_file , sep = \" \\t \" , index = False , float_format = \" %.6g \" ) self . logger . info ( f \"Save { len ( loci ) } independent loci to { loci_file } \" ) leadsnp_file = f \" { outprefix } .leadsnp.txt\" lead_snp . to_csv ( leadsnp_file , sep = \" \\t \" , index = False , float_format = \" %.6g \" ) self . logger . info ( f \"Save { len ( lead_snp ) } independent lead snps to { leadsnp_file } \" ) return lead_snp , loci","title":"identify_indep_loci()"},{"location":"api/Loci/#easyfinemap.loci.Loci.indep_snps_by_conditional","text":"Identify the independent snps by conditional analysis. Parameters: Name Type Description Default sumstats pd . DataFrame The summary statistics. required ldref str The LD reference file, (plink bfile format, containing wildcard {chrom}), e.g. EUR.chr{chrom}. required sample_size int The sample size. required sig_threshold float , optional The significance threshold, by default 5e-8 5e-08 cojo_window_kb int , optional The cojo window, by default 10000, in kb 10000 cojo_collinear float , optional The cojo collinear, by default 0.9 0.9 diff_freq float , optional The difference frequency, by default 0.2 0.2 use_ref_EAF bool , optional Whether to use the reference EAF, by default False False only_use_sig_snps bool , optional Whether to only use the significant snps, by default False False ldblock Optional [ pd . DataFrame ], optional The LD block, run cojo in each LD block, by default None None threads int , optional The number of threads, by default 1 1 Source code in easyfinemap/loci.py 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 @staticmethod def indep_snps_by_conditional ( sumstats : pd . DataFrame , ldref : str , sample_size : int , sig_threshold : float = 5e-8 , cojo_window_kb : int = 10000 , cojo_collinear : float = 0.9 , diff_freq : float = 0.2 , use_ref_EAF : bool = False , only_use_sig_snps : bool = False , ldblock : Optional [ pd . DataFrame ] = None , threads : int = 1 , ) -> pd . DataFrame : \"\"\" Identify the independent snps by conditional analysis. Parameters ---------- sumstats : pd.DataFrame The summary statistics. ldref : str The LD reference file, (plink bfile format, containing wildcard {chrom}), e.g. EUR.chr{chrom}. sample_size : int The sample size. sig_threshold : float, optional The significance threshold, by default 5e-8 cojo_window_kb : int, optional The cojo window, by default 10000, in kb cojo_collinear : float, optional The cojo collinear, by default 0.9 diff_freq : float, optional The difference frequency, by default 0.2 use_ref_EAF : bool, optional Whether to use the reference EAF, by default False only_use_sig_snps : bool, optional Whether to only use the significant snps, by default False ldblock : Optional[pd.DataFrame], optional The LD block, run cojo in each LD block, by default None threads : int, optional The number of threads, by default 1 \"\"\" logger = logging . getLogger ( 'COJO' ) if not use_ref_EAF and ColName . EAF not in sumstats . columns : raise ValueError ( f \" { ColName . EAF } is not in the sumstats, please set use_ref_EAF to True\" ) sig_df = sumstats [ sumstats [ ColName . P ] <= sig_threshold ] logger . debug ( f \"Number of significant snps: { len ( sig_df ) } \" ) logger . debug ( f \"Number of chromosomes: { len ( sig_df [ ColName . CHR ] . unique ()) } \" ) args_list = [] loci = Loci () if ldblock is not None : sig_blocks = loci . indep_snps_by_distance ( sig_df , ldblock = ldblock ) sig_blocks = loci . leadsnp2loci ( sig_blocks , ldblock = ldblock ) for i in sig_blocks . index : if only_use_sig_snps : in_df = sig_df [ ( sig_df [ ColName . CHR ] == sig_blocks . loc [ i ][ ColName . CHR ]) & ( sig_df [ ColName . BP ] >= sig_blocks . loc [ i ][ ColName . START ]) & ( sig_df [ ColName . BP ] <= sig_blocks . loc [ i ][ ColName . END ]) ] else : in_df = sumstats [ ( sumstats [ ColName . CHR ] == sig_blocks . loc [ i ][ ColName . CHR ]) & ( sumstats [ ColName . BP ] >= sig_blocks . loc [ i ][ ColName . START ]) & ( sumstats [ ColName . BP ] <= sig_blocks . loc [ i ][ ColName . END ]) ] args_list . append ( ( in_df , ldref . format ( chrom = sig_blocks . loc [ i ][ ColName . CHR ]), sample_size , cojo_window_kb , cojo_collinear , diff_freq , sig_threshold , use_ref_EAF , ) ) else : for chrom in sig_df [ ColName . CHR ] . unique (): if only_use_sig_snps : in_df = sig_df [ sig_df [ ColName . CHR ] == chrom ] else : in_df = sumstats [ sumstats [ ColName . CHR ] == chrom ] args_list . append ( ( in_df , ldref . format ( chrom = chrom ), sample_size , cojo_window_kb , cojo_collinear , diff_freq , sig_threshold , use_ref_EAF , ) ) with ProcessPoolExecutor ( max_workers = threads ) as executor : results = [] with Progress ( TextColumn ( \" {task.description} \" ), BarColumn (), MofNCompleteColumn (), TimeElapsedColumn (), auto_refresh = True , ) as progress : task = progress . add_task ( \"Run cojo-slct\" , total = len ( args_list )) for _ in executor . map ( loci . cojo_slct , * zip ( * args_list )): progress . update ( task , advance = 1 ) progress . refresh () results . append ( _ ) cojo_snps = pd . concat ( results , axis = 0 , ignore_index = True ) return cojo_snps","title":"indep_snps_by_conditional()"},{"location":"api/Loci/#easyfinemap.loci.Loci.indep_snps_by_distance","text":"Identify the independent snps by distance only. Parameters: Name Type Description Default sig_df pd . DataFrame The significant snps. required distance int , optional The distance threshold, by default 500, unit: kb 500 ldblock Optional [ pd . DataFrame ], optional The ld block information, use boundary to identify the independent snps, by default None None Returns: Type Description pd . DataFrame The independent snps. Source code in easyfinemap/loci.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 @staticmethod def indep_snps_by_distance ( sig_df : pd . DataFrame , distance : int = 500 , ldblock : Optional [ pd . DataFrame ] = None ) -> pd . DataFrame : \"\"\" Identify the independent snps by distance only. Parameters ---------- sig_df : pd.DataFrame The significant snps. distance : int, optional The distance threshold, by default 500, unit: kb ldblock : Optional[pd.DataFrame], optional The ld block information, use boundary to identify the independent snps, by default None Returns ------- pd.DataFrame The independent snps. \"\"\" sig_df = sig_df . sort_values ( ColName . P ) . copy () lead_snp = [] if ldblock is not None : while len ( sig_df ): lead_snp . append ( sig_df . iloc [[ 0 ]]) sig_block = ldblock [ ( ldblock [ ColName . CHR ] == sig_df . iloc [ 0 ][ ColName . CHR ]) & ( ldblock [ ColName . START ] <= sig_df . iloc [ 0 ][ ColName . BP ]) & ( ldblock [ ColName . END ] >= sig_df . iloc [ 0 ][ ColName . BP ]) ] sig_df = sig_df [ ~ ( ( sig_df [ ColName . CHR ] == sig_df . iloc [ 0 ][ ColName . CHR ]) & ( sig_df [ ColName . BP ] >= sig_block . iloc [ 0 ][ ColName . START ]) & ( sig_df [ ColName . BP ] <= sig_block . iloc [ 0 ][ ColName . END ]) ) ] else : distance = distance * 1000 while len ( sig_df ): lead_snp . append ( sig_df . iloc [[ 0 ]]) sig_df = sig_df [ ~ ( ( sig_df [ ColName . CHR ] == sig_df . iloc [ 0 ][ ColName . CHR ]) & ( sig_df [ ColName . BP ] >= sig_df . iloc [ 0 ][ ColName . BP ] - distance ) & ( sig_df [ ColName . BP ] <= sig_df . iloc [ 0 ][ ColName . BP ] + distance ) ) ] # type: ignore lead_snp = pd . concat ( lead_snp , axis = 0 , ignore_index = True ) return lead_snp","title":"indep_snps_by_distance()"},{"location":"api/Loci/#easyfinemap.loci.Loci.indep_snps_by_ldclumping","text":"Identify the independent snps by LD clumping. Parameters: Name Type Description Default sig_df pd . DataFrame The significant snps. required ldref str The LD reference file, (plink bfile format, containing wildcard {chrom}), e.g. EUR.chr{chrom}. required clump_p1 float , optional The p1 threshold, by default 5e-8 5e-08 clump_kb int , optional The kb threshold, by default 500, unit: kb 500 clump_r2 float , optional The r2 threshold, by default 0.1 0.1 Returns: Type Description pd . DataFrame Source code in easyfinemap/loci.py 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 @staticmethod def indep_snps_by_ldclumping ( sig_df : pd . DataFrame , ldref : str , clump_p1 : float = 5e-8 , clump_kb : int = 500 , clump_r2 : float = 0.1 ) -> pd . DataFrame : \"\"\" Identify the independent snps by LD clumping. Parameters ---------- sig_df : pd.DataFrame The significant snps. ldref : str The LD reference file, (plink bfile format, containing wildcard {chrom}), e.g. EUR.chr{chrom}. clump_p1 : float, optional The p1 threshold, by default 5e-8 clump_kb : int, optional The kb threshold, by default 500, unit: kb clump_r2 : float, optional The r2 threshold, by default 0.1 Returns ------- pd.DataFrame \"\"\" clumped_snps = [] for chrom in sig_df [ ColName . CHR ] . unique (): sig_df_chr = sig_df [ sig_df [ ColName . CHR ] == chrom ] clumped_snps . append ( Loci () . clump_per_chr ( sig_df_chr , ldref , clump_p1 , clump_kb , clump_r2 )) # type: ignore clumped_snps = pd . concat ( clumped_snps , axis = 0 , ignore_index = True ) return clumped_snps","title":"indep_snps_by_ldclumping()"},{"location":"api/Loci/#easyfinemap.loci.Loci.leadsnp2loci","text":"Expand the independent lead snps to independent loci by given range. Parameters: Name Type Description Default lead_snps pd . DataFrame The independent lead snps. required range int , optional The range, by default 500, unit: kb 500 if_merge bool , optional Whether merge the overlapped loci, by default False False ldblock Optional [ pd . DataFrame ], optional The ld block, using LD block to expand the independent loci, by default None None Returns: Type Description pd . DataFrame The independent loci. Source code in easyfinemap/loci.py 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 @staticmethod def leadsnp2loci ( lead_snps : pd . DataFrame , range : int = 500 , if_merge : bool = False , ldblock : Optional [ pd . DataFrame ] = None ) -> pd . DataFrame : \"\"\" Expand the independent lead snps to independent loci by given range. Parameters ---------- lead_snps : pd.DataFrame The independent lead snps. range : int, optional The range, by default 500, unit: kb if_merge : bool, optional Whether merge the overlapped loci, by default False ldblock : Optional[pd.DataFrame], optional The ld block, using LD block to expand the independent loci, by default None Returns ------- pd.DataFrame The independent loci. \"\"\" loci_df = lead_snps . copy () loci_df = loci_df [[ ColName . CHR , ColName . BP , ColName . P , ColName . SNPID ]] loci_df . columns = [ ColName . CHR , ColName . LEAD_SNP_BP , ColName . LEAD_SNP_P , ColName . LEAD_SNP ] # type: ignore if ldblock is not None : loci_df [ ColName . START ] = 0 loci_df [ ColName . END ] = 0 for i in loci_df . index : sub_ldblock = ldblock [ ( ldblock [ ColName . CHR ] == loci_df . loc [ i , ColName . CHR ]) & ( ldblock [ ColName . START ] <= loci_df . loc [ i , ColName . LEAD_SNP_BP ]) & ( ldblock [ ColName . END ] >= loci_df . loc [ i , ColName . LEAD_SNP_BP ]) ] if sub_ldblock . empty : continue else : loci_df . loc [ i , ColName . START ] = sub_ldblock . iloc [ 0 ][ ColName . START ] loci_df . loc [ i , ColName . END ] = sub_ldblock . iloc [ 0 ][ ColName . END ] else : range = range * 1000 loci_df [ ColName . START ] = loci_df [ ColName . LEAD_SNP_BP ] - range loci_df [ ColName . START ] = loci_df [ ColName . START ] . apply ( lambda x : 0 if x < 0 else x ) loci_df [ ColName . END ] = loci_df [ ColName . LEAD_SNP_BP ] + range loci_df = loci_df [ ColName . loci_cols ] . copy () if if_merge : loci_df = Loci . merge_overlapped_loci ( loci_df ) loci_df = loci_df . sort_values ( by = [ ColName . CHR , ColName . START , ColName . END ]) return loci_df","title":"leadsnp2loci()"},{"location":"api/Loci/#easyfinemap.loci.Loci.merge_overlapped_loci","text":"Merge the overlapped loci. Parameters: Name Type Description Default loci_df pd . DataFrame The independent loci. required Returns: Type Description pd . DataFrame The merged independent loci. Source code in easyfinemap/loci.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 @staticmethod def merge_overlapped_loci ( loci_df : pd . DataFrame ): \"\"\" Merge the overlapped loci. Parameters ---------- loci_df : pd.DataFrame The independent loci. Returns ------- pd.DataFrame The merged independent loci. \"\"\" merged_loci = loci_df . copy () merged_loci . sort_values ([ ColName . CHR , ColName . START , ColName . END ], inplace = True ) merged_loci [ 'no_overlap' ] = merged_loci [ ColName . START ] > merged_loci [ ColName . END ] . shift () . cummax () merged_loci [ 'diff_chr' ] = merged_loci [ ColName . CHR ] != merged_loci [ ColName . CHR ] . shift () merged_loci [ \"break\" ] = merged_loci [ \"no_overlap\" ] | merged_loci [ 'diff_chr' ] merged_loci [ 'group' ] = merged_loci [ 'break' ] . cumsum () merged_loci = merged_loci . sort_values ([ 'group' , ColName . LEAD_SNP_P ], ascending = True ) agg_func = {} for col in loci_df . columns : if col == ColName . START : agg_func [ col ] = 'min' elif col == ColName . END : agg_func [ col ] = 'max' else : agg_func [ col ] = 'first' result = merged_loci . groupby ( \"group\" ) . agg ( agg_func ) result . reset_index ( drop = True , inplace = True ) return result","title":"merge_overlapped_loci()"},{"location":"api/cli/","text":"Console script for easy_finemap. FinemapMethod \u00b6 Bases: str , Enum The method to perform fine-mapping. LociMethod \u00b6 Bases: str , Enum The method to identify the lead SNPs. fine_mapping ( sumstats_path = typer . Argument ( Ellipsis , help = 'The path to the GWAS summary statistics file.' ), loci_path = typer . Argument ( Ellipsis , help = 'The path to the loci file, generated by get-loci command.' ), lead_snps_path = typer . Argument ( Ellipsis , help = 'The path to the lead SNPs file, generated by get-loci command.' ), methods = typer . Option ( Ellipsis , '--methods' , '-m' , help = 'The methods to use.' ), outfile = typer . Argument ( Ellipsis , help = 'The output file.' ), var_prior = typer . Option ( 0.5 , '--var-prior' , help = 'The prior variance for the aBF method.' ), conditional = typer . Option ( False , '--conditional' , '-c' , help = 'Whether to use conditional mode.' ), prior_file = typer . Option ( None , '--prior-file' , help = 'The path to the prior file.' ), sample_size = typer . Option ( None , '--sample-size' , '-n' , help = 'The sample size for conditional mode.' ), ldref = typer . Option ( None , '--ldref' , help = 'The path to the LD reference file.' ), cond_snps_wind_kb = typer . Option ( 10000 , '--cond-snps-wind-kb' , help = 'The conditional SNPs window size, in kb.' ), max_causal = typer . Option ( 1 , '--max-causal' , help = 'The maximum number of causal SNPs.' ), credible_threshold = typer . Option ( None , '--credible-threshold' , help = 'The credible threshold.' ), credible_method = typer . Option ( None , '--credible-method' , help = 'The fine-mapping method for credible set.' ), use_ref_EAF = typer . Option ( False , '--use-ref-eaf' , help = 'Whether to use the reference panel EAF.' ), threads = typer . Option ( 1 , '--threads' , '-t' , help = 'The number of threads.' )) \u00b6 Fine mapping. Source code in easyfinemap/cli.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 @app . command () def fine_mapping ( sumstats_path : str = typer . Argument ( ... , help = \"The path to the GWAS summary statistics file.\" ), loci_path : str = typer . Argument ( ... , help = \"The path to the loci file, generated by get-loci command.\" ), lead_snps_path : str = typer . Argument ( ... , help = \"The path to the lead SNPs file, generated by get-loci command.\" ), methods : List [ FinemapMethod ] = typer . Option ( ... , \"--methods\" , \"-m\" , help = \"The methods to use.\" ), outfile : str = typer . Argument ( ... , help = \"The output file.\" ), var_prior : float = typer . Option ( 0.5 , \"--var-prior\" , help = \"The prior variance for the aBF method.\" ), conditional : bool = typer . Option ( False , \"--conditional\" , \"-c\" , help = \"Whether to use conditional mode.\" ), prior_file : Optional [ str ] = typer . Option ( None , \"--prior-file\" , help = \"The path to the prior file.\" ), sample_size : Optional [ int ] = typer . Option ( None , \"--sample-size\" , \"-n\" , help = \"The sample size for conditional mode.\" ), ldref : str = typer . Option ( None , \"--ldref\" , help = \"The path to the LD reference file.\" ), cond_snps_wind_kb : int = typer . Option ( 10000 , \"--cond-snps-wind-kb\" , help = \"The conditional SNPs window size, in kb.\" ), max_causal : int = typer . Option ( 1 , \"--max-causal\" , help = \"The maximum number of causal SNPs.\" ), credible_threshold : Optional [ float ] = typer . Option ( None , \"--credible-threshold\" , help = \"The credible threshold.\" ), credible_method : Optional [ str ] = typer . Option ( None , \"--credible-method\" , help = \"The fine-mapping method for credible set.\" ), use_ref_EAF : bool = typer . Option ( False , \"--use-ref-eaf\" , help = \"Whether to use the reference panel EAF.\" ), threads : int = typer . Option ( 1 , \"--threads\" , \"-t\" , help = \"The number of threads.\" ), ) -> None : \"\"\"Fine mapping.\"\"\" if os . path . exists ( sumstats_path ) and os . path . exists ( loci_path ) and os . path . exists ( lead_snps_path ): # sumstats = pd.read_csv(sumstats_path, sep=\"\\t\") loci = pd . read_csv ( loci_path , sep = \" \\t \" ) lead_snps = pd . read_csv ( lead_snps_path , sep = \" \\t \" ) EasyFinemap () . finemap_all_loci ( sumstats = sumstats_path , loci = loci , lead_snps = lead_snps , methods = methods , # type: ignore outfile = outfile , var_prior = var_prior , conditional = conditional , prior_file = prior_file , sample_size = sample_size , ldref = ldref , cond_snps_wind_kb = cond_snps_wind_kb , max_causal = max_causal , credible_threshold = credible_threshold , credible_method = credible_method , use_ref_EAF = use_ref_EAF , threads = threads , ) else : logging . error ( f \"No such file of { sumstats_path } or { loci_path } or { lead_snps_path } .\" ) sys . exit ( 1 ) get_loci ( sumstats_path = typer . Argument ( Ellipsis , help = 'The path to the GWAS summary statistics file.' ), output = typer . Argument ( Ellipsis , help = 'The output prefix.' ), sig_threshold = typer . Option ( 5e-08 , '--sig-threshold' , '-s' , help = 'The significance threshold.' ), loci_extension = typer . Option ( 500 , '--loci-extension' , '-l' , help = 'The extension from lead SNPs, in kb' ), ldblock = typer . Option ( None , '--ldblock' , help = 'The path to the LD block file.' ), if_merge = typer . Option ( False , '--merge-loci' , '-i' , help = 'Whether to merge the loci, not recommanded for conditional mode.' ), method = typer . Option ( LociMethod . distance , '--method' , '-m' , help = 'The method to identify the lead SNPs.' ), distance = typer . Option ( 50 , '--distance' , '-d' , help = 'The distance threshold for distance method, in kb.' ), ldref = typer . Option ( None , '--ldref' , help = 'The path to the LD reference file.' ), clump_kb = typer . Option ( 500 , '--clump-kb' , '-c' , help = 'The clumping window size, in kb.' ), clump_r2 = typer . Option ( 0.1 , '--clump-r2' , '-r' , help = 'The clumping r2 threshold.' ), sample_size = typer . Option ( None , '--sample-size' , '-n' , help = 'The sample size for conditional method.' ), cojo_window_kb = typer . Option ( 10000 , '--cojo-window-kb' , help = 'The cojo window size, in kb.' ), cojo_collinear = typer . Option ( 0.9 , '--cojo-collinear' , help = 'The cojo collinear threshold.' ), diff_freq = typer . Option ( 0.2 , '--diff-freq' , help = 'The difference in allele frequency threshold.' ), use_ref_eaf = typer . Option ( False , '--use-ref-eaf' , help = 'Whether to use the reference panel EAF.' ), only_use_sig_snps = typer . Option ( False , '--only-use-sig-snps' , help = 'Whether to only use the significant SNPs.' ), threads = typer . Option ( 1 , '--threads' , '-t' , help = 'The number of threads.' )) \u00b6 Get the loci from the GWAS summary statistics file. Source code in easyfinemap/cli.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 @app . command () def get_loci ( sumstats_path : Path = typer . Argument ( ... , help = \"The path to the GWAS summary statistics file.\" ), output : str = typer . Argument ( ... , help = \"The output prefix.\" ), sig_threshold : float = typer . Option ( 5e-8 , \"--sig-threshold\" , \"-s\" , help = \"The significance threshold.\" ), loci_extension : int = typer . Option ( 500 , \"--loci-extension\" , \"-l\" , help = \"The extension from lead SNPs, in kb\" ), ldblock : str = typer . Option ( None , \"--ldblock\" , help = \"The path to the LD block file.\" ), if_merge : bool = typer . Option ( False , \"--merge-loci\" , \"-i\" , help = \"Whether to merge the loci, not recommanded for conditional mode.\" ), method : LociMethod = typer . Option ( LociMethod . distance , \"--method\" , \"-m\" , help = \"The method to identify the lead SNPs.\" ), distance : int = typer . Option ( 50 , \"--distance\" , \"-d\" , help = \"The distance threshold for distance method, in kb.\" ), ldref : str = typer . Option ( None , \"--ldref\" , help = \"The path to the LD reference file.\" ), clump_kb : int = typer . Option ( 500 , \"--clump-kb\" , \"-c\" , help = \"The clumping window size, in kb.\" ), clump_r2 : float = typer . Option ( 0.1 , \"--clump-r2\" , \"-r\" , help = \"The clumping r2 threshold.\" ), sample_size : int = typer . Option ( None , \"--sample-size\" , \"-n\" , help = \"The sample size for conditional method.\" ), cojo_window_kb : int = typer . Option ( 10000 , \"--cojo-window-kb\" , help = \"The cojo window size, in kb.\" ), cojo_collinear : float = typer . Option ( 0.9 , \"--cojo-collinear\" , help = \"The cojo collinear threshold.\" ), diff_freq : float = typer . Option ( 0.2 , \"--diff-freq\" , help = \"The difference in allele frequency threshold.\" ), use_ref_eaf : bool = typer . Option ( False , \"--use-ref-eaf\" , help = \"Whether to use the reference panel EAF.\" ), only_use_sig_snps : bool = typer . Option ( False , \"--only-use-sig-snps\" , help = \"Whether to only use the significant SNPs.\" ), threads : int = typer . Option ( 1 , \"--threads\" , \"-t\" , help = \"The number of threads.\" ), ) -> None : \"\"\"Get the loci from the GWAS summary statistics file.\"\"\" if sumstats_path . exists (): logging . info ( f \"Loading { sumstats_path } ...\" ) sumstats = pd . read_csv ( sumstats_path , sep = \" \\t \" ) Loci () . identify_indep_loci ( sumstats = sumstats , sig_threshold = sig_threshold , loci_extend = loci_extension , ldblock = ldblock , if_merge = if_merge , outprefix = output , ldref = ldref , method = method , distance = distance , clump_kb = clump_kb , clump_r2 = clump_r2 , sample_size = sample_size , cojo_window_kb = cojo_window_kb , cojo_collinear = cojo_collinear , diff_freq = diff_freq , use_ref_EAF = use_ref_eaf , only_use_sig_snps = only_use_sig_snps , threads = threads , ) else : logging . error ( f \"No such file of { sumstats_path } .\" ) sys . exit ( 1 ) main ( version = typer . Option ( False , '--version' , '-V' , help = 'Show version.' ), verbose = typer . Option ( False , '--verbose' , '-v' , help = 'Show verbose info.' )) \u00b6 Source code in easyfinemap/cli.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 @app . callback ( invoke_without_command = True , no_args_is_help = True ) def main ( version : bool = typer . Option ( False , '--version' , '-V' , help = 'Show version.' ), verbose : bool = typer . Option ( False , '--verbose' , '-v' , help = 'Show verbose info.' ), ): \"\"\"EasyFinemap: A user-friendly tool for fine-mapping.\"\"\" console = Console () console . rule ( \"[bold blue]EasyFinemap[/bold blue]\" ) console . print ( f \"Version: { __version__ } \" , justify = \"center\" ) console . print ( \"Author: Jianhua Wang\" , justify = \"center\" ) console . print ( \"Email: jianhua.mert@gmail.com\" , justify = \"center\" ) if version : typer . echo ( f 'EasyFinemap version: { __version__ } ' ) raise typer . Exit () if verbose : logging . getLogger () . setLevel ( logging . DEBUG ) logging . info ( 'Verbose mode is on.' ) else : logging . getLogger () . setLevel ( logging . INFO ) validate_ldref ( ldref_path = typer . Argument ( Ellipsis , help = 'The path to the LD reference file.' ), outprefix = typer . Argument ( Ellipsis , help = 'The output prefix.' ), file_type = typer . Option ( 'plink' , '--file-type' , '-f' , help = 'The file type of the LD reference file.' ), mac = typer . Option ( 10 , '--mac' , '-m' , help = 'The minor allele count threshold.' ), threads = typer . Option ( 1 , '--threads' , '-t' , help = 'The number of threads.' )) \u00b6 Validate the LD reference file. Source code in easyfinemap/cli.py 71 72 73 74 75 76 77 78 79 80 81 @app . command () def validate_ldref ( ldref_path : str = typer . Argument ( ... , help = \"The path to the LD reference file.\" ), outprefix : str = typer . Argument ( ... , help = \"The output prefix.\" ), file_type : str = typer . Option ( \"plink\" , \"--file-type\" , \"-f\" , help = \"The file type of the LD reference file.\" ), mac : int = typer . Option ( 10 , \"--mac\" , \"-m\" , help = \"The minor allele count threshold.\" ), threads : int = typer . Option ( 1 , \"--threads\" , \"-t\" , help = \"The number of threads.\" ), ) -> None : \"\"\"Validate the LD reference file.\"\"\" ld = LDRef () ld . valid ( ldref_path , outprefix , file_type , mac , threads )","title":"CLI"},{"location":"api/cli/#easyfinemap.cli.FinemapMethod","text":"Bases: str , Enum The method to perform fine-mapping.","title":"FinemapMethod"},{"location":"api/cli/#easyfinemap.cli.LociMethod","text":"Bases: str , Enum The method to identify the lead SNPs.","title":"LociMethod"},{"location":"api/cli/#easyfinemap.cli.fine_mapping","text":"Fine mapping. Source code in easyfinemap/cli.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 @app . command () def fine_mapping ( sumstats_path : str = typer . Argument ( ... , help = \"The path to the GWAS summary statistics file.\" ), loci_path : str = typer . Argument ( ... , help = \"The path to the loci file, generated by get-loci command.\" ), lead_snps_path : str = typer . Argument ( ... , help = \"The path to the lead SNPs file, generated by get-loci command.\" ), methods : List [ FinemapMethod ] = typer . Option ( ... , \"--methods\" , \"-m\" , help = \"The methods to use.\" ), outfile : str = typer . Argument ( ... , help = \"The output file.\" ), var_prior : float = typer . Option ( 0.5 , \"--var-prior\" , help = \"The prior variance for the aBF method.\" ), conditional : bool = typer . Option ( False , \"--conditional\" , \"-c\" , help = \"Whether to use conditional mode.\" ), prior_file : Optional [ str ] = typer . Option ( None , \"--prior-file\" , help = \"The path to the prior file.\" ), sample_size : Optional [ int ] = typer . Option ( None , \"--sample-size\" , \"-n\" , help = \"The sample size for conditional mode.\" ), ldref : str = typer . Option ( None , \"--ldref\" , help = \"The path to the LD reference file.\" ), cond_snps_wind_kb : int = typer . Option ( 10000 , \"--cond-snps-wind-kb\" , help = \"The conditional SNPs window size, in kb.\" ), max_causal : int = typer . Option ( 1 , \"--max-causal\" , help = \"The maximum number of causal SNPs.\" ), credible_threshold : Optional [ float ] = typer . Option ( None , \"--credible-threshold\" , help = \"The credible threshold.\" ), credible_method : Optional [ str ] = typer . Option ( None , \"--credible-method\" , help = \"The fine-mapping method for credible set.\" ), use_ref_EAF : bool = typer . Option ( False , \"--use-ref-eaf\" , help = \"Whether to use the reference panel EAF.\" ), threads : int = typer . Option ( 1 , \"--threads\" , \"-t\" , help = \"The number of threads.\" ), ) -> None : \"\"\"Fine mapping.\"\"\" if os . path . exists ( sumstats_path ) and os . path . exists ( loci_path ) and os . path . exists ( lead_snps_path ): # sumstats = pd.read_csv(sumstats_path, sep=\"\\t\") loci = pd . read_csv ( loci_path , sep = \" \\t \" ) lead_snps = pd . read_csv ( lead_snps_path , sep = \" \\t \" ) EasyFinemap () . finemap_all_loci ( sumstats = sumstats_path , loci = loci , lead_snps = lead_snps , methods = methods , # type: ignore outfile = outfile , var_prior = var_prior , conditional = conditional , prior_file = prior_file , sample_size = sample_size , ldref = ldref , cond_snps_wind_kb = cond_snps_wind_kb , max_causal = max_causal , credible_threshold = credible_threshold , credible_method = credible_method , use_ref_EAF = use_ref_EAF , threads = threads , ) else : logging . error ( f \"No such file of { sumstats_path } or { loci_path } or { lead_snps_path } .\" ) sys . exit ( 1 )","title":"fine_mapping()"},{"location":"api/cli/#easyfinemap.cli.get_loci","text":"Get the loci from the GWAS summary statistics file. Source code in easyfinemap/cli.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 @app . command () def get_loci ( sumstats_path : Path = typer . Argument ( ... , help = \"The path to the GWAS summary statistics file.\" ), output : str = typer . Argument ( ... , help = \"The output prefix.\" ), sig_threshold : float = typer . Option ( 5e-8 , \"--sig-threshold\" , \"-s\" , help = \"The significance threshold.\" ), loci_extension : int = typer . Option ( 500 , \"--loci-extension\" , \"-l\" , help = \"The extension from lead SNPs, in kb\" ), ldblock : str = typer . Option ( None , \"--ldblock\" , help = \"The path to the LD block file.\" ), if_merge : bool = typer . Option ( False , \"--merge-loci\" , \"-i\" , help = \"Whether to merge the loci, not recommanded for conditional mode.\" ), method : LociMethod = typer . Option ( LociMethod . distance , \"--method\" , \"-m\" , help = \"The method to identify the lead SNPs.\" ), distance : int = typer . Option ( 50 , \"--distance\" , \"-d\" , help = \"The distance threshold for distance method, in kb.\" ), ldref : str = typer . Option ( None , \"--ldref\" , help = \"The path to the LD reference file.\" ), clump_kb : int = typer . Option ( 500 , \"--clump-kb\" , \"-c\" , help = \"The clumping window size, in kb.\" ), clump_r2 : float = typer . Option ( 0.1 , \"--clump-r2\" , \"-r\" , help = \"The clumping r2 threshold.\" ), sample_size : int = typer . Option ( None , \"--sample-size\" , \"-n\" , help = \"The sample size for conditional method.\" ), cojo_window_kb : int = typer . Option ( 10000 , \"--cojo-window-kb\" , help = \"The cojo window size, in kb.\" ), cojo_collinear : float = typer . Option ( 0.9 , \"--cojo-collinear\" , help = \"The cojo collinear threshold.\" ), diff_freq : float = typer . Option ( 0.2 , \"--diff-freq\" , help = \"The difference in allele frequency threshold.\" ), use_ref_eaf : bool = typer . Option ( False , \"--use-ref-eaf\" , help = \"Whether to use the reference panel EAF.\" ), only_use_sig_snps : bool = typer . Option ( False , \"--only-use-sig-snps\" , help = \"Whether to only use the significant SNPs.\" ), threads : int = typer . Option ( 1 , \"--threads\" , \"-t\" , help = \"The number of threads.\" ), ) -> None : \"\"\"Get the loci from the GWAS summary statistics file.\"\"\" if sumstats_path . exists (): logging . info ( f \"Loading { sumstats_path } ...\" ) sumstats = pd . read_csv ( sumstats_path , sep = \" \\t \" ) Loci () . identify_indep_loci ( sumstats = sumstats , sig_threshold = sig_threshold , loci_extend = loci_extension , ldblock = ldblock , if_merge = if_merge , outprefix = output , ldref = ldref , method = method , distance = distance , clump_kb = clump_kb , clump_r2 = clump_r2 , sample_size = sample_size , cojo_window_kb = cojo_window_kb , cojo_collinear = cojo_collinear , diff_freq = diff_freq , use_ref_EAF = use_ref_eaf , only_use_sig_snps = only_use_sig_snps , threads = threads , ) else : logging . error ( f \"No such file of { sumstats_path } .\" ) sys . exit ( 1 )","title":"get_loci()"},{"location":"api/cli/#easyfinemap.cli.main","text":"Source code in easyfinemap/cli.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 @app . callback ( invoke_without_command = True , no_args_is_help = True ) def main ( version : bool = typer . Option ( False , '--version' , '-V' , help = 'Show version.' ), verbose : bool = typer . Option ( False , '--verbose' , '-v' , help = 'Show verbose info.' ), ): \"\"\"EasyFinemap: A user-friendly tool for fine-mapping.\"\"\" console = Console () console . rule ( \"[bold blue]EasyFinemap[/bold blue]\" ) console . print ( f \"Version: { __version__ } \" , justify = \"center\" ) console . print ( \"Author: Jianhua Wang\" , justify = \"center\" ) console . print ( \"Email: jianhua.mert@gmail.com\" , justify = \"center\" ) if version : typer . echo ( f 'EasyFinemap version: { __version__ } ' ) raise typer . Exit () if verbose : logging . getLogger () . setLevel ( logging . DEBUG ) logging . info ( 'Verbose mode is on.' ) else : logging . getLogger () . setLevel ( logging . INFO )","title":"main()"},{"location":"api/cli/#easyfinemap.cli.validate_ldref","text":"Validate the LD reference file. Source code in easyfinemap/cli.py 71 72 73 74 75 76 77 78 79 80 81 @app . command () def validate_ldref ( ldref_path : str = typer . Argument ( ... , help = \"The path to the LD reference file.\" ), outprefix : str = typer . Argument ( ... , help = \"The output prefix.\" ), file_type : str = typer . Option ( \"plink\" , \"--file-type\" , \"-f\" , help = \"The file type of the LD reference file.\" ), mac : int = typer . Option ( 10 , \"--mac\" , \"-m\" , help = \"The minor allele count threshold.\" ), threads : int = typer . Option ( 1 , \"--threads\" , \"-t\" , help = \"The number of threads.\" ), ) -> None : \"\"\"Validate the LD reference file.\"\"\" ld = LDRef () ld . valid ( ldref_path , outprefix , file_type , mac , threads )","title":"validate_ldref()"},{"location":"api/easyfinemap/","text":"Bases: object Main class. Source code in easyfinemap/easyfinemap.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self ): \"\"\"Initialize.\"\"\" self . logger = logging . getLogger ( 'EasyFinemap' ) tool = Tools () self . finemap = tool . finemap self . paintor = tool . paintor self . gcta = tool . gcta self . plink = tool . plink self . bcftools = tool . bcftools self . caviarbf = tool . caviarbf self . model_search = tool . model_search self . tmp_root = Path . cwd () / \"tmp\" / \"easyfinemap\" if not self . tmp_root . exists (): self . tmp_root . mkdir ( parents = True ) annotate_prior ( sumstats , prior_file ) \u00b6 Annotate prior from polyfun results. Parameters: Name Type Description Default sumstats pd . DataFrame Summary statistics. required prior_file str Path to prior file, present only support polyfun's results: https://github.com/omerwe/polyfun/blob/master/snpvar_meta.chr1_7.parquet https://github.com/omerwe/polyfun/blob/master/snpvar_meta.chr1_7.parquet required Returns: Type Description pd . DataFrame Annotated summary statistics. Source code in easyfinemap/easyfinemap.py 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 def annotate_prior ( self , sumstats : pd . DataFrame , prior_file : str , ) -> pd . DataFrame : \"\"\" Annotate prior from polyfun results. Parameters ---------- sumstats : pd.DataFrame Summary statistics. prior_file : str Path to prior file, present only support polyfun's results: https://github.com/omerwe/polyfun/blob/master/snpvar_meta.chr1_7.parquet https://github.com/omerwe/polyfun/blob/master/snpvar_meta.chr1_7.parquet Returns ------- pd.DataFrame Annotated summary statistics. \"\"\" # check tabix index if not os . path . exists ( f \" { prior_file } .tbi\" ): raise ValueError ( f \"No tabix index for { prior_file } \" ) # check header header = pd . read_csv ( prior_file , sep = \" \\t \" , nrows = 0 ) . columns . tolist () if 'snpvar_bin' not in header : raise ValueError ( f \"No snpvar_bin in { prior_file } \" ) # annotate tb = tabix . open ( prior_file ) chrom = sumstats [ ColName . CHR ] . unique ()[ 0 ] start = sumstats [ ColName . BP ] . min () end = sumstats [ ColName . BP ] . max () prior_df = pd . DataFrame ( data = tb . query ( str ( chrom ), start , end ), columns = header ) prior_df = prior_df . rename ( columns = { \"snpvar_bin\" : \"SNPVAR\" }) prior_df [ 'SNPVAR' ] = prior_df [ 'SNPVAR' ] . astype ( float ) prior_df = sg . make_SNPID_unique ( prior_df , ColName . CHR , ColName . BP , 'A1' , 'A2' ) prior_df = prior_df . drop_duplicates ( subset = ColName . SNPID ) prior_map = prior_df [[ 'SNPID' , 'SNPVAR' ]] . set_index ( 'SNPID' ) . to_dict ()[ 'SNPVAR' ] sumstats [ 'SNPVAR' ] = sumstats [ ColName . SNPID ] . map ( prior_map ) . fillna ( 0 ) return sumstats cond_sumstat ( sumstats , lead_snp , lead_snps , ldref , sample_size , use_ref_EAF = False , cond_snps_wind_kb = 1000 , temp_dir = None , ** kwargs ) \u00b6 Conditional sumstat. Parameters: Name Type Description Default sumstats pd . DataFrame Summary statistics. required lead_snp str Lead SNP. required ldref str Path to LD reference. required sample_size int Sample size. required use_ref_EAF bool , optional Use reference EAF, by default False False cond_snps_wind_kb int , optional Conditional SNPs window in kb, by default 1000 1000 temp_dir Optional [ str ], optional Path to tempdir, by default None None Returns: Type Description pd . DataFrame Conditional sumstat. Source code in easyfinemap/easyfinemap.py 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 @io_in_tempdir ( './tmp/easyfinemap' ) def cond_sumstat ( self , sumstats : pd . DataFrame , lead_snp : str , lead_snps : pd . DataFrame , ldref : str , sample_size : int , use_ref_EAF : bool = False , cond_snps_wind_kb : int = 1000 , temp_dir : Optional [ str ] = None , ** kwargs , ) -> pd . DataFrame : \"\"\" Conditional sumstat. Parameters ---------- sumstats : pd.DataFrame Summary statistics. lead_snp : str Lead SNP. ldref : str Path to LD reference. sample_size : int Sample size. use_ref_EAF : bool, optional Use reference EAF, by default False cond_snps_wind_kb : int, optional Conditional SNPs window in kb, by default 1000 temp_dir : Optional[str], optional Path to tempdir, by default None Returns ------- pd.DataFrame Conditional sumstat. \"\"\" if lead_snp is None : raise ValueError ( \"Lead SNP is required for conditional finemapping\" ) if lead_snps is None : raise ValueError ( \"Lead SNPs are required for conditional finemapping\" ) lead_snp_chr = lead_snps . loc [ lead_snps [ ColName . SNPID ] == lead_snp , ColName . CHR ] . values [ 0 ] lead_snp_bp : int = lead_snps . loc [ lead_snps [ ColName . SNPID ] == lead_snp , ColName . BP ] . values [ 0 ] # type: ignore cond_snps = lead_snps [ ( lead_snps [ ColName . CHR ] == lead_snp_chr ) & ( lead_snps [ ColName . BP ] >= lead_snp_bp - cond_snps_wind_kb * 1000 ) & ( lead_snps [ ColName . BP ] <= lead_snp_bp + cond_snps_wind_kb * 1000 ) & ( lead_snps [ ColName . SNPID ] != lead_snp ) ] if cond_snps . empty : self . logger . debug ( f \"No conditional SNPs found for { lead_snp } \" ) cond_res = sumstats . copy () cond_res [ ColName . COJO_BETA ] = cond_res [ ColName . BETA ] cond_res [ ColName . COJO_SE ] = cond_res [ ColName . SE ] cond_res [ ColName . COJO_P ] = cond_res [ ColName . P ] else : ld = LDRef () chrom = lead_snp_chr cojo_input = ld . intersect ( sumstats , ldref , f \" { temp_dir } /cojo_input_ { chrom } \" , use_ref_EAF ) cond_res = ld . cojo_cond ( cojo_input , cond_snps , f \" { temp_dir } /cojo_input_ { chrom } \" , sample_size , use_ref_EAF ) # type: ignore return cond_res finemap_all_loci ( sumstats , loci , lead_snps , methods , var_prior = 0.2 , conditional = False , prior_file = None , sample_size = None , ldref = None , cond_snps_wind_kb = 1000 , max_causal = 1 , credible_threshold = None , credible_method = None , use_ref_EAF = False , outfile = None , threads = 1 ) \u00b6 Perform finemapping for all loci. Parameters: Name Type Description Default sumstats pd . DataFrame Summary statistics. required loci pd . DataFrame Loci. required lead_snps pd . DataFrame Lead SNPs. required methods List [ str ] Finemapping methods. required var_prior float , optional Variance prior, by default 0.2 0.2 conditional bool , optional Conditional finemapping, by default False False prior_file Optional [ str ], optional Path to prior file, by default None None sample_size Optional [ int ], optional Sample size, by default None None ldref Optional [ str ], optional LD reference, by default None None cond_snps_wind_kb int , optional Conditional SNPs window, by default 1000 1000 max_causal int , optional Maximum number of causal variants, by default 1 1 credible_threshold Optional [ float ], optional Credible threshold, by default None None credible_method Optional [ str ], optional Credible method, by default None None use_ref_EAF bool , optional Use reference EAF, by default False False outfile Optional [ str ], optional Output file, by default None None threads int , optional Number of threads, by default 1 1 Source code in easyfinemap/easyfinemap.py 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 def finemap_all_loci ( self , sumstats : pd . DataFrame , loci : pd . DataFrame , lead_snps : pd . DataFrame , methods : List [ str ], var_prior : float = 0.2 , conditional : bool = False , prior_file : Optional [ str ] = None , sample_size : Optional [ int ] = None , ldref : Optional [ str ] = None , cond_snps_wind_kb : int = 1000 , max_causal : int = 1 , credible_threshold : Optional [ float ] = None , credible_method : Optional [ str ] = None , use_ref_EAF : bool = False , outfile : Optional [ str ] = None , threads : int = 1 , ): \"\"\" Perform finemapping for all loci. Parameters ---------- sumstats : pd.DataFrame Summary statistics. loci : pd.DataFrame Loci. lead_snps : pd.DataFrame Lead SNPs. methods : List[str] Finemapping methods. var_prior : float, optional Variance prior, by default 0.2 conditional : bool, optional Conditional finemapping, by default False prior_file : Optional[str], optional Path to prior file, by default None sample_size : Optional[int], optional Sample size, by default None ldref : Optional[str], optional LD reference, by default None cond_snps_wind_kb : int, optional Conditional SNPs window, by default 1000 max_causal : int, optional Maximum number of causal variants, by default 1 credible_threshold : Optional[float], optional Credible threshold, by default None credible_method : Optional[str], optional Credible method, by default None use_ref_EAF : bool, optional Use reference EAF, by default False outfile : Optional[str], optional Output file, by default None threads : int, optional Number of threads, by default 1 \"\"\" # sumstats = sg.make_SNPID_unique(sumstats, ColName.CHR, ColName.BP, ColName.EA, ColName.NEA) if credible_threshold and credible_method is None and methods != [ \"all\" ] and len ( methods ) == 1 : credible_method = methods [ 0 ] kwargs_list = [] for chrom , start , end , lead_snp in loci [[ ColName . CHR , ColName . START , ColName . END , ColName . LEAD_SNP ]] . values : locus_sumstats = sg . export_sumstats ( sumstats , chrom , start , end ) locus_sumstats = sg . make_SNPID_unique ( locus_sumstats , ColName . CHR , ColName . BP , ColName . EA , ColName . NEA ) kwargs = { \"sumstats\" : locus_sumstats , \"lead_snp\" : lead_snp , \"lead_snps\" : lead_snps , \"methods\" : methods , \"var_prior\" : var_prior , \"conditional\" : conditional , \"prior_file\" : prior_file , \"sample_size\" : sample_size , \"ldref\" : ldref . format ( chrom = chrom ) if ldref else None , \"cond_snps_wind_kb\" : cond_snps_wind_kb , \"max_causal\" : max_causal , \"credible_threshold\" : credible_threshold , \"credible_method\" : credible_method , \"use_ref_EAF\" : use_ref_EAF , } kwargs_list . append ( kwargs ) ef = EasyFinemap () # output = [] # with Progress( # TextColumn(\"{task.description}\"), # BarColumn(), # MofNCompleteColumn(), # TimeElapsedColumn(), # auto_refresh=True, # ) as progress: # with Pool(threads) as p: # task = progress.add_task(\"Perform Fine-mapping...\", total=len(loci)) # results = [p.apply_async(ef.finemap_locus, kwds=kwargs) for kwargs in kwargs_list] # for res in results: # progress.update(task, advance=1) # progress.refresh() # output.append(res.get()) with ProcessPoolExecutor ( max_workers = threads ) as executor : output = [] with Progress ( TextColumn ( \" {task.description} \" ), BarColumn (), MofNCompleteColumn (), TimeElapsedColumn (), auto_refresh = True , ) as progress : task = progress . add_task ( \"Perform Fine-mapping...\" , total = len ( kwargs_list )) for _ in executor . map ( ef . finemap_locus_parallel , kwargs_list ): progress . update ( task , advance = 1 ) progress . refresh () output . append ( _ ) output_df = pd . concat ( output , ignore_index = True ) if outfile : output_df . to_csv ( outfile , sep = \" \\t \" , index = False , float_format = \" %0.6g \" ) else : return output_df finemap_locus ( sumstats , methods , lead_snp , conditional = False , prior_file = None , temp_dir = None , ** kwargs ) \u00b6 Finemap a locus. Check if LD is needed, abf does not need LD. If LD is needed, intersect the locus with the LD reference and make the LD matrix. Run the finemapping method. Get the finemapping results. Merge the finemapping results with the input sumstats. Return the credible set or full summary statistics with posterior probabilities. Parameters: Name Type Description Default sumstats pd . DataFrame Summary statistics. required methods List [ str ] Finemapping methods. required lead_snp str Lead SNP. required conditional bool , optional Conditional finemapping, by default False False temp_dir Optional [ str ], optional Temporary directory, by default None None Returns: Type Description pd . DataFrame Finemapping results. Source code in easyfinemap/easyfinemap.py 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 @io_in_tempdir ( './tmp/easyfinemap' ) def finemap_locus ( self , sumstats : pd . DataFrame , methods : List [ str ], lead_snp : str , conditional : bool = False , prior_file : Optional [ str ] = None , temp_dir : Optional [ str ] = None , ** kwargs , ) -> pd . DataFrame : \"\"\" Finemap a locus. 1. Check if LD is needed, abf does not need LD. 2. If LD is needed, intersect the locus with the LD reference and make the LD matrix. 3. Run the finemapping method. 4. Get the finemapping results. 5. Merge the finemapping results with the input sumstats. 6. Return the credible set or full summary statistics with posterior probabilities. Parameters ---------- sumstats : pd.DataFrame Summary statistics. methods : List[str] Finemapping methods. lead_snp : str Lead SNP. conditional : bool, optional Conditional finemapping, by default False temp_dir : Optional[str], optional Temporary directory, by default None Returns ------- pd.DataFrame Finemapping results. \"\"\" if conditional : cond_res = self . cond_sumstat ( sumstats = sumstats , lead_snp = lead_snp , ** kwargs ) fm_input = cond_res . copy () fm_input [ ColName . BETA ] = cond_res [ ColName . COJO_BETA ] fm_input [ ColName . SE ] = cond_res [ ColName . COJO_SE ] fm_input [ ColName . P ] = cond_res [ ColName . COJO_P ] out_sumstats = sumstats . merge ( cond_res [[ ColName . SNPID , ColName . COJO_BETA , ColName . COJO_SE , ColName . COJO_P ]], on = ColName . SNPID , how = \"left\" , ) max_causal = kwargs . get ( \"max_causal\" , 1 ) if max_causal > 1 : self . logger . warning ( \"Conditional finemapping does not support multiple causal variants\" ) else : fm_input = sumstats . copy () out_sumstats = sumstats . copy () allowed_methods = [ \"abf\" , \"finemap\" , \"paintor\" , \"caviarbf\" , \"susie\" , \"polyfun_finemap\" , \"polyfun_susie\" ] if \"all\" in methods : methods = allowed_methods fm_input_ol = fm_input . copy () if prior_file : fm_input_ol = self . annotate_prior ( fm_input_ol , prior_file ) for method in methods : if method == \"abf\" : abf_pp = self . run_abf ( sumstats = fm_input_ol , ** kwargs ) out_sumstats [ ColName . PP_ABF ] = out_sumstats [ ColName . SNPID ] . map ( abf_pp ) elif method in [ \"finemap\" , \"paintor\" , \"caviarbf\" , \"susie\" , \"polyfun_finemap\" , \"polyfun_susie\" ]: ld_matrix = f \" { temp_dir } /intersc.ld\" if not os . path . exists ( ld_matrix ): # TODO: reduce the number of SNPs when using paintor and caviarbf in multiple causal variant mode fm_input_ol = self . prepare_ld_matrix ( sumstats = fm_input_ol , outprefix = f \" { temp_dir } /intersc\" , ** kwargs ) if method == \"finemap\" : finemap_pp = self . run_finemap ( sumstats = fm_input_ol , ld_matrix = ld_matrix , ** kwargs ) out_sumstats [ ColName . PP_FINEMAP ] = out_sumstats [ ColName . SNPID ] . map ( finemap_pp ) elif method == \"paintor\" : paintor_pp = self . run_paintor ( sumstats = fm_input_ol , ld_matrix = ld_matrix , ** kwargs ) out_sumstats [ ColName . PP_PAINTOR ] = out_sumstats [ ColName . SNPID ] . map ( paintor_pp ) elif method == \"caviarbf\" : caviarbf_pp = self . run_caviarbf ( sumstats = fm_input_ol , ld_matrix = ld_matrix , ** kwargs ) out_sumstats [ ColName . PP_CAVIARBF ] = out_sumstats [ ColName . SNPID ] . map ( caviarbf_pp ) elif method == \"susie\" : susie_pp = self . run_susie ( sumstats = fm_input_ol , ld_matrix = ld_matrix , ** kwargs ) out_sumstats [ ColName . PP_SUSIE ] = out_sumstats [ ColName . SNPID ] . map ( susie_pp ) elif method == \"polyfun_finemap\" : polyfun_finemap_pp = self . run_finemap ( sumstats = fm_input_ol , ld_matrix = ld_matrix , ** kwargs ) out_sumstats [ ColName . PP_POLYFUN_FINEMAP ] = out_sumstats [ ColName . SNPID ] . map ( polyfun_finemap_pp ) elif method == \"polyfun_susie\" : polyfun_susie_pp = self . run_susie ( sumstats = fm_input_ol , ld_matrix = ld_matrix , ** kwargs ) out_sumstats [ ColName . PP_POLYFUN_SUSIE ] = out_sumstats [ ColName . SNPID ] . map ( polyfun_susie_pp ) else : raise ValueError ( f \"Method { method } is not supported\" ) credible_set = self . get_credset ( finemap_res = out_sumstats , ** kwargs ) credible_set [ ColName . LEAD_SNP ] = lead_snp return credible_set finemap_locus_parallel ( kwargs ) \u00b6 Perform finemapping for a locus in parallel. Parameters: Name Type Description Default kwargs dict Keyword arguments. required Returns: Type Description pd . DataFrame Finemapping results. Source code in easyfinemap/easyfinemap.py 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 def finemap_locus_parallel ( self , kwargs ): \"\"\" Perform finemapping for a locus in parallel. Parameters ---------- kwargs : dict Keyword arguments. Returns ------- pd.DataFrame Finemapping results. \"\"\" return self . finemap_locus ( ** kwargs ) get_credset ( finemap_res , max_causal , credible_threshold = None , credible_method = None , ** kwargs ) \u00b6 Get credible set. Parameters: Name Type Description Default finemap_res pd . DataFrame Finemap results. required max_causal int Maximum number of causal variants. required credible_threshold Optional [ float ], optional Credible threshold, by default None None credible_method Optional [ str ], optional Credible set method, by default None None Returns: Type Description pd . DataFrame Credible set. Source code in easyfinemap/easyfinemap.py 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 def get_credset ( self , finemap_res : pd . DataFrame , max_causal : int , credible_threshold : Optional [ float ] = None , credible_method : Optional [ str ] = None , ** kwargs , ) -> pd . DataFrame : \"\"\" Get credible set. Parameters ---------- finemap_res : pd.DataFrame Finemap results. max_causal : int Maximum number of causal variants. credible_threshold : Optional[float], optional Credible threshold, by default None credible_method : Optional[str], optional Credible set method, by default None Returns ------- pd.DataFrame Credible set. \"\"\" if credible_threshold is None : return finemap_res else : credible_threshold = credible_threshold * max_causal if credible_method : pp_col = f \"PP_ { credible_method . upper () } \" credible_set = finemap_res . sort_values ( pp_col , ascending = False ) credible_set = finemap_res . sort_values ( by = pp_col , ascending = False ) credible_set = credible_set [ credible_set [ pp_col ] . shift () . fillna ( 0 ) . cumsum () <= credible_threshold ] else : raise ValueError ( \"Must specify credible set method when credible threshold is specified\" ) return credible_set . reset_index ( drop = True ) prepare_ld_matrix ( sumstats , ldref , outprefix , use_ref_EAF = False , ** kwargs ) \u00b6 Prepare LD matrix. Parameters: Name Type Description Default sumstats pd . DataFrame Summary statistics. required ldref str Path to LD reference. required outprefix str Output prefix. required use_ref_EAF bool , optional Use reference EAF, by default False False Returns: Type Description pd . DataFrame LD matrix. Source code in easyfinemap/easyfinemap.py 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 def prepare_ld_matrix ( self , sumstats : pd . DataFrame , ldref : str , outprefix : str , use_ref_EAF : bool = False , ** kwargs , ) -> pd . DataFrame : \"\"\" Prepare LD matrix. Parameters ---------- sumstats : pd.DataFrame Summary statistics. ldref : str Path to LD reference. outprefix : str Output prefix. use_ref_EAF : bool, optional Use reference EAF, by default False Returns ------- pd.DataFrame LD matrix. \"\"\" if ldref is None : raise ValueError ( \"LD reference is required for LD-based finemapping\" ) ld = LDRef () sumstats_ol = ld . intersect ( sumstats , ldref , outprefix , use_ref_EAF ) ld . make_ld ( outprefix , outprefix ) return sumstats_ol run_abf ( sumstats , var_prior = 0.2 , max_causal = 1 , ** kwargs ) \u00b6 Run ABF. calculate the approximate Bayes factor (ABF) from BETA and SE, using the formula: SNP_BF = sqrt(SE/(SE + W^2))EXP(W^2/(SE + W^2)*(BETA^2/SE^2)/2) where W is variance prior, usually set to 0.15 for quantitative traits and 0.2 for binary traits. the posterior probability of each variant being causal is calculated using the formula: PP(causal) = SNP_BF / sum(all_SNP_BFs) Reference: Asimit, J. L. et al. Eur J Hum Genet (2016) Parameters: Name Type Description Default sumstats pd . DataFrame Summary statistics. required var_prior float , optional Variance prior, by default 0.2, usually set to 0.15 for quantitative traits and 0.2 for binary traits. 0.2 max_causal int , optional Maximum number of causal variants, by default 1 1 Returns: Type Description pd . Series The result of ABF. Source code in easyfinemap/easyfinemap.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def run_abf ( self , sumstats : pd . DataFrame , var_prior : float = 0.2 , max_causal : int = 1 , ** kwargs ) -> pd . Series : \"\"\" Run ABF. calculate the approximate Bayes factor (ABF) from BETA and SE, using the formula: SNP_BF = sqrt(SE/(SE + W^2))EXP(W^2/(SE + W^2)*(BETA^2/SE^2)/2) where W is variance prior, usually set to 0.15 for quantitative traits and 0.2 for binary traits. the posterior probability of each variant being causal is calculated using the formula: PP(causal) = SNP_BF / sum(all_SNP_BFs) Reference: Asimit, J. L. et al. Eur J Hum Genet (2016) Parameters ---------- sumstats : pd.DataFrame Summary statistics. var_prior : float, optional Variance prior, by default 0.2, usually set to 0.15 for quantitative traits and 0.2 for binary traits. max_causal : int, optional Maximum number of causal variants, by default 1 Returns ------- pd.Series The result of ABF. \"\"\" if max_causal > 1 : raise NotImplementedError ( \"ABF only support single causal variant.\" ) df = sumstats . copy () df [ \"W2\" ] = var_prior ** 2 df [ \"SNP_BF\" ] = np . sqrt (( df [ ColName . SE ] ** 2 / ( df [ ColName . SE ] ** 2 + df [ \"W2\" ]))) * np . exp ( df [ \"W2\" ] / ( df [ ColName . BETA ] ** 2 + df [ \"W2\" ]) * ( df [ ColName . BETA ] ** 2 / df [ ColName . SE ] ** 2 ) / 2 ) df [ ColName . PP_ABF ] = df [ \"SNP_BF\" ] / df [ \"SNP_BF\" ] . sum () return pd . Series ( data = df [ ColName . PP_ABF ] . values , index = df [ ColName . SNPID ] . tolist ()) run_caviarbf ( sumstats , ld_matrix , max_causal = 1 , temp_dir = None , ** kwargs ) \u00b6 Run CAVIAR-BF. Parameters: Name Type Description Default sumstats pd . DataFrame Summary statistics. required ld_matrix str Path to LD matrix. required max_causal int , optional Maximum number of causal variants, by default 1 1 temp_dir Optional [ str ], optional Path to tempdir, by default None None Returns: Type Description pd . Series The result of CAVIAR-BF. Source code in easyfinemap/easyfinemap.py 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 @io_in_tempdir ( './tmp/easyfinemap' ) def run_caviarbf ( self , sumstats : pd . DataFrame , ld_matrix : str , max_causal : int = 1 , temp_dir : Optional [ str ] = None , ** kwargs ): \"\"\" Run CAVIAR-BF. Parameters ---------- sumstats : pd.DataFrame Summary statistics. ld_matrix : str Path to LD matrix. max_causal : int, optional Maximum number of causal variants, by default 1 temp_dir : Optional[str], optional Path to tempdir, by default None Returns ------- pd.Series The result of CAVIAR-BF. \"\"\" caviar_input = sumstats . copy () caviar_input [ ColName . Z ] = caviar_input [ ColName . BETA ] / caviar_input [ ColName . SE ] caviar_input [[ ColName . SNPID , ColName . Z ]] . to_csv ( f \" { temp_dir } /caviar.input\" , sep = \" \" , index = False , header = False ) n_variants = caviar_input . shape [ 0 ] cmd = [ self . caviarbf , \"-z\" , f \" { temp_dir } /caviar.input\" , \"-r\" , ld_matrix , \"-t\" , \"0\" , \"-a\" , \"0.1281429\" , \"-n\" , str ( n_variants ), \"-c\" , str ( max_causal ), \"-o\" , f \" { temp_dir } /caviar.output\" , ] self . logger . debug ( f \"run CAVIAR-BF: { ' ' . join ( cmd ) } \" ) run ( cmd , stdout = PIPE , stderr = PIPE , universal_newlines = True ) cmd = [ self . model_search , \"-i\" , f \" { temp_dir } /caviar.output\" , \"-m\" , str ( n_variants ), \"-p\" , \"0\" , \"-o\" , f \" { temp_dir } /caviar.prior0\" , ] self . logger . debug ( f \"run CAVIAR-BF: { ' ' . join ( cmd ) } \" ) res = run ( cmd , stdout = PIPE , stderr = PIPE , universal_newlines = True ) if res . returncode != 0 : self . logger . error ( res . stderr ) raise RuntimeError ( res . stderr ) else : caviar_res = pd . read_csv ( f \" { temp_dir } /caviar.prior0.marginal\" , sep = \" \" , header = None ) caviar_res . sort_values ( by = 0 , inplace = True ) # type: ignore caviar_res = pd . Series ( caviar_res [ 1 ] . values , index = caviar_input [ ColName . SNPID ] . tolist ()) return caviar_res run_finemap ( sumstats , ld_matrix , sample_size , max_causal = 1 , prior_file = None , temp_dir = None , ** kwargs ) \u00b6 Run FINEMAP. Parameters: Name Type Description Default sumstats pd . DataFrame Summary statistics. required ld_matrix str Path to LD matrix. required sample_size int Sample size. required max_causal int , optional Maximum number of causal variants, by default 1 1 prior_file Optional [ str ], optional Path to prior file, by default None None temp_dir Optional [ str ], optional Path to tempdir, by default None None Returns: Type Description pd . Series The result of FINEMAP. Source code in easyfinemap/easyfinemap.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 @io_in_tempdir ( './tmp/easyfinemap' ) def run_finemap ( self , sumstats : pd . DataFrame , ld_matrix : str , sample_size : int , max_causal : int = 1 , prior_file : Optional [ str ] = None , temp_dir : Optional [ str ] = None , ** kwargs , ) -> pd . Series : \"\"\" Run FINEMAP. Parameters ---------- sumstats : pd.DataFrame Summary statistics. ld_matrix : str Path to LD matrix. sample_size : int Sample size. max_causal : int, optional Maximum number of causal variants, by default 1 prior_file : Optional[str], optional Path to prior file, by default None temp_dir : Optional[str], optional Path to tempdir, by default None Returns ------- pd.Series The result of FINEMAP. \"\"\" if ColName . MAF not in sumstats . columns : raise ValueError ( f \" { ColName . MAF } is required for FINEMAP.\" ) finemap_input = sumstats . copy () finemap_input [ ColName . MAF ] = finemap_input [ ColName . MAF ] . replace ( 0 , 0.00001 ) if prior_file : finemap_input = finemap_input [ [ ColName . SNPID , ColName . CHR , ColName . BP , ColName . EA , ColName . NEA , ColName . MAF , ColName . BETA , ColName . SE , 'SNPVAR' , ] ] finemap_input . rename ( columns = { ColName . SNPID : \"rsid\" , ColName . CHR : \"chromosome\" , ColName . BP : \"position\" , ColName . MAF : \"maf\" , ColName . BETA : \"beta\" , ColName . SE : \"se\" , ColName . EA : \"allele1\" , ColName . NEA : \"allele2\" , 'SNPVAR' : 'prob' , }, inplace = True , ) finemap_input [ 'prob' ] = finemap_input [ 'prob' ] / finemap_input [ 'prob' ] . sum () else : finemap_input = finemap_input [ [ ColName . SNPID , ColName . CHR , ColName . BP , ColName . EA , ColName . NEA , ColName . MAF , ColName . BETA , ColName . SE ] ] finemap_input . rename ( columns = { ColName . SNPID : \"rsid\" , ColName . CHR : \"chromosome\" , ColName . BP : \"position\" , ColName . MAF : \"maf\" , ColName . BETA : \"beta\" , ColName . SE : \"se\" , ColName . EA : \"allele1\" , ColName . NEA : \"allele2\" , }, inplace = True , ) finemap_input . to_csv ( f \" { temp_dir } /finemap.z\" , sep = \" \" , index = False , float_format = \" %0.5f \" ) with open ( f \" { temp_dir } /finemap.master\" , \"w\" ) as f : master_content = [ f \" { temp_dir } /finemap.z\" , ld_matrix , f \" { temp_dir } /finemap.snp\" , f \" { temp_dir } /finemap.config\" , f \" { temp_dir } /finemap.cred\" , f \" { temp_dir } /finemap.log\" , str ( sample_size ), ] f . write ( \"z;ld;snp;config;cred;log;n_samples \\n \" ) f . write ( \";\" . join ( master_content )) cmd = [ self . finemap , \"--sss\" , \"--in-files\" , f \" { temp_dir } /finemap.master\" , \"--n-causal-snps\" , str ( max_causal ), \"--prior-snps\" if prior_file else \"\" , ] res = run ( cmd , stdout = PIPE , stderr = PIPE , universal_newlines = True ) self . logger . debug ( f \"run FINEMAP: { ' ' . join ( cmd ) } \" ) if res . returncode != 0 : self . logger . error ( res . stderr ) raise RuntimeError ( res . stderr ) else : # if max_causal == 1: finemap_res = pd . read_csv ( f \" { temp_dir } /finemap.snp\" , sep = \" \" , usecols = [ \"rsid\" , \"prob\" ]) finemap_res = pd . Series ( finemap_res [ \"prob\" ] . values , index = finemap_res [ \"rsid\" ] . values ) # type: ignore # else: # raise NotImplementedError return finemap_res run_paintor ( sumstats , ld_matrix , max_causal = 1 , temp_dir = None , ** kwargs ) \u00b6 Run PAINTOR. Parameters: Name Type Description Default sumstats pd . DataFrame Summary statistics. required ld_matrix str Path to LD matrix. required max_causal int , optional Maximum number of causal variants, by default 1 1 temp_dir Optional [ str ], optional Path to tempdir, by default None None Returns: Type Description pd . Series The result of PAINTOR. Source code in easyfinemap/easyfinemap.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 @io_in_tempdir ( './tmp/easyfinemap' ) def run_paintor ( self , sumstats : pd . DataFrame , ld_matrix : str , max_causal : int = 1 , temp_dir : Optional [ str ] = None , ** kwargs ): \"\"\" Run PAINTOR. Parameters ---------- sumstats : pd.DataFrame Summary statistics. ld_matrix : str Path to LD matrix. max_causal : int, optional Maximum number of causal variants, by default 1 temp_dir : Optional[str], optional Path to tempdir, by default None Returns ------- pd.Series The result of PAINTOR. \"\"\" paintor_input = sumstats . copy () paintor_input [ \"coding\" ] = 1 # TODO: support paintor annotation mode paintor_input [ \"Zscore\" ] = paintor_input [ ColName . BETA ] / paintor_input [ ColName . SE ] input_prefix = \"paintor.processed\" paintor_input [[ ColName . SNPID , ColName . CHR , ColName . BP , \"Zscore\" ]] . to_csv ( f \" { temp_dir } / { input_prefix } \" , sep = \" \" , index = False ) paintor_input [ \"coding\" ] . to_csv ( f \" { temp_dir } / { input_prefix } .annotations\" , sep = \" \" , index = False , header = True ) with open ( f \" { temp_dir } / { input_prefix } .input\" , \"w\" ) as f : f . write ( input_prefix ) ld_matrix_abs_path = os . path . abspath ( ld_matrix ) run ( [ 'ln' , \"-s\" , ld_matrix_abs_path , f ' { temp_dir } / { input_prefix } .ld' ], stdout = PIPE , stderr = PIPE , universal_newlines = True , ) cmd = [ self . paintor , \"-input\" , f \" { temp_dir } / { input_prefix } .input\" , \"-out\" , temp_dir , \"-Zhead\" , \"Zscore\" , \"-LDname\" , \"ld\" , \"-enumerate\" , str ( max_causal ), \"-in\" , temp_dir , ] self . logger . debug ( f \"run PAINTOR: { ' ' . join ( cmd ) } \" ) res = run ( cmd , stdout = PIPE , stderr = PIPE , universal_newlines = True ) if res . returncode != 0 : self . logger . error ( res . stderr ) raise RuntimeError ( res . stderr ) else : paintor_res = pd . read_csv ( f \" { temp_dir } /paintor.processed.results\" , sep = \" \" , usecols = [ \"SNPID\" , \"Posterior_Prob\" ] ) paintor_res = pd . Series ( paintor_res [ \"Posterior_Prob\" ] . values , index = paintor_res [ \"SNPID\" ] . tolist ()) return paintor_res run_susie ( sumstats , ld_matrix , sample_size , max_causal = 1 , prior_file = None , temp_dir = None , ** kwargs ) \u00b6 Run SuSiE. Parameters: Name Type Description Default sumstats pd . DataFrame Summary statistics. required ld_matrix str Path to LD matrix. required sample_size int Sample size. required max_causal int , optional Maximum number of causal variants, by default 1 1 prior_file Optional [ str ], optional Path to prior file, by default None None Returns: Type Description pd . Series The result of SuSiE. Source code in easyfinemap/easyfinemap.py 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 @io_in_tempdir ( './tmp/easyfinemap' ) def run_susie ( self , sumstats : pd . DataFrame , ld_matrix : str , sample_size : int , max_causal : int = 1 , prior_file : Optional [ str ] = None , temp_dir : Optional [ str ] = None , ** kwargs , ) -> pd . Series : \"\"\" Run SuSiE. Parameters ---------- sumstats : pd.DataFrame Summary statistics. ld_matrix : str Path to LD matrix. sample_size : int Sample size. max_causal : int, optional Maximum number of causal variants, by default 1 prior_file : Optional[str], optional Path to prior file, by default None Returns ------- pd.Series The result of SuSiE. \"\"\" susie_input = sumstats . copy () susie_input [ ColName . Z ] = susie_input [ ColName . BETA ] / susie_input [ ColName . SE ] if prior_file : susie_input [ 'SNPVAR' ] = susie_input [ 'SNPVAR' ] / susie_input [ 'SNPVAR' ] . sum () else : susie_input [ 'SNPVAR' ] = 1 / len ( susie_input ) susie_input [[ ColName . SNPID , ColName . Z ]] . to_csv ( f \" { temp_dir } /susie.input\" , sep = \" \" , index = False , header = True ) import rpy2.robjects as ro from rpy2.rinterface_lib.callbacks import logger as rpy2_logger rpy2_logger . setLevel ( logging . ERROR ) ro . r ( f '''library('data.table') ld = fread(' { ld_matrix } ', sep=' ', header=FALSE) ld = as.matrix(ld) df = fread(' { temp_dir } /susie.input', sep=' ', header=TRUE) z = df$Z prior = df$SNPVAR library('susieR') res = susie_rss(z, ld, n= { sample_size } , L = { max_causal } ) pip = res$pip''' ) susie_input [ 'pip' ] = ro . r ( 'pip' ) susie_res = pd . Series ( susie_input [ 'pip' ] . values , index = susie_input [ ColName . SNPID ] . tolist ()) return susie_res","title":"EasyFinemap"},{"location":"api/easyfinemap/#easyfinemap.easyfinemap.EasyFinemap.annotate_prior","text":"Annotate prior from polyfun results. Parameters: Name Type Description Default sumstats pd . DataFrame Summary statistics. required prior_file str Path to prior file, present only support polyfun's results: https://github.com/omerwe/polyfun/blob/master/snpvar_meta.chr1_7.parquet https://github.com/omerwe/polyfun/blob/master/snpvar_meta.chr1_7.parquet required Returns: Type Description pd . DataFrame Annotated summary statistics. Source code in easyfinemap/easyfinemap.py 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 def annotate_prior ( self , sumstats : pd . DataFrame , prior_file : str , ) -> pd . DataFrame : \"\"\" Annotate prior from polyfun results. Parameters ---------- sumstats : pd.DataFrame Summary statistics. prior_file : str Path to prior file, present only support polyfun's results: https://github.com/omerwe/polyfun/blob/master/snpvar_meta.chr1_7.parquet https://github.com/omerwe/polyfun/blob/master/snpvar_meta.chr1_7.parquet Returns ------- pd.DataFrame Annotated summary statistics. \"\"\" # check tabix index if not os . path . exists ( f \" { prior_file } .tbi\" ): raise ValueError ( f \"No tabix index for { prior_file } \" ) # check header header = pd . read_csv ( prior_file , sep = \" \\t \" , nrows = 0 ) . columns . tolist () if 'snpvar_bin' not in header : raise ValueError ( f \"No snpvar_bin in { prior_file } \" ) # annotate tb = tabix . open ( prior_file ) chrom = sumstats [ ColName . CHR ] . unique ()[ 0 ] start = sumstats [ ColName . BP ] . min () end = sumstats [ ColName . BP ] . max () prior_df = pd . DataFrame ( data = tb . query ( str ( chrom ), start , end ), columns = header ) prior_df = prior_df . rename ( columns = { \"snpvar_bin\" : \"SNPVAR\" }) prior_df [ 'SNPVAR' ] = prior_df [ 'SNPVAR' ] . astype ( float ) prior_df = sg . make_SNPID_unique ( prior_df , ColName . CHR , ColName . BP , 'A1' , 'A2' ) prior_df = prior_df . drop_duplicates ( subset = ColName . SNPID ) prior_map = prior_df [[ 'SNPID' , 'SNPVAR' ]] . set_index ( 'SNPID' ) . to_dict ()[ 'SNPVAR' ] sumstats [ 'SNPVAR' ] = sumstats [ ColName . SNPID ] . map ( prior_map ) . fillna ( 0 ) return sumstats","title":"annotate_prior()"},{"location":"api/easyfinemap/#easyfinemap.easyfinemap.EasyFinemap.cond_sumstat","text":"Conditional sumstat. Parameters: Name Type Description Default sumstats pd . DataFrame Summary statistics. required lead_snp str Lead SNP. required ldref str Path to LD reference. required sample_size int Sample size. required use_ref_EAF bool , optional Use reference EAF, by default False False cond_snps_wind_kb int , optional Conditional SNPs window in kb, by default 1000 1000 temp_dir Optional [ str ], optional Path to tempdir, by default None None Returns: Type Description pd . DataFrame Conditional sumstat. Source code in easyfinemap/easyfinemap.py 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 @io_in_tempdir ( './tmp/easyfinemap' ) def cond_sumstat ( self , sumstats : pd . DataFrame , lead_snp : str , lead_snps : pd . DataFrame , ldref : str , sample_size : int , use_ref_EAF : bool = False , cond_snps_wind_kb : int = 1000 , temp_dir : Optional [ str ] = None , ** kwargs , ) -> pd . DataFrame : \"\"\" Conditional sumstat. Parameters ---------- sumstats : pd.DataFrame Summary statistics. lead_snp : str Lead SNP. ldref : str Path to LD reference. sample_size : int Sample size. use_ref_EAF : bool, optional Use reference EAF, by default False cond_snps_wind_kb : int, optional Conditional SNPs window in kb, by default 1000 temp_dir : Optional[str], optional Path to tempdir, by default None Returns ------- pd.DataFrame Conditional sumstat. \"\"\" if lead_snp is None : raise ValueError ( \"Lead SNP is required for conditional finemapping\" ) if lead_snps is None : raise ValueError ( \"Lead SNPs are required for conditional finemapping\" ) lead_snp_chr = lead_snps . loc [ lead_snps [ ColName . SNPID ] == lead_snp , ColName . CHR ] . values [ 0 ] lead_snp_bp : int = lead_snps . loc [ lead_snps [ ColName . SNPID ] == lead_snp , ColName . BP ] . values [ 0 ] # type: ignore cond_snps = lead_snps [ ( lead_snps [ ColName . CHR ] == lead_snp_chr ) & ( lead_snps [ ColName . BP ] >= lead_snp_bp - cond_snps_wind_kb * 1000 ) & ( lead_snps [ ColName . BP ] <= lead_snp_bp + cond_snps_wind_kb * 1000 ) & ( lead_snps [ ColName . SNPID ] != lead_snp ) ] if cond_snps . empty : self . logger . debug ( f \"No conditional SNPs found for { lead_snp } \" ) cond_res = sumstats . copy () cond_res [ ColName . COJO_BETA ] = cond_res [ ColName . BETA ] cond_res [ ColName . COJO_SE ] = cond_res [ ColName . SE ] cond_res [ ColName . COJO_P ] = cond_res [ ColName . P ] else : ld = LDRef () chrom = lead_snp_chr cojo_input = ld . intersect ( sumstats , ldref , f \" { temp_dir } /cojo_input_ { chrom } \" , use_ref_EAF ) cond_res = ld . cojo_cond ( cojo_input , cond_snps , f \" { temp_dir } /cojo_input_ { chrom } \" , sample_size , use_ref_EAF ) # type: ignore return cond_res","title":"cond_sumstat()"},{"location":"api/easyfinemap/#easyfinemap.easyfinemap.EasyFinemap.finemap_all_loci","text":"Perform finemapping for all loci. Parameters: Name Type Description Default sumstats pd . DataFrame Summary statistics. required loci pd . DataFrame Loci. required lead_snps pd . DataFrame Lead SNPs. required methods List [ str ] Finemapping methods. required var_prior float , optional Variance prior, by default 0.2 0.2 conditional bool , optional Conditional finemapping, by default False False prior_file Optional [ str ], optional Path to prior file, by default None None sample_size Optional [ int ], optional Sample size, by default None None ldref Optional [ str ], optional LD reference, by default None None cond_snps_wind_kb int , optional Conditional SNPs window, by default 1000 1000 max_causal int , optional Maximum number of causal variants, by default 1 1 credible_threshold Optional [ float ], optional Credible threshold, by default None None credible_method Optional [ str ], optional Credible method, by default None None use_ref_EAF bool , optional Use reference EAF, by default False False outfile Optional [ str ], optional Output file, by default None None threads int , optional Number of threads, by default 1 1 Source code in easyfinemap/easyfinemap.py 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 def finemap_all_loci ( self , sumstats : pd . DataFrame , loci : pd . DataFrame , lead_snps : pd . DataFrame , methods : List [ str ], var_prior : float = 0.2 , conditional : bool = False , prior_file : Optional [ str ] = None , sample_size : Optional [ int ] = None , ldref : Optional [ str ] = None , cond_snps_wind_kb : int = 1000 , max_causal : int = 1 , credible_threshold : Optional [ float ] = None , credible_method : Optional [ str ] = None , use_ref_EAF : bool = False , outfile : Optional [ str ] = None , threads : int = 1 , ): \"\"\" Perform finemapping for all loci. Parameters ---------- sumstats : pd.DataFrame Summary statistics. loci : pd.DataFrame Loci. lead_snps : pd.DataFrame Lead SNPs. methods : List[str] Finemapping methods. var_prior : float, optional Variance prior, by default 0.2 conditional : bool, optional Conditional finemapping, by default False prior_file : Optional[str], optional Path to prior file, by default None sample_size : Optional[int], optional Sample size, by default None ldref : Optional[str], optional LD reference, by default None cond_snps_wind_kb : int, optional Conditional SNPs window, by default 1000 max_causal : int, optional Maximum number of causal variants, by default 1 credible_threshold : Optional[float], optional Credible threshold, by default None credible_method : Optional[str], optional Credible method, by default None use_ref_EAF : bool, optional Use reference EAF, by default False outfile : Optional[str], optional Output file, by default None threads : int, optional Number of threads, by default 1 \"\"\" # sumstats = sg.make_SNPID_unique(sumstats, ColName.CHR, ColName.BP, ColName.EA, ColName.NEA) if credible_threshold and credible_method is None and methods != [ \"all\" ] and len ( methods ) == 1 : credible_method = methods [ 0 ] kwargs_list = [] for chrom , start , end , lead_snp in loci [[ ColName . CHR , ColName . START , ColName . END , ColName . LEAD_SNP ]] . values : locus_sumstats = sg . export_sumstats ( sumstats , chrom , start , end ) locus_sumstats = sg . make_SNPID_unique ( locus_sumstats , ColName . CHR , ColName . BP , ColName . EA , ColName . NEA ) kwargs = { \"sumstats\" : locus_sumstats , \"lead_snp\" : lead_snp , \"lead_snps\" : lead_snps , \"methods\" : methods , \"var_prior\" : var_prior , \"conditional\" : conditional , \"prior_file\" : prior_file , \"sample_size\" : sample_size , \"ldref\" : ldref . format ( chrom = chrom ) if ldref else None , \"cond_snps_wind_kb\" : cond_snps_wind_kb , \"max_causal\" : max_causal , \"credible_threshold\" : credible_threshold , \"credible_method\" : credible_method , \"use_ref_EAF\" : use_ref_EAF , } kwargs_list . append ( kwargs ) ef = EasyFinemap () # output = [] # with Progress( # TextColumn(\"{task.description}\"), # BarColumn(), # MofNCompleteColumn(), # TimeElapsedColumn(), # auto_refresh=True, # ) as progress: # with Pool(threads) as p: # task = progress.add_task(\"Perform Fine-mapping...\", total=len(loci)) # results = [p.apply_async(ef.finemap_locus, kwds=kwargs) for kwargs in kwargs_list] # for res in results: # progress.update(task, advance=1) # progress.refresh() # output.append(res.get()) with ProcessPoolExecutor ( max_workers = threads ) as executor : output = [] with Progress ( TextColumn ( \" {task.description} \" ), BarColumn (), MofNCompleteColumn (), TimeElapsedColumn (), auto_refresh = True , ) as progress : task = progress . add_task ( \"Perform Fine-mapping...\" , total = len ( kwargs_list )) for _ in executor . map ( ef . finemap_locus_parallel , kwargs_list ): progress . update ( task , advance = 1 ) progress . refresh () output . append ( _ ) output_df = pd . concat ( output , ignore_index = True ) if outfile : output_df . to_csv ( outfile , sep = \" \\t \" , index = False , float_format = \" %0.6g \" ) else : return output_df","title":"finemap_all_loci()"},{"location":"api/easyfinemap/#easyfinemap.easyfinemap.EasyFinemap.finemap_locus","text":"Finemap a locus. Check if LD is needed, abf does not need LD. If LD is needed, intersect the locus with the LD reference and make the LD matrix. Run the finemapping method. Get the finemapping results. Merge the finemapping results with the input sumstats. Return the credible set or full summary statistics with posterior probabilities. Parameters: Name Type Description Default sumstats pd . DataFrame Summary statistics. required methods List [ str ] Finemapping methods. required lead_snp str Lead SNP. required conditional bool , optional Conditional finemapping, by default False False temp_dir Optional [ str ], optional Temporary directory, by default None None Returns: Type Description pd . DataFrame Finemapping results. Source code in easyfinemap/easyfinemap.py 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 @io_in_tempdir ( './tmp/easyfinemap' ) def finemap_locus ( self , sumstats : pd . DataFrame , methods : List [ str ], lead_snp : str , conditional : bool = False , prior_file : Optional [ str ] = None , temp_dir : Optional [ str ] = None , ** kwargs , ) -> pd . DataFrame : \"\"\" Finemap a locus. 1. Check if LD is needed, abf does not need LD. 2. If LD is needed, intersect the locus with the LD reference and make the LD matrix. 3. Run the finemapping method. 4. Get the finemapping results. 5. Merge the finemapping results with the input sumstats. 6. Return the credible set or full summary statistics with posterior probabilities. Parameters ---------- sumstats : pd.DataFrame Summary statistics. methods : List[str] Finemapping methods. lead_snp : str Lead SNP. conditional : bool, optional Conditional finemapping, by default False temp_dir : Optional[str], optional Temporary directory, by default None Returns ------- pd.DataFrame Finemapping results. \"\"\" if conditional : cond_res = self . cond_sumstat ( sumstats = sumstats , lead_snp = lead_snp , ** kwargs ) fm_input = cond_res . copy () fm_input [ ColName . BETA ] = cond_res [ ColName . COJO_BETA ] fm_input [ ColName . SE ] = cond_res [ ColName . COJO_SE ] fm_input [ ColName . P ] = cond_res [ ColName . COJO_P ] out_sumstats = sumstats . merge ( cond_res [[ ColName . SNPID , ColName . COJO_BETA , ColName . COJO_SE , ColName . COJO_P ]], on = ColName . SNPID , how = \"left\" , ) max_causal = kwargs . get ( \"max_causal\" , 1 ) if max_causal > 1 : self . logger . warning ( \"Conditional finemapping does not support multiple causal variants\" ) else : fm_input = sumstats . copy () out_sumstats = sumstats . copy () allowed_methods = [ \"abf\" , \"finemap\" , \"paintor\" , \"caviarbf\" , \"susie\" , \"polyfun_finemap\" , \"polyfun_susie\" ] if \"all\" in methods : methods = allowed_methods fm_input_ol = fm_input . copy () if prior_file : fm_input_ol = self . annotate_prior ( fm_input_ol , prior_file ) for method in methods : if method == \"abf\" : abf_pp = self . run_abf ( sumstats = fm_input_ol , ** kwargs ) out_sumstats [ ColName . PP_ABF ] = out_sumstats [ ColName . SNPID ] . map ( abf_pp ) elif method in [ \"finemap\" , \"paintor\" , \"caviarbf\" , \"susie\" , \"polyfun_finemap\" , \"polyfun_susie\" ]: ld_matrix = f \" { temp_dir } /intersc.ld\" if not os . path . exists ( ld_matrix ): # TODO: reduce the number of SNPs when using paintor and caviarbf in multiple causal variant mode fm_input_ol = self . prepare_ld_matrix ( sumstats = fm_input_ol , outprefix = f \" { temp_dir } /intersc\" , ** kwargs ) if method == \"finemap\" : finemap_pp = self . run_finemap ( sumstats = fm_input_ol , ld_matrix = ld_matrix , ** kwargs ) out_sumstats [ ColName . PP_FINEMAP ] = out_sumstats [ ColName . SNPID ] . map ( finemap_pp ) elif method == \"paintor\" : paintor_pp = self . run_paintor ( sumstats = fm_input_ol , ld_matrix = ld_matrix , ** kwargs ) out_sumstats [ ColName . PP_PAINTOR ] = out_sumstats [ ColName . SNPID ] . map ( paintor_pp ) elif method == \"caviarbf\" : caviarbf_pp = self . run_caviarbf ( sumstats = fm_input_ol , ld_matrix = ld_matrix , ** kwargs ) out_sumstats [ ColName . PP_CAVIARBF ] = out_sumstats [ ColName . SNPID ] . map ( caviarbf_pp ) elif method == \"susie\" : susie_pp = self . run_susie ( sumstats = fm_input_ol , ld_matrix = ld_matrix , ** kwargs ) out_sumstats [ ColName . PP_SUSIE ] = out_sumstats [ ColName . SNPID ] . map ( susie_pp ) elif method == \"polyfun_finemap\" : polyfun_finemap_pp = self . run_finemap ( sumstats = fm_input_ol , ld_matrix = ld_matrix , ** kwargs ) out_sumstats [ ColName . PP_POLYFUN_FINEMAP ] = out_sumstats [ ColName . SNPID ] . map ( polyfun_finemap_pp ) elif method == \"polyfun_susie\" : polyfun_susie_pp = self . run_susie ( sumstats = fm_input_ol , ld_matrix = ld_matrix , ** kwargs ) out_sumstats [ ColName . PP_POLYFUN_SUSIE ] = out_sumstats [ ColName . SNPID ] . map ( polyfun_susie_pp ) else : raise ValueError ( f \"Method { method } is not supported\" ) credible_set = self . get_credset ( finemap_res = out_sumstats , ** kwargs ) credible_set [ ColName . LEAD_SNP ] = lead_snp return credible_set","title":"finemap_locus()"},{"location":"api/easyfinemap/#easyfinemap.easyfinemap.EasyFinemap.finemap_locus_parallel","text":"Perform finemapping for a locus in parallel. Parameters: Name Type Description Default kwargs dict Keyword arguments. required Returns: Type Description pd . DataFrame Finemapping results. Source code in easyfinemap/easyfinemap.py 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 def finemap_locus_parallel ( self , kwargs ): \"\"\" Perform finemapping for a locus in parallel. Parameters ---------- kwargs : dict Keyword arguments. Returns ------- pd.DataFrame Finemapping results. \"\"\" return self . finemap_locus ( ** kwargs )","title":"finemap_locus_parallel()"},{"location":"api/easyfinemap/#easyfinemap.easyfinemap.EasyFinemap.get_credset","text":"Get credible set. Parameters: Name Type Description Default finemap_res pd . DataFrame Finemap results. required max_causal int Maximum number of causal variants. required credible_threshold Optional [ float ], optional Credible threshold, by default None None credible_method Optional [ str ], optional Credible set method, by default None None Returns: Type Description pd . DataFrame Credible set. Source code in easyfinemap/easyfinemap.py 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 def get_credset ( self , finemap_res : pd . DataFrame , max_causal : int , credible_threshold : Optional [ float ] = None , credible_method : Optional [ str ] = None , ** kwargs , ) -> pd . DataFrame : \"\"\" Get credible set. Parameters ---------- finemap_res : pd.DataFrame Finemap results. max_causal : int Maximum number of causal variants. credible_threshold : Optional[float], optional Credible threshold, by default None credible_method : Optional[str], optional Credible set method, by default None Returns ------- pd.DataFrame Credible set. \"\"\" if credible_threshold is None : return finemap_res else : credible_threshold = credible_threshold * max_causal if credible_method : pp_col = f \"PP_ { credible_method . upper () } \" credible_set = finemap_res . sort_values ( pp_col , ascending = False ) credible_set = finemap_res . sort_values ( by = pp_col , ascending = False ) credible_set = credible_set [ credible_set [ pp_col ] . shift () . fillna ( 0 ) . cumsum () <= credible_threshold ] else : raise ValueError ( \"Must specify credible set method when credible threshold is specified\" ) return credible_set . reset_index ( drop = True )","title":"get_credset()"},{"location":"api/easyfinemap/#easyfinemap.easyfinemap.EasyFinemap.prepare_ld_matrix","text":"Prepare LD matrix. Parameters: Name Type Description Default sumstats pd . DataFrame Summary statistics. required ldref str Path to LD reference. required outprefix str Output prefix. required use_ref_EAF bool , optional Use reference EAF, by default False False Returns: Type Description pd . DataFrame LD matrix. Source code in easyfinemap/easyfinemap.py 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 def prepare_ld_matrix ( self , sumstats : pd . DataFrame , ldref : str , outprefix : str , use_ref_EAF : bool = False , ** kwargs , ) -> pd . DataFrame : \"\"\" Prepare LD matrix. Parameters ---------- sumstats : pd.DataFrame Summary statistics. ldref : str Path to LD reference. outprefix : str Output prefix. use_ref_EAF : bool, optional Use reference EAF, by default False Returns ------- pd.DataFrame LD matrix. \"\"\" if ldref is None : raise ValueError ( \"LD reference is required for LD-based finemapping\" ) ld = LDRef () sumstats_ol = ld . intersect ( sumstats , ldref , outprefix , use_ref_EAF ) ld . make_ld ( outprefix , outprefix ) return sumstats_ol","title":"prepare_ld_matrix()"},{"location":"api/easyfinemap/#easyfinemap.easyfinemap.EasyFinemap.run_abf","text":"Run ABF. calculate the approximate Bayes factor (ABF) from BETA and SE, using the formula: SNP_BF = sqrt(SE/(SE + W^2))EXP(W^2/(SE + W^2)*(BETA^2/SE^2)/2) where W is variance prior, usually set to 0.15 for quantitative traits and 0.2 for binary traits. the posterior probability of each variant being causal is calculated using the formula: PP(causal) = SNP_BF / sum(all_SNP_BFs) Reference: Asimit, J. L. et al. Eur J Hum Genet (2016) Parameters: Name Type Description Default sumstats pd . DataFrame Summary statistics. required var_prior float , optional Variance prior, by default 0.2, usually set to 0.15 for quantitative traits and 0.2 for binary traits. 0.2 max_causal int , optional Maximum number of causal variants, by default 1 1 Returns: Type Description pd . Series The result of ABF. Source code in easyfinemap/easyfinemap.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def run_abf ( self , sumstats : pd . DataFrame , var_prior : float = 0.2 , max_causal : int = 1 , ** kwargs ) -> pd . Series : \"\"\" Run ABF. calculate the approximate Bayes factor (ABF) from BETA and SE, using the formula: SNP_BF = sqrt(SE/(SE + W^2))EXP(W^2/(SE + W^2)*(BETA^2/SE^2)/2) where W is variance prior, usually set to 0.15 for quantitative traits and 0.2 for binary traits. the posterior probability of each variant being causal is calculated using the formula: PP(causal) = SNP_BF / sum(all_SNP_BFs) Reference: Asimit, J. L. et al. Eur J Hum Genet (2016) Parameters ---------- sumstats : pd.DataFrame Summary statistics. var_prior : float, optional Variance prior, by default 0.2, usually set to 0.15 for quantitative traits and 0.2 for binary traits. max_causal : int, optional Maximum number of causal variants, by default 1 Returns ------- pd.Series The result of ABF. \"\"\" if max_causal > 1 : raise NotImplementedError ( \"ABF only support single causal variant.\" ) df = sumstats . copy () df [ \"W2\" ] = var_prior ** 2 df [ \"SNP_BF\" ] = np . sqrt (( df [ ColName . SE ] ** 2 / ( df [ ColName . SE ] ** 2 + df [ \"W2\" ]))) * np . exp ( df [ \"W2\" ] / ( df [ ColName . BETA ] ** 2 + df [ \"W2\" ]) * ( df [ ColName . BETA ] ** 2 / df [ ColName . SE ] ** 2 ) / 2 ) df [ ColName . PP_ABF ] = df [ \"SNP_BF\" ] / df [ \"SNP_BF\" ] . sum () return pd . Series ( data = df [ ColName . PP_ABF ] . values , index = df [ ColName . SNPID ] . tolist ())","title":"run_abf()"},{"location":"api/easyfinemap/#easyfinemap.easyfinemap.EasyFinemap.run_caviarbf","text":"Run CAVIAR-BF. Parameters: Name Type Description Default sumstats pd . DataFrame Summary statistics. required ld_matrix str Path to LD matrix. required max_causal int , optional Maximum number of causal variants, by default 1 1 temp_dir Optional [ str ], optional Path to tempdir, by default None None Returns: Type Description pd . Series The result of CAVIAR-BF. Source code in easyfinemap/easyfinemap.py 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 @io_in_tempdir ( './tmp/easyfinemap' ) def run_caviarbf ( self , sumstats : pd . DataFrame , ld_matrix : str , max_causal : int = 1 , temp_dir : Optional [ str ] = None , ** kwargs ): \"\"\" Run CAVIAR-BF. Parameters ---------- sumstats : pd.DataFrame Summary statistics. ld_matrix : str Path to LD matrix. max_causal : int, optional Maximum number of causal variants, by default 1 temp_dir : Optional[str], optional Path to tempdir, by default None Returns ------- pd.Series The result of CAVIAR-BF. \"\"\" caviar_input = sumstats . copy () caviar_input [ ColName . Z ] = caviar_input [ ColName . BETA ] / caviar_input [ ColName . SE ] caviar_input [[ ColName . SNPID , ColName . Z ]] . to_csv ( f \" { temp_dir } /caviar.input\" , sep = \" \" , index = False , header = False ) n_variants = caviar_input . shape [ 0 ] cmd = [ self . caviarbf , \"-z\" , f \" { temp_dir } /caviar.input\" , \"-r\" , ld_matrix , \"-t\" , \"0\" , \"-a\" , \"0.1281429\" , \"-n\" , str ( n_variants ), \"-c\" , str ( max_causal ), \"-o\" , f \" { temp_dir } /caviar.output\" , ] self . logger . debug ( f \"run CAVIAR-BF: { ' ' . join ( cmd ) } \" ) run ( cmd , stdout = PIPE , stderr = PIPE , universal_newlines = True ) cmd = [ self . model_search , \"-i\" , f \" { temp_dir } /caviar.output\" , \"-m\" , str ( n_variants ), \"-p\" , \"0\" , \"-o\" , f \" { temp_dir } /caviar.prior0\" , ] self . logger . debug ( f \"run CAVIAR-BF: { ' ' . join ( cmd ) } \" ) res = run ( cmd , stdout = PIPE , stderr = PIPE , universal_newlines = True ) if res . returncode != 0 : self . logger . error ( res . stderr ) raise RuntimeError ( res . stderr ) else : caviar_res = pd . read_csv ( f \" { temp_dir } /caviar.prior0.marginal\" , sep = \" \" , header = None ) caviar_res . sort_values ( by = 0 , inplace = True ) # type: ignore caviar_res = pd . Series ( caviar_res [ 1 ] . values , index = caviar_input [ ColName . SNPID ] . tolist ()) return caviar_res","title":"run_caviarbf()"},{"location":"api/easyfinemap/#easyfinemap.easyfinemap.EasyFinemap.run_finemap","text":"Run FINEMAP. Parameters: Name Type Description Default sumstats pd . DataFrame Summary statistics. required ld_matrix str Path to LD matrix. required sample_size int Sample size. required max_causal int , optional Maximum number of causal variants, by default 1 1 prior_file Optional [ str ], optional Path to prior file, by default None None temp_dir Optional [ str ], optional Path to tempdir, by default None None Returns: Type Description pd . Series The result of FINEMAP. Source code in easyfinemap/easyfinemap.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 @io_in_tempdir ( './tmp/easyfinemap' ) def run_finemap ( self , sumstats : pd . DataFrame , ld_matrix : str , sample_size : int , max_causal : int = 1 , prior_file : Optional [ str ] = None , temp_dir : Optional [ str ] = None , ** kwargs , ) -> pd . Series : \"\"\" Run FINEMAP. Parameters ---------- sumstats : pd.DataFrame Summary statistics. ld_matrix : str Path to LD matrix. sample_size : int Sample size. max_causal : int, optional Maximum number of causal variants, by default 1 prior_file : Optional[str], optional Path to prior file, by default None temp_dir : Optional[str], optional Path to tempdir, by default None Returns ------- pd.Series The result of FINEMAP. \"\"\" if ColName . MAF not in sumstats . columns : raise ValueError ( f \" { ColName . MAF } is required for FINEMAP.\" ) finemap_input = sumstats . copy () finemap_input [ ColName . MAF ] = finemap_input [ ColName . MAF ] . replace ( 0 , 0.00001 ) if prior_file : finemap_input = finemap_input [ [ ColName . SNPID , ColName . CHR , ColName . BP , ColName . EA , ColName . NEA , ColName . MAF , ColName . BETA , ColName . SE , 'SNPVAR' , ] ] finemap_input . rename ( columns = { ColName . SNPID : \"rsid\" , ColName . CHR : \"chromosome\" , ColName . BP : \"position\" , ColName . MAF : \"maf\" , ColName . BETA : \"beta\" , ColName . SE : \"se\" , ColName . EA : \"allele1\" , ColName . NEA : \"allele2\" , 'SNPVAR' : 'prob' , }, inplace = True , ) finemap_input [ 'prob' ] = finemap_input [ 'prob' ] / finemap_input [ 'prob' ] . sum () else : finemap_input = finemap_input [ [ ColName . SNPID , ColName . CHR , ColName . BP , ColName . EA , ColName . NEA , ColName . MAF , ColName . BETA , ColName . SE ] ] finemap_input . rename ( columns = { ColName . SNPID : \"rsid\" , ColName . CHR : \"chromosome\" , ColName . BP : \"position\" , ColName . MAF : \"maf\" , ColName . BETA : \"beta\" , ColName . SE : \"se\" , ColName . EA : \"allele1\" , ColName . NEA : \"allele2\" , }, inplace = True , ) finemap_input . to_csv ( f \" { temp_dir } /finemap.z\" , sep = \" \" , index = False , float_format = \" %0.5f \" ) with open ( f \" { temp_dir } /finemap.master\" , \"w\" ) as f : master_content = [ f \" { temp_dir } /finemap.z\" , ld_matrix , f \" { temp_dir } /finemap.snp\" , f \" { temp_dir } /finemap.config\" , f \" { temp_dir } /finemap.cred\" , f \" { temp_dir } /finemap.log\" , str ( sample_size ), ] f . write ( \"z;ld;snp;config;cred;log;n_samples \\n \" ) f . write ( \";\" . join ( master_content )) cmd = [ self . finemap , \"--sss\" , \"--in-files\" , f \" { temp_dir } /finemap.master\" , \"--n-causal-snps\" , str ( max_causal ), \"--prior-snps\" if prior_file else \"\" , ] res = run ( cmd , stdout = PIPE , stderr = PIPE , universal_newlines = True ) self . logger . debug ( f \"run FINEMAP: { ' ' . join ( cmd ) } \" ) if res . returncode != 0 : self . logger . error ( res . stderr ) raise RuntimeError ( res . stderr ) else : # if max_causal == 1: finemap_res = pd . read_csv ( f \" { temp_dir } /finemap.snp\" , sep = \" \" , usecols = [ \"rsid\" , \"prob\" ]) finemap_res = pd . Series ( finemap_res [ \"prob\" ] . values , index = finemap_res [ \"rsid\" ] . values ) # type: ignore # else: # raise NotImplementedError return finemap_res","title":"run_finemap()"},{"location":"api/easyfinemap/#easyfinemap.easyfinemap.EasyFinemap.run_paintor","text":"Run PAINTOR. Parameters: Name Type Description Default sumstats pd . DataFrame Summary statistics. required ld_matrix str Path to LD matrix. required max_causal int , optional Maximum number of causal variants, by default 1 1 temp_dir Optional [ str ], optional Path to tempdir, by default None None Returns: Type Description pd . Series The result of PAINTOR. Source code in easyfinemap/easyfinemap.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 @io_in_tempdir ( './tmp/easyfinemap' ) def run_paintor ( self , sumstats : pd . DataFrame , ld_matrix : str , max_causal : int = 1 , temp_dir : Optional [ str ] = None , ** kwargs ): \"\"\" Run PAINTOR. Parameters ---------- sumstats : pd.DataFrame Summary statistics. ld_matrix : str Path to LD matrix. max_causal : int, optional Maximum number of causal variants, by default 1 temp_dir : Optional[str], optional Path to tempdir, by default None Returns ------- pd.Series The result of PAINTOR. \"\"\" paintor_input = sumstats . copy () paintor_input [ \"coding\" ] = 1 # TODO: support paintor annotation mode paintor_input [ \"Zscore\" ] = paintor_input [ ColName . BETA ] / paintor_input [ ColName . SE ] input_prefix = \"paintor.processed\" paintor_input [[ ColName . SNPID , ColName . CHR , ColName . BP , \"Zscore\" ]] . to_csv ( f \" { temp_dir } / { input_prefix } \" , sep = \" \" , index = False ) paintor_input [ \"coding\" ] . to_csv ( f \" { temp_dir } / { input_prefix } .annotations\" , sep = \" \" , index = False , header = True ) with open ( f \" { temp_dir } / { input_prefix } .input\" , \"w\" ) as f : f . write ( input_prefix ) ld_matrix_abs_path = os . path . abspath ( ld_matrix ) run ( [ 'ln' , \"-s\" , ld_matrix_abs_path , f ' { temp_dir } / { input_prefix } .ld' ], stdout = PIPE , stderr = PIPE , universal_newlines = True , ) cmd = [ self . paintor , \"-input\" , f \" { temp_dir } / { input_prefix } .input\" , \"-out\" , temp_dir , \"-Zhead\" , \"Zscore\" , \"-LDname\" , \"ld\" , \"-enumerate\" , str ( max_causal ), \"-in\" , temp_dir , ] self . logger . debug ( f \"run PAINTOR: { ' ' . join ( cmd ) } \" ) res = run ( cmd , stdout = PIPE , stderr = PIPE , universal_newlines = True ) if res . returncode != 0 : self . logger . error ( res . stderr ) raise RuntimeError ( res . stderr ) else : paintor_res = pd . read_csv ( f \" { temp_dir } /paintor.processed.results\" , sep = \" \" , usecols = [ \"SNPID\" , \"Posterior_Prob\" ] ) paintor_res = pd . Series ( paintor_res [ \"Posterior_Prob\" ] . values , index = paintor_res [ \"SNPID\" ] . tolist ()) return paintor_res","title":"run_paintor()"},{"location":"api/easyfinemap/#easyfinemap.easyfinemap.EasyFinemap.run_susie","text":"Run SuSiE. Parameters: Name Type Description Default sumstats pd . DataFrame Summary statistics. required ld_matrix str Path to LD matrix. required sample_size int Sample size. required max_causal int , optional Maximum number of causal variants, by default 1 1 prior_file Optional [ str ], optional Path to prior file, by default None None Returns: Type Description pd . Series The result of SuSiE. Source code in easyfinemap/easyfinemap.py 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 @io_in_tempdir ( './tmp/easyfinemap' ) def run_susie ( self , sumstats : pd . DataFrame , ld_matrix : str , sample_size : int , max_causal : int = 1 , prior_file : Optional [ str ] = None , temp_dir : Optional [ str ] = None , ** kwargs , ) -> pd . Series : \"\"\" Run SuSiE. Parameters ---------- sumstats : pd.DataFrame Summary statistics. ld_matrix : str Path to LD matrix. sample_size : int Sample size. max_causal : int, optional Maximum number of causal variants, by default 1 prior_file : Optional[str], optional Path to prior file, by default None Returns ------- pd.Series The result of SuSiE. \"\"\" susie_input = sumstats . copy () susie_input [ ColName . Z ] = susie_input [ ColName . BETA ] / susie_input [ ColName . SE ] if prior_file : susie_input [ 'SNPVAR' ] = susie_input [ 'SNPVAR' ] / susie_input [ 'SNPVAR' ] . sum () else : susie_input [ 'SNPVAR' ] = 1 / len ( susie_input ) susie_input [[ ColName . SNPID , ColName . Z ]] . to_csv ( f \" { temp_dir } /susie.input\" , sep = \" \" , index = False , header = True ) import rpy2.robjects as ro from rpy2.rinterface_lib.callbacks import logger as rpy2_logger rpy2_logger . setLevel ( logging . ERROR ) ro . r ( f '''library('data.table') ld = fread(' { ld_matrix } ', sep=' ', header=FALSE) ld = as.matrix(ld) df = fread(' { temp_dir } /susie.input', sep=' ', header=TRUE) z = df$Z prior = df$SNPVAR library('susieR') res = susie_rss(z, ld, n= { sample_size } , L = { max_causal } ) pip = res$pip''' ) susie_input [ 'pip' ] = ro . r ( 'pip' ) susie_res = pd . Series ( susie_input [ 'pip' ] . values , index = susie_input [ ColName . SNPID ] . tolist ()) return susie_res","title":"run_susie()"},{"location":"api/tools/","text":"check if tools are installed and return their path. Source code in easyfinemap/tools.py 11 12 13 def __init__ ( self ): \"\"\"Initialize.\"\"\" self . logger = logging . getLogger ( \"Tools\" ) bcftools property \u00b6 Check if bcftools is installed. caviarbf property \u00b6 Check if caviarbf is installed. finemap property \u00b6 Check if finemap is installed. gcta property \u00b6 Check if gcta is installed. model_search property \u00b6 Check if model_search is installed. paintor property \u00b6 Check if paintor is installed. plink property \u00b6 Check if plink is installed.","title":"Tools"},{"location":"api/tools/#easyfinemap.tools.Tools.bcftools","text":"Check if bcftools is installed.","title":"bcftools"},{"location":"api/tools/#easyfinemap.tools.Tools.caviarbf","text":"Check if caviarbf is installed.","title":"caviarbf"},{"location":"api/tools/#easyfinemap.tools.Tools.finemap","text":"Check if finemap is installed.","title":"finemap"},{"location":"api/tools/#easyfinemap.tools.Tools.gcta","text":"Check if gcta is installed.","title":"gcta"},{"location":"api/tools/#easyfinemap.tools.Tools.model_search","text":"Check if model_search is installed.","title":"model_search"},{"location":"api/tools/#easyfinemap.tools.Tools.paintor","text":"Check if paintor is installed.","title":"paintor"},{"location":"api/tools/#easyfinemap.tools.Tools.plink","text":"Check if plink is installed.","title":"plink"},{"location":"api/utils/","text":"Utils for easyfinemap. get_significant_snps ( df , pvalue_threshold = 5e-08 , use_most_sig_if_no_sig = True ) \u00b6 Get the significant snps from the input file, filter by pvalue. Parameters: Name Type Description Default df pd . DataFrame The input summary statistics. required pvalue_threshold float , optional The pvalue threshold, by default 5e-8 5e-08 use_most_sig_if_no_sig bool , optional Whether to use the most significant SNP if no significant SNP found, by default True True Returns: Type Description pd . DataFrame The significant snps, sorted by pvalue. Source code in easyfinemap/utils.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def get_significant_snps ( df : pd . DataFrame , pvalue_threshold : float = 5e-8 , use_most_sig_if_no_sig : bool = True ): \"\"\" Get the significant snps from the input file, filter by pvalue. Parameters ---------- df : pd.DataFrame The input summary statistics. pvalue_threshold : float, optional The pvalue threshold, by default 5e-8 use_most_sig_if_no_sig : bool, optional Whether to use the most significant SNP if no significant SNP found, by default True Returns ------- pd.DataFrame The significant snps, sorted by pvalue. \"\"\" sig_df = df . loc [ df [ ColName . P ] < pvalue_threshold ] . copy () if sig_df . empty : if use_most_sig_if_no_sig : sig_df = df . loc [ df [ ColName . P ] == df [ ColName . P ] . min ()] . copy () logging . debug ( f \"Use the most significant SNP: { sig_df [ ColName . SNPID ] . values [ 0 ] } \" ) logging . debug ( f \"pvalue: { sig_df [ ColName . P ] . values [ 0 ] } \" ) else : raise ValueError ( \"No significant SNPs found.\" ) else : sig_df . sort_values ( ColName . P , inplace = True ) sig_df . reset_index ( drop = True , inplace = True ) return sig_df io_in_tempdir ( dir = './tmp' ) \u00b6 Make tempdir for process. Parameters: Name Type Description Default dir str , optional The tempdir, by default './tmp' './tmp' Returns: Type Description decorator The decorator of io in tempdir. Source code in easyfinemap/utils.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def io_in_tempdir ( dir = './tmp' ): \"\"\" Make tempdir for process. Parameters ---------- dir : str, optional The tempdir, by default './tmp' Returns ------- decorator The decorator of io in tempdir. \"\"\" def decorator ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): temp_dir = tempfile . mkdtemp ( dir = dir ) logger = logging . getLogger ( \"IO\" ) logger . debug ( f \"Tempdir: { temp_dir } \" ) try : result = func ( * args , temp_dir = temp_dir , ** kwargs ) except Exception : raise else : if logging . getLogger () . getEffectiveLevel () >= logging . INFO : shutil . rmtree ( temp_dir ) pass return result # type: ignore return wrapper return decorator make_SNPID_unique ( sumstat , replace_rsIDcol = False , remove_duplicates = True ) \u00b6 Make the SNPID unique. The unique SNPID is chr-bp-sorted(EA,NEA) Parameters: Name Type Description Default sumstat pd . DataFrame The input summary statistics. required replace_rsIDcol bool , optional Whether to replace the rsID column with the unique SNPID, by default False False remove_duplicates bool , optional Whether to remove the duplicated SNPs, keep the one with smallest P-value, by default True True Returns: Type Description pd . DataFrame The summary statistics with unique SNPID. Source code in easyfinemap/utils.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def make_SNPID_unique ( sumstat : pd . DataFrame , replace_rsIDcol : bool = False , remove_duplicates : bool = True ): \"\"\" Make the SNPID unique. The unique SNPID is chr-bp-sorted(EA,NEA) Parameters ---------- sumstat : pd.DataFrame The input summary statistics. replace_rsIDcol : bool, optional Whether to replace the rsID column with the unique SNPID, by default False remove_duplicates : bool, optional Whether to remove the duplicated SNPs, keep the one with smallest P-value, by default True Returns ------- pd.DataFrame The summary statistics with unique SNPID. \"\"\" df = sumstat . copy () allele_df = df [[ ColName . EA , ColName . NEA ]] . copy () b = allele_df . values b . sort ( axis = 1 ) allele_df [[ ColName . EA , ColName . NEA ]] = b allele_df [ ColName . SNPID ] = ( df [ ColName . CHR ] . astype ( str ) + \"-\" + df [ ColName . BP ] . astype ( str ) + \"-\" + allele_df [ ColName . EA ] + \"-\" + allele_df [ ColName . NEA ] ) if replace_rsIDcol : df [ ColName . RSID ] = allele_df [ ColName . SNPID ] else : if ColName . SNPID in df . columns : df . drop ( ColName . SNPID , axis = 1 , inplace = True ) df . insert ( loc = 0 , column = ColName . SNPID , value = allele_df [ ColName . SNPID ] . values ) # type: ignore if remove_duplicates : df . sort_values ( ColName . P , inplace = True ) if replace_rsIDcol : df . drop_duplicates ( subset = [ ColName . RSID ], keep = \"first\" , inplace = True ) else : df . drop_duplicates ( subset = [ ColName . SNPID ], keep = \"first\" , inplace = True ) df . sort_values ([ ColName . CHR , ColName . BP ], inplace = True ) df . reset_index ( drop = True , inplace = True ) return df","title":"utils"},{"location":"api/utils/#easyfinemap.utils.get_significant_snps","text":"Get the significant snps from the input file, filter by pvalue. Parameters: Name Type Description Default df pd . DataFrame The input summary statistics. required pvalue_threshold float , optional The pvalue threshold, by default 5e-8 5e-08 use_most_sig_if_no_sig bool , optional Whether to use the most significant SNP if no significant SNP found, by default True True Returns: Type Description pd . DataFrame The significant snps, sorted by pvalue. Source code in easyfinemap/utils.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def get_significant_snps ( df : pd . DataFrame , pvalue_threshold : float = 5e-8 , use_most_sig_if_no_sig : bool = True ): \"\"\" Get the significant snps from the input file, filter by pvalue. Parameters ---------- df : pd.DataFrame The input summary statistics. pvalue_threshold : float, optional The pvalue threshold, by default 5e-8 use_most_sig_if_no_sig : bool, optional Whether to use the most significant SNP if no significant SNP found, by default True Returns ------- pd.DataFrame The significant snps, sorted by pvalue. \"\"\" sig_df = df . loc [ df [ ColName . P ] < pvalue_threshold ] . copy () if sig_df . empty : if use_most_sig_if_no_sig : sig_df = df . loc [ df [ ColName . P ] == df [ ColName . P ] . min ()] . copy () logging . debug ( f \"Use the most significant SNP: { sig_df [ ColName . SNPID ] . values [ 0 ] } \" ) logging . debug ( f \"pvalue: { sig_df [ ColName . P ] . values [ 0 ] } \" ) else : raise ValueError ( \"No significant SNPs found.\" ) else : sig_df . sort_values ( ColName . P , inplace = True ) sig_df . reset_index ( drop = True , inplace = True ) return sig_df","title":"get_significant_snps()"},{"location":"api/utils/#easyfinemap.utils.io_in_tempdir","text":"Make tempdir for process. Parameters: Name Type Description Default dir str , optional The tempdir, by default './tmp' './tmp' Returns: Type Description decorator The decorator of io in tempdir. Source code in easyfinemap/utils.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def io_in_tempdir ( dir = './tmp' ): \"\"\" Make tempdir for process. Parameters ---------- dir : str, optional The tempdir, by default './tmp' Returns ------- decorator The decorator of io in tempdir. \"\"\" def decorator ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): temp_dir = tempfile . mkdtemp ( dir = dir ) logger = logging . getLogger ( \"IO\" ) logger . debug ( f \"Tempdir: { temp_dir } \" ) try : result = func ( * args , temp_dir = temp_dir , ** kwargs ) except Exception : raise else : if logging . getLogger () . getEffectiveLevel () >= logging . INFO : shutil . rmtree ( temp_dir ) pass return result # type: ignore return wrapper return decorator","title":"io_in_tempdir()"},{"location":"api/utils/#easyfinemap.utils.make_SNPID_unique","text":"Make the SNPID unique. The unique SNPID is chr-bp-sorted(EA,NEA) Parameters: Name Type Description Default sumstat pd . DataFrame The input summary statistics. required replace_rsIDcol bool , optional Whether to replace the rsID column with the unique SNPID, by default False False remove_duplicates bool , optional Whether to remove the duplicated SNPs, keep the one with smallest P-value, by default True True Returns: Type Description pd . DataFrame The summary statistics with unique SNPID. Source code in easyfinemap/utils.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def make_SNPID_unique ( sumstat : pd . DataFrame , replace_rsIDcol : bool = False , remove_duplicates : bool = True ): \"\"\" Make the SNPID unique. The unique SNPID is chr-bp-sorted(EA,NEA) Parameters ---------- sumstat : pd.DataFrame The input summary statistics. replace_rsIDcol : bool, optional Whether to replace the rsID column with the unique SNPID, by default False remove_duplicates : bool, optional Whether to remove the duplicated SNPs, keep the one with smallest P-value, by default True Returns ------- pd.DataFrame The summary statistics with unique SNPID. \"\"\" df = sumstat . copy () allele_df = df [[ ColName . EA , ColName . NEA ]] . copy () b = allele_df . values b . sort ( axis = 1 ) allele_df [[ ColName . EA , ColName . NEA ]] = b allele_df [ ColName . SNPID ] = ( df [ ColName . CHR ] . astype ( str ) + \"-\" + df [ ColName . BP ] . astype ( str ) + \"-\" + allele_df [ ColName . EA ] + \"-\" + allele_df [ ColName . NEA ] ) if replace_rsIDcol : df [ ColName . RSID ] = allele_df [ ColName . SNPID ] else : if ColName . SNPID in df . columns : df . drop ( ColName . SNPID , axis = 1 , inplace = True ) df . insert ( loc = 0 , column = ColName . SNPID , value = allele_df [ ColName . SNPID ] . values ) # type: ignore if remove_duplicates : df . sort_values ( ColName . P , inplace = True ) if replace_rsIDcol : df . drop_duplicates ( subset = [ ColName . RSID ], keep = \"first\" , inplace = True ) else : df . drop_duplicates ( subset = [ ColName . SNPID ], keep = \"first\" , inplace = True ) df . sort_values ([ ColName . CHR , ColName . BP ], inplace = True ) df . reset_index ( drop = True , inplace = True ) return df","title":"make_SNPID_unique()"}]}